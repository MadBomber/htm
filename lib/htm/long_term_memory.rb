# frozen_string_literal: true

require 'pg'
require 'pgvector'
require 'json'
require 'connection_pool'
require 'lru_redux'
require 'digest'

class HTM
  # Long-term Memory - PostgreSQL/TimescaleDB-backed permanent storage
  #
  # LongTermMemory provides durable storage for all memory nodes with:
  # - Vector similarity search (RAG)
  # - Full-text search
  # - Time-range queries
  # - Relationship graphs
  # - Tag system
  # - Connection pooling for efficiency
  # - Query timeouts for reliability
  #
  class LongTermMemory
    DEFAULT_POOL_SIZE = 5
    DEFAULT_POOL_TIMEOUT = 5  # seconds to wait for a connection from the pool
    DEFAULT_QUERY_TIMEOUT = 30_000  # milliseconds (30 seconds)
    MAX_VECTOR_DIMENSION = 2000  # Maximum supported dimension with HNSW index (pgvector limitation)
    DEFAULT_CACHE_SIZE = 1000  # Number of queries to cache
    DEFAULT_CACHE_TTL = 300    # Cache lifetime in seconds (5 minutes)

    attr_reader :pool_size, :query_timeout

    def initialize(config, pool_size: DEFAULT_POOL_SIZE, query_timeout: DEFAULT_QUERY_TIMEOUT, cache_size: DEFAULT_CACHE_SIZE, cache_ttl: DEFAULT_CACHE_TTL)
      @config = config
      raise "Database configuration required" unless @config

      @pool_size = pool_size
      @query_timeout = query_timeout  # in milliseconds

      # Create connection pool
      @pool = ConnectionPool.new(size: @pool_size, timeout: DEFAULT_POOL_TIMEOUT) do
        conn = PG.connect(@config)
        # Set statement timeout for all queries on this connection
        conn.exec("SET statement_timeout = #{@query_timeout}")

        # Configure embedding generation via pgai
        configure_embedding_settings(conn)

        # Configure topic extraction via pgai
        configure_topic_settings(conn)

        conn
      end

      # Initialize query result cache (disable with cache_size: 0)
      if cache_size > 0
        @query_cache = LruRedux::TTL::ThreadSafeCache.new(cache_size, cache_ttl)
        @cache_stats = { hits: 0, misses: 0 }
      end
    end

    # Add a node to long-term memory
    #
    # Embeddings can be provided or generated by pgai database triggers (if available).
    #
    # @param key [String] Node identifier
    # @param value [String] Node content
    # @param type [String, nil] Node type
    # @param category [String, nil] Node category
    # @param importance [Float] Importance score
    # @param token_count [Integer] Token count
    # @param robot_id [String] Robot identifier
    # @param embedding [Array<Float>, nil] Optional pre-generated embedding vector
    # @return [Integer] Node database ID
    #
    def add(key:, value:, type: nil, category: nil, importance: 1.0, token_count: 0, robot_id:, embedding: nil)
      # Insert node with optional embedding
      node_id = with_connection do |conn|
        if embedding
          # Convert embedding array to PostgreSQL vector format
          embedding_str = "[#{embedding.join(',')}]"
          result = conn.exec_params(
            <<~SQL,
              INSERT INTO nodes (key, value, type, category, importance, token_count, robot_id, embedding)
              VALUES ($1, $2, $3, $4, $5, $6, $7, $8::vector)
              RETURNING id
            SQL
            [key, value, type, category, importance, token_count, robot_id, embedding_str]
          )
        else
          result = conn.exec_params(
            <<~SQL,
              INSERT INTO nodes (key, value, type, category, importance, token_count, robot_id)
              VALUES ($1, $2, $3, $4, $5, $6, $7)
              RETURNING id
            SQL
            [key, value, type, category, importance, token_count, robot_id]
          )
        end
        result.first['id'].to_i
      end

      # Invalidate cache since database content changed
      invalidate_cache!

      node_id
    end

    # Retrieve a node by key
    #
    # @param key [String] Node identifier
    # @return [Hash, nil] Node data or nil
    #
    def retrieve(key)
      with_connection do |conn|
        result = conn.exec_params("SELECT * FROM nodes WHERE key = $1", [key])
        result.first
      end
    end

    # Update last_accessed timestamp
    #
    # @param key [String] Node identifier
    # @return [void]
    #
    def update_last_accessed(key)
      with_connection do |conn|
        conn.exec_params(
          "UPDATE nodes SET last_accessed = CURRENT_TIMESTAMP WHERE key = $1",
          [key]
        )
      end
    end

    # Delete a node
    #
    # @param key [String] Node identifier
    # @return [void]
    #
    def delete(key)
      with_connection do |conn|
        conn.exec_params("DELETE FROM nodes WHERE key = $1", [key])
      end

      # Invalidate cache since database content changed
      invalidate_cache!
    end

    # Get node database ID
    #
    # @param key [String] Node identifier
    # @return [Integer, nil] Database ID or nil
    #
    def get_node_id(key)
      with_connection do |conn|
        result = conn.exec_params("SELECT id FROM nodes WHERE key = $1", [key])
        result.first&.fetch('id')&.to_i
      end
    end

    # Vector similarity search
    #
    # @param timeframe [Range] Time range to search
    # @param query [String] Search query
    # @param limit [Integer] Maximum results
    # @param embedding_service [Object] Service to generate embeddings
    # @return [Array<Hash>] Matching nodes
    #
    def search(timeframe:, query:, limit:, embedding_service:)
      # Return uncached if cache disabled
      return search_uncached(timeframe: timeframe, query: query, limit: limit, embedding_service: embedding_service) unless @query_cache

      # Generate cache key
      cache_key = cache_key_for(:search, timeframe, query, limit)

      # Try to get from cache
      cached = @query_cache[cache_key]
      if cached
        @cache_stats[:hits] += 1
        return cached
      end

      # Cache miss - execute query
      @cache_stats[:misses] += 1
      result = search_uncached(timeframe: timeframe, query: query, limit: limit, embedding_service: embedding_service)

      # Store in cache
      @query_cache[cache_key] = result
      result
    end

    # Full-text search
    #
    # @param timeframe [Range] Time range to search
    # @param query [String] Search query
    # @param limit [Integer] Maximum results
    # @return [Array<Hash>] Matching nodes
    #
    def search_fulltext(timeframe:, query:, limit:)
      # Return uncached if cache disabled
      return search_fulltext_uncached(timeframe: timeframe, query: query, limit: limit) unless @query_cache

      # Generate cache key
      cache_key = cache_key_for(:fulltext, timeframe, query, limit)

      # Try to get from cache
      cached = @query_cache[cache_key]
      if cached
        @cache_stats[:hits] += 1
        return cached
      end

      # Cache miss - execute query
      @cache_stats[:misses] += 1
      result = search_fulltext_uncached(timeframe: timeframe, query: query, limit: limit)

      # Store in cache
      @query_cache[cache_key] = result
      result
    end

    # Hybrid search (full-text + vector)
    #
    # @param timeframe [Range] Time range to search
    # @param query [String] Search query
    # @param limit [Integer] Maximum results
    # @param embedding_service [Object] Service to generate embeddings
    # @param prefilter_limit [Integer] Candidates to consider (default: 100)
    # @return [Array<Hash>] Matching nodes
    #
    def search_hybrid(timeframe:, query:, limit:, embedding_service:, prefilter_limit: 100)
      # Return uncached if cache disabled
      return search_hybrid_uncached(timeframe: timeframe, query: query, limit: limit, embedding_service: embedding_service, prefilter_limit: prefilter_limit) unless @query_cache

      # Generate cache key
      cache_key = cache_key_for(:hybrid, timeframe, query, limit, prefilter_limit)

      # Try to get from cache
      cached = @query_cache[cache_key]
      if cached
        @cache_stats[:hits] += 1
        return cached
      end

      # Cache miss - execute query
      @cache_stats[:misses] += 1
      result = search_hybrid_uncached(timeframe: timeframe, query: query, limit: limit, embedding_service: embedding_service, prefilter_limit: prefilter_limit)

      # Store in cache
      @query_cache[cache_key] = result
      result
    end

    # Add a relationship between nodes
    #
    # @param from [String] From node key
    # @param to [String] To node key
    # @param type [String, nil] Relationship type
    # @param strength [Float] Relationship strength
    # @return [void]
    #
    def add_relationship(from:, to:, type: nil, strength: 1.0)
      with_connection do |conn|
        from_id = get_node_id(from)
        to_id = get_node_id(to)
        return unless from_id && to_id

        conn.exec_params(
          <<~SQL,
            INSERT INTO relationships (from_node_id, to_node_id, relationship_type, strength)
            VALUES ($1, $2, $3, $4)
            ON CONFLICT (from_node_id, to_node_id, relationship_type) DO NOTHING
          SQL
          [from_id, to_id, type, strength]
        )
      end
    end

    # Add a tag to a node
    #
    # @param node_id [Integer] Node database ID
    # @param tag [String] Tag name
    # @return [void]
    #
    def add_tag(node_id:, tag:)
      with_connection do |conn|
        conn.exec_params(
          "INSERT INTO tags (node_id, tag) VALUES ($1, $2) ON CONFLICT DO NOTHING",
          [node_id, tag]
        )
      end
    end

    # Mark nodes as evicted from working memory
    #
    # @param keys [Array<String>] Node keys
    # @return [void]
    #
    def mark_evicted(keys)
      return if keys.empty?

      with_connection do |conn|
        conn.exec_params(
          "UPDATE nodes SET in_working_memory = FALSE WHERE key = ANY($1::text[])",
          [keys]
        )
      end
    end

    # Register a robot
    #
    # @param robot_id [String] Robot identifier
    # @param robot_name [String] Robot name
    # @return [void]
    #
    def register_robot(robot_id, robot_name)
      with_connection do |conn|
        conn.exec_params(
          <<~SQL,
            INSERT INTO robots (id, name)
            VALUES ($1, $2)
            ON CONFLICT (id) DO UPDATE SET name = $2, last_active = CURRENT_TIMESTAMP
          SQL
          [robot_id, robot_name]
        )
      end
    end

    # Update robot activity timestamp
    #
    # @param robot_id [String] Robot identifier
    # @return [void]
    #
    def update_robot_activity(robot_id)
      with_connection do |conn|
        conn.exec_params(
          "UPDATE robots SET last_active = CURRENT_TIMESTAMP WHERE id = $1",
          [robot_id]
        )
      end
    end

    # Log an operation
    #
    # @param operation [String] Operation type
    # @param node_id [Integer, nil] Node database ID
    # @param robot_id [String] Robot identifier
    # @param details [Hash] Operation details
    # @return [void]
    #
    def log_operation(operation:, node_id:, robot_id:, details:)
      with_connection do |conn|
        conn.exec_params(
          "INSERT INTO operations_log (operation, node_id, robot_id, details) VALUES ($1, $2, $3, $4)",
          [operation, node_id, robot_id, details.to_json]
        )
      end
    end

    # Get memory statistics
    #
    # @return [Hash] Statistics
    #
    def stats
      base_stats = with_connection do |conn|
        {
          total_nodes: conn.exec("SELECT COUNT(*) FROM nodes").first['count'].to_i,
          nodes_by_robot: conn.exec(
            "SELECT robot_id, COUNT(*) as count FROM nodes GROUP BY robot_id"
          ).to_a.map { |r| [r['robot_id'], r['count'].to_i] }.to_h,
          nodes_by_type: conn.exec("SELECT * FROM node_stats").to_a,
          total_relationships: conn.exec("SELECT COUNT(*) FROM relationships").first['count'].to_i,
          total_tags: conn.exec("SELECT COUNT(*) FROM tags").first['count'].to_i,
          oldest_memory: conn.exec("SELECT MIN(created_at) FROM nodes").first['min'],
          newest_memory: conn.exec("SELECT MAX(created_at) FROM nodes").first['max'],
          active_robots: conn.exec("SELECT COUNT(*) FROM robots").first['count'].to_i,
          robot_activity: conn.exec("SELECT * FROM robot_activity").to_a,
          database_size: conn.exec("SELECT pg_database_size(current_database())").first['pg_database_size'].to_i
        }
      end

      # Include cache statistics if cache is enabled
      if @query_cache
        base_stats[:cache] = cache_stats
      end

      base_stats
    end

    # Shutdown the connection pool
    # Call this when shutting down the application
    def shutdown
      @pool.shutdown { |conn| conn.close }
    end

    # Retrieve nodes by ontological topic
    #
    # @param topic_path [String] Topic hierarchy path
    # @param exact [Boolean] Exact match or prefix match
    # @param limit [Integer] Maximum results
    # @return [Array<Hash>] Matching nodes
    #
    def nodes_by_topic(topic_path, exact: false, limit: 50)
      with_connection do |conn|
        if exact
          result = conn.exec_params(
            <<~SQL,
              SELECT DISTINCT n.*
              FROM nodes n
              JOIN tags t ON t.node_id = n.id
              WHERE t.tag = $1
              ORDER BY n.created_at DESC
              LIMIT $2
            SQL
            [topic_path, limit]
          )
        else
          result = conn.exec_params(
            <<~SQL,
              SELECT DISTINCT n.*
              FROM nodes n
              JOIN tags t ON t.node_id = n.id
              WHERE t.tag LIKE $1
              ORDER BY n.created_at DESC
              LIMIT $2
            SQL
            ["#{topic_path}%", limit]
          )
        end
        result.to_a
      end
    end

    # Get ontology structure view
    #
    # @return [Array<Hash>] Ontology structure
    #
    def ontology_structure
      with_connection do |conn|
        result = conn.exec("SELECT * FROM ontology_structure WHERE root_topic IS NOT NULL ORDER BY root_topic, level1_topic, level2_topic")
        result.to_a
      end
    end

    # Get topic relationships (co-occurrence)
    #
    # @param min_shared_nodes [Integer] Minimum shared nodes
    # @param limit [Integer] Maximum relationships
    # @return [Array<Hash>] Topic relationships
    #
    def topic_relationships(min_shared_nodes: 2, limit: 50)
      with_connection do |conn|
        result = conn.exec_params(
          <<~SQL,
            SELECT t1.tag AS topic1, t2.tag AS topic2, COUNT(DISTINCT t1.node_id) AS shared_nodes
            FROM tags t1
            JOIN tags t2 ON t1.node_id = t2.node_id AND t1.tag < t2.tag
            GROUP BY t1.tag, t2.tag
            HAVING COUNT(DISTINCT t1.node_id) >= $1
            ORDER BY shared_nodes DESC
            LIMIT $2
          SQL
          [min_shared_nodes, limit]
        )
        result.to_a
      end
    end

    # Get topics for a specific node
    #
    # @param node_id [Integer] Node database ID
    # @return [Array<String>] Topic paths
    #
    def node_topics(node_id)
      with_connection do |conn|
        result = conn.exec_params(
          "SELECT tag FROM tags WHERE node_id = $1 ORDER BY tag",
          [node_id]
        )
        result.map { |row| row['tag'] }
      end
    end

    private

    # Generate cache key for query
    #
    # @param method [Symbol] Search method name
    # @param timeframe [Range] Time range
    # @param query [String] Search query
    # @param limit [Integer] Result limit
    # @param args [Array] Additional arguments
    # @return [String] Cache key
    #
    def cache_key_for(method, timeframe, query, limit, *args)
      key_parts = [
        method,
        timeframe.begin.to_i,
        timeframe.end.to_i,
        query,
        limit,
        *args
      ]
      Digest::SHA256.hexdigest(key_parts.join('|'))
    end

    # Get cache statistics
    #
    # @return [Hash, nil] Cache stats or nil if cache disabled
    #
    def cache_stats
      return nil unless @query_cache

      total = @cache_stats[:hits] + @cache_stats[:misses]
      hit_rate = total > 0 ? (@cache_stats[:hits].to_f / total * 100).round(2) : 0.0

      {
        hits: @cache_stats[:hits],
        misses: @cache_stats[:misses],
        hit_rate: hit_rate,
        size: @query_cache.count
      }
    end

    # Invalidate (clear) the query cache
    #
    # @return [void]
    #
    def invalidate_cache!
      @query_cache.clear if @query_cache
    end

    # Uncached vector similarity search
    #
    # Uses pgai to generate query embedding directly in the database.
    #
    # @param timeframe [Range] Time range to search
    # @param query [String] Search query
    # @param limit [Integer] Maximum results
    # @param embedding_service [Object] DEPRECATED - Not used (kept for API compatibility)
    # @return [Array<Hash>] Matching nodes
    #
    def search_uncached(timeframe:, query:, limit:, embedding_service: nil)
      # Query embedding is generated in the database using pgai
      # The generate_node_embedding function uses current configuration
      with_connection do |conn|
        # Get current embedding configuration
        provider = conn.exec("SELECT current_setting('htm.embedding_provider', true)").first&.fetch('current_setting', nil) || 'ollama'
        model = conn.exec("SELECT current_setting('htm.embedding_model', true)").first&.fetch('current_setting', nil) || 'nomic-embed-text'
        ollama_host = conn.exec("SELECT current_setting('htm.ollama_url', true)").first&.fetch('current_setting', nil) || 'http://localhost:11434'

        # Generate query embedding using pgai based on current provider
        if provider == 'ollama'
          result = conn.exec_params(
            <<~SQL,
              WITH query_embedding AS (
                SELECT ai.ollama_embed($1, $2, host => $3) as embedding
              )
              SELECT id, key, value, type, category, importance, created_at, robot_id, token_count,
                     1 - (nodes.embedding <=> query_embedding.embedding) as similarity
              FROM nodes, query_embedding
              WHERE created_at BETWEEN $4 AND $5
              ORDER BY nodes.embedding <=> query_embedding.embedding
              LIMIT $6
            SQL
            [model, query, ollama_host, timeframe.begin, timeframe.end, limit]
          )
        else  # openai
          api_key = conn.exec("SELECT current_setting('htm.openai_api_key', true)").first&.fetch('current_setting', nil)
          result = conn.exec_params(
            <<~SQL,
              WITH query_embedding AS (
                SELECT ai.openai_embed($1, $2, api_key => $3) as embedding
              )
              SELECT id, key, value, type, category, importance, created_at, robot_id, token_count,
                     1 - (nodes.embedding <=> query_embedding.embedding) as similarity
              FROM nodes, query_embedding
              WHERE created_at BETWEEN $4 AND $5
              ORDER BY nodes.embedding <=> query_embedding.embedding
              LIMIT $6
            SQL
            [model, query, api_key, timeframe.begin, timeframe.end, limit]
          )
        end

        result.to_a
      end
    end

    # Uncached full-text search
    #
    # @param timeframe [Range] Time range to search
    # @param query [String] Search query
    # @param limit [Integer] Maximum results
    # @return [Array<Hash>] Matching nodes
    #
    def search_fulltext_uncached(timeframe:, query:, limit:)
      with_connection do |conn|
        result = conn.exec_params(
          <<~SQL,
            SELECT id, key, value, type, category, importance, created_at, robot_id, token_count,
                   ts_rank(to_tsvector('english', value), plainto_tsquery('english', $1)) as rank
            FROM nodes
            WHERE created_at BETWEEN $2 AND $3
            AND to_tsvector('english', value) @@ plainto_tsquery('english', $1)
            ORDER BY rank DESC
            LIMIT $4
          SQL
          [query, timeframe.begin, timeframe.end, limit]
        )
        result.to_a
      end
    end

    # Uncached hybrid search
    #
    # Uses pgai to generate query embedding directly in the database.
    # Combines full-text search for candidate selection with vector similarity for ranking.
    #
    # @param timeframe [Range] Time range to search
    # @param query [String] Search query
    # @param limit [Integer] Maximum results
    # @param embedding_service [Object] DEPRECATED - Not used (kept for API compatibility)
    # @param prefilter_limit [Integer] Candidates to consider
    # @return [Array<Hash>] Matching nodes
    #
    def search_hybrid_uncached(timeframe:, query:, limit:, embedding_service: nil, prefilter_limit:)
      # Query embedding is generated in the database using pgai
      with_connection do |conn|
        # Get current embedding configuration
        provider = conn.exec("SELECT current_setting('htm.embedding_provider', true)").first&.fetch('current_setting', nil) || 'ollama'
        model = conn.exec("SELECT current_setting('htm.embedding_model', true)").first&.fetch('current_setting', nil) || 'nomic-embed-text'
        ollama_host = conn.exec("SELECT current_setting('htm.ollama_url', true)").first&.fetch('current_setting', nil) || 'http://localhost:11434'

        # Generate query embedding using pgai based on current provider
        if provider == 'ollama'
          result = conn.exec_params(
            <<~SQL,
              WITH query_embedding AS (
                SELECT ai.ollama_embed($1, $2, host => $3) as embedding
              ),
              candidates AS (
                SELECT id, key, value, type, category, importance, created_at, robot_id, token_count, embedding
                FROM nodes
                WHERE created_at BETWEEN $4 AND $5
                AND to_tsvector('english', value) @@ plainto_tsquery('english', $2)
                LIMIT $6
              )
              SELECT id, key, value, type, category, importance, created_at, robot_id, token_count,
                     1 - (candidates.embedding <=> query_embedding.embedding) as similarity
              FROM candidates, query_embedding
              ORDER BY candidates.embedding <=> query_embedding.embedding
              LIMIT $7
            SQL
            [model, query, ollama_host, timeframe.begin, timeframe.end, prefilter_limit, limit]
          )
        else  # openai
          api_key = conn.exec("SELECT current_setting('htm.openai_api_key', true)").first&.fetch('current_setting', nil)
          result = conn.exec_params(
            <<~SQL,
              WITH query_embedding AS (
                SELECT ai.openai_embed($1, $2, api_key => $3) as embedding
              ),
              candidates AS (
                SELECT id, key, value, type, category, importance, created_at, robot_id, token_count, embedding
                FROM nodes
                WHERE created_at BETWEEN $4 AND $5
                AND to_tsvector('english', value) @@ plainto_tsquery('english', $2)
                LIMIT $6
              )
              SELECT id, key, value, type, category, importance, created_at, robot_id, token_count,
                     1 - (candidates.embedding <=> query_embedding.embedding) as similarity
              FROM candidates, query_embedding
              ORDER BY candidates.embedding <=> query_embedding.embedding
              LIMIT $7
            SQL
            [model, query, api_key, timeframe.begin, timeframe.end, prefilter_limit, limit]
          )
        end

        result.to_a
      end
    end

    # Configure embedding generation settings from environment variables
    #
    # @param conn [PG::Connection] PostgreSQL connection
    # @return [void]
    #
    def configure_embedding_settings(conn)
      provider = ENV.fetch('HTM_EMBEDDINGS_PROVIDER', 'ollama')
      model = ENV.fetch('HTM_EMBEDDINGS_MODEL', 'nomic-embed-text')
      base_url = ENV.fetch('HTM_EMBEDDINGS_BASE_URL', 'http://localhost:11434')
      dimension = ENV.fetch('HTM_EMBEDDINGS_DIMENSION', '768')

      conn.exec_params("SELECT set_config('htm.embedding_provider', $1, false)", [provider])
      conn.exec_params("SELECT set_config('htm.embedding_model', $1, false)", [model])
      conn.exec_params("SELECT set_config('htm.ollama_url', $1, false)", [base_url])
      conn.exec_params("SELECT set_config('htm.embedding_dimension', $1, false)", [dimension])

      # Set OpenAI API key if provided
      if ENV['HTM_OPENAI_API_KEY']
        conn.exec_params("SELECT set_config('htm.openai_api_key', $1, false)", [ENV['HTM_OPENAI_API_KEY']])
      end
    end

    # Configure topic extraction settings from environment variables
    #
    # @param conn [PG::Connection] PostgreSQL connection
    # @return [void]
    #
    def configure_topic_settings(conn)
      provider = ENV.fetch('HTM_TOPIC_PROVIDER', 'ollama')
      model = ENV.fetch('HTM_TOPIC_MODEL', 'llama3')
      base_url = ENV.fetch('HTM_TOPIC_BASE_URL', 'http://localhost:11434')

      conn.exec_params("SELECT set_config('htm.topic_provider', $1, false)", [provider])
      conn.exec_params("SELECT set_config('htm.topic_model', $1, false)", [model])
      conn.exec_params("SELECT set_config('htm.topic_base_url', $1, false)", [base_url])
    end

    def with_connection
      @pool.with do |conn|
        # Pgvector is automatically available after requiring 'pgvector'
        # No explicit registration needed
        yield(conn)
      end
    rescue PG::QueryCanceled => e
      # Query timeout exceeded
      raise HTM::QueryTimeoutError, "Database query exceeded timeout of #{@query_timeout}ms: #{e.message}"
    rescue PG::ConnectionBad, PG::UnableToSend => e
      # Connection issues
      raise HTM::DatabaseError, "Database connection error: #{e.message}"
    rescue PG::Error => e
      # Other PostgreSQL errors
      raise HTM::DatabaseError, "Database error: #{e.message}"
    end
  end
end
