# frozen_string_literal: true

require 'tiktoken_ruby'
require_relative 'errors'

class HTM
  # Embedding Service - Configure pgai for automatic embedding generation
  #
  # HTM now uses pgai (PostgreSQL AI extension) for embedding generation directly
  # in the database. This service configures pgai and provides token counting.
  #
  # Embeddings are automatically generated via database triggers when nodes are
  # inserted or updated. No application-side embedding generation needed.
  #
  # Supported providers (via pgai):
  # - :ollama - Ollama with configurable model (default: nomic-embed-text)
  # - :openai - OpenAI embeddings (requires OPENAI_API_KEY)
  #
  class EmbeddingService
    # Known embedding dimensions for common models
    KNOWN_DIMENSIONS = {
      # Ollama models
      'nomic-embed-text' => 768,
      'all-minilm' => 384,
      'mxbai-embed-large' => 1024,
      'embeddinggemma' => 768,
      'embeddinggemma:latest' => 768,

      # OpenAI models
      'text-embedding-3-small' => 1536,
      'text-embedding-3-large' => 3072,
      'text-embedding-ada-002' => 1536
    }.freeze

    attr_reader :provider, :model, :dimensions, :db_config

    # Initialize embedding service
    #
    # @param provider [Symbol] Embedding provider (:ollama, :openai)
    # @param model [String] Model name (default: 'nomic-embed-text' for ollama)
    # @param ollama_url [String] Ollama server URL (default: http://localhost:11434)
    # @param dimensions [Integer] Expected embedding dimensions (auto-detected if not provided)
    # @param db_config [Hash] Database configuration for setting pgai config
    #
    def initialize(provider = :ollama, model: nil, ollama_url: nil, dimensions: nil, db_config: nil)
      @provider = provider
      @model = model || default_model_for_provider(provider)
      @ollama_url = ollama_url || ENV['OLLAMA_URL'] || 'http://localhost:11434'
      @tokenizer = Tiktoken.encoding_for_model("gpt-3.5-turbo")
      @db_config = db_config

      # Auto-detect dimensions from known models, or use provided value
      @dimensions = dimensions || KNOWN_DIMENSIONS[@model]

      # Warn if we don't know the expected dimensions
      if @dimensions.nil?
        warn "WARNING: Unknown embedding dimensions for model '#{@model}'. Using pgai default."
      end

      # Configure pgai in database if db_config provided
      configure_pgai if @db_config
    end

    # Configure pgai in the database
    #
    # Sets the embedding provider, model, and other parameters via PostgreSQL
    # configuration functions. This affects how the database trigger generates embeddings.
    #
    def configure_pgai
      require 'pg'

      conn = PG.connect(@db_config)

      case @provider
      when :ollama
        conn.exec_params(
          "SELECT htm_set_embedding_config($1, $2, $3, NULL, $4)",
          ['ollama', @model, @ollama_url, @dimensions]
        )
      when :openai
        api_key = ENV['OPENAI_API_KEY']
        unless api_key
          raise HTM::EmbeddingError, "OPENAI_API_KEY environment variable not set"
        end

        conn.exec_params(
          "SELECT htm_set_embedding_config($1, $2, NULL, $3, $4)",
          ['openai', @model, api_key, @dimensions]
        )
      else
        raise HTM::EmbeddingError, "Unknown provider: #{@provider}. Use :ollama or :openai"
      end

      conn.close
    rescue PG::Error => e
      raise HTM::EmbeddingError, "Failed to configure pgai: #{e.message}"
    end

    # Count tokens in text
    #
    # @param text [String] Text to count
    # @return [Integer] Token count
    #
    def count_tokens(text)
      @tokenizer.encode(text.to_s).length
    rescue StandardError
      # Fallback to simple word count if tokenizer fails
      text.to_s.split.size
    end

    # Get embedding dimensions for current model
    #
    # @return [Integer] Embedding dimensions
    #
    def embedding_dimensions
      @dimensions || KNOWN_DIMENSIONS[@model] || 768
    end

    # Generate embedding for text
    #
    # When pgai is available, embeddings are generated by database triggers.
    # When pgai is not available, this method generates embeddings client-side.
    #
    # @param text [String] Text to embed
    # @return [Array<Float>] Embedding vector
    #
    def embed(text)
      case @provider
      when :ollama
        embed_with_ollama(text)
      when :openai
        embed_with_openai(text)
      else
        raise HTM::EmbeddingError, "Unknown provider: #{@provider}"
      end
    end

    private

    def default_model_for_provider(provider)
      case provider
      when :ollama
        'nomic-embed-text'
      when :openai
        'text-embedding-3-small'
      else
        raise HTM::EmbeddingError, "Unknown provider: #{provider}"
      end
    end

    def embed_with_ollama(text)
      require 'net/http'
      require 'json'

      uri = URI("#{@ollama_url}/api/embeddings")
      request = Net::HTTP::Post.new(uri)
      request['Content-Type'] = 'application/json'
      request.body = JSON.generate({
        model: @model,
        prompt: text
      })

      response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: uri.scheme == 'https') do |http|
        http.request(request)
      end

      unless response.is_a?(Net::HTTPSuccess)
        raise HTM::EmbeddingError, "Ollama API error: #{response.code} #{response.message}"
      end

      result = JSON.parse(response.body)
      result['embedding']
    rescue StandardError => e
      raise HTM::EmbeddingError, "Failed to generate embedding with Ollama: #{e.message}"
    end

    def embed_with_openai(text)
      require 'net/http'
      require 'json'

      api_key = ENV['OPENAI_API_KEY']
      unless api_key
        raise HTM::EmbeddingError, "OPENAI_API_KEY environment variable not set"
      end

      uri = URI('https://api.openai.com/v1/embeddings')
      request = Net::HTTP::Post.new(uri)
      request['Content-Type'] = 'application/json'
      request['Authorization'] = "Bearer #{api_key}"
      request.body = JSON.generate({
        model: @model,
        input: text
      })

      response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
        http.request(request)
      end

      unless response.is_a?(Net::HTTPSuccess)
        raise HTM::EmbeddingError, "OpenAI API error: #{response.code} #{response.message}"
      end

      result = JSON.parse(response.body)
      result.dig('data', 0, 'embedding')
    rescue StandardError => e
      raise HTM::EmbeddingError, "Failed to generate embedding with OpenAI: #{e.message}"
    end
  end
end
