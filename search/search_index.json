{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>       \u26a0\ufe0f WARNING \u26a0\ufe0f     </p> <p>       This documentation is AI-generated and may contain hallucinations, inaccuracies, or outdated information.       Always verify critical details in the source code.     </p> <p>A hierarchical and temporal system for encoding, storing, and retrieving information\u2014operating across varying levels of abstraction (from simple to detailed concepts and their relationships) and across time (from the present to the past). </p>"},{"location":"#what-does-it-mean","title":"What does it mean?","text":"<ul> <li>Hierarchical: operates across multiple levels of abstraction, from simple concepts to detailed relationships</li> <li>Temporal: functions across time, from the present moment to historical data</li> <li>Memory function: encodes, stores, and retrieves information</li> </ul> <p>HTM: a hierarchical and temporal memory system that organizes and recalls information at multiple levels of detail over extended timeframes.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#two-tier-memory-architecture","title":"Two-Tier Memory Architecture","text":"<p>HTM implements a realistic memory model inspired by human cognition:</p> <ul> <li>Working Memory: Token-limited active context optimized for immediate LLM consumption</li> <li>Long-term Memory: Durable PostgreSQL storage that persists forever</li> </ul> <p>The two tiers work seamlessly together - working memory pulls from long-term storage as needed, and automatically evicts less important memories back to long-term storage when space is limited.</p>"},{"location":"#never-forget-philosophy","title":"Never-Forget Philosophy","text":"<p>HTM follows a \"never forget unless explicitly told\" principle:</p> <ul> <li>All memories persist in long-term storage indefinitely</li> <li>Working memory evictions move data to long-term storage, never delete it</li> <li>Only explicit <code>forget()</code> commands with confirmation actually delete data</li> <li>Complete audit trail of all memory operations</li> </ul>"},{"location":"#rag-based-retrieval","title":"RAG-Based Retrieval","text":"<p>HTM uses advanced Retrieval-Augmented Generation techniques:</p> <ul> <li>Vector Similarity Search: Semantic search using pgvector with embeddings from Ollama</li> <li>Full-Text Search: PostgreSQL full-text search for keyword matching</li> <li>Hybrid Search: Combines both vector and full-text for best results</li> <li>Temporal Filtering: Natural language time queries like \"last week\" or \"yesterday\"</li> <li>Importance Scoring: Prioritize critical memories over trivial ones</li> </ul>"},{"location":"#multi-robot-hive-mind","title":"Multi-Robot Hive Mind","text":"<p>HTM enables multiple AI robots to share a collective memory:</p> <ul> <li>All robots share access to global long-term memory</li> <li>Track which robot said what and when</li> <li>Cross-robot context awareness and conversation continuity</li> <li>Query conversation timelines across multiple robots</li> </ul>"},{"location":"#knowledge-graph","title":"Knowledge Graph","text":"<p>Build rich relationship networks between memories:</p> <ul> <li>Link related memories together</li> <li>Tag-based categorization</li> <li>Importance scoring for prioritization</li> <li>Navigate memory relationships programmatically</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<p>Here's how simple it is to get started with HTM:</p> <pre><code>require 'htm'\n\n# Initialize HTM for your robot\nhtm = HTM.new(\n  robot_name: \"Code Helper\",\n  working_memory_size: 128_000,    # 128k tokens\n  embedding_service: :ollama,       # Use Ollama for embeddings\n  embedding_model: 'gpt-oss'        # Default embedding model\n)\n\n# Add memories (embeddings generated automatically)\nhtm.add_node(\n  \"decision_001\",\n  \"We decided to use PostgreSQL for HTM storage\",\n  type: :decision,\n  category: \"architecture\",\n  importance: 9.0,\n  tags: [\"database\", \"architecture\"]\n)\n\n# Recall memories from the past\nmemories = htm.recall(\n  timeframe: \"last week\",\n  topic: \"database decisions\",\n  strategy: :hybrid\n)\n\n# Create context for your LLM\ncontext = htm.create_context(\n  strategy: :balanced,\n  max_tokens: 50_000\n)\n\n# Use context in your LLM prompt\nresponse = llm.chat(\n  system: \"You are a helpful assistant. \" \\\n          \"Here's your memory context:\\n#{context}\",\n  user: \"What database did we decide to use?\"\n)\n\n# Explicit deletion only when needed\nhtm.forget(\"old_decision\", confirm: :confirmed)\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":"<p>HTM is perfect for:</p> <ul> <li>AI Coding Assistants: Remember project decisions, code patterns, and user preferences</li> <li>Customer Service Bots: Maintain conversation history and customer context</li> <li>Personal AI Assistants: Remember user preferences, habits, and important information</li> <li>Research Assistants: Build knowledge graphs from documents and conversations</li> <li>Multi-Agent Systems: Enable collaborative memory across multiple AI agents</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>HTM consists of several key components working together:</p> <p></p>"},{"location":"#component-breakdown","title":"Component Breakdown","text":"<ul> <li>HTM API: Main interface for all memory operations</li> <li>WorkingMemory: Token-limited in-memory cache for immediate LLM use</li> <li>LongTermMemory: PostgreSQL-backed durable storage</li> <li>EmbeddingService: Generates vector embeddings via RubyLLM and Ollama</li> <li>Database: Schema management and connection pooling</li> </ul>"},{"location":"#memory-types","title":"Memory Types","text":"<p>HTM supports different categories of memories:</p> <ul> <li><code>:fact</code>: Immutable facts (\"User's name is Dewayne\")</li> <li><code>:context</code>: Conversation state and context</li> <li><code>:code</code>: Code snippets and programming patterns</li> <li><code>:preference</code>: User preferences and settings</li> <li><code>:decision</code>: Architectural and design decisions</li> <li><code>:question</code>: Unresolved questions needing answers</li> </ul> <p>Each type can have custom importance scores, tags, and relationships.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to add intelligent memory to your LLM application? Follow these steps:</p> <ol> <li>Installation: Set up HTM, PostgreSQL, TimescaleDB, and Ollama</li> <li>Quick Start: Build your first HTM-powered application in 5 minutes</li> <li>User Guide: Deep dive into all HTM features</li> <li>API Reference: Complete API documentation</li> </ol>"},{"location":"#community-and-support","title":"Community and Support","text":"<ul> <li>GitHub: https://github.com/madbomber/htm</li> <li>Issues: Report bugs and request features</li> <li>Discussions: Ask questions and share your projects</li> </ul>"},{"location":"#philosophy","title":"Philosophy","text":"<p>HTM is built on several core principles:</p> <ol> <li>Never Forget: Data should persist unless explicitly deleted</li> <li>Context is King: LLMs need rich, relevant context to perform well</li> <li>Time Matters: Recent and temporal context is crucial for AI cognition</li> <li>Relationships Count: Memories are interconnected, not isolated</li> <li>Hive Mind: Multiple agents should share collective intelligence</li> </ol>"},{"location":"#credits","title":"Credits","text":"<p>HTM is developed by Dewayne VanHoozer with design assistance from Claude (Anthropic).</p> <p>Licensed under the MIT License.</p> <p>Next Steps:</p> <ul> <li>Install HTM and set up your environment</li> <li>Follow the Quick Start Guide to build your first application</li> <li>Explore the User Guide for advanced features</li> <li>Check out the API Reference for detailed documentation</li> </ul>"},{"location":"database_rake_tasks/","title":"HTM Database Rake Tasks Reference","text":"<p>Complete reference for HTM database management tasks.</p>"},{"location":"database_rake_tasks/#quick-start","title":"Quick Start","text":"<pre><code># First time setup\ncd /path/to/htm\ndirenv allow                  # Load environment variables from .envrc\nrake htm:db:setup            # Create schema and run migrations\n</code></pre>"},{"location":"database_rake_tasks/#available-tasks","title":"Available Tasks","text":""},{"location":"database_rake_tasks/#setup-and-schema","title":"Setup and Schema","text":""},{"location":"database_rake_tasks/#rake-htmdbsetup","title":"<code>rake htm:db:setup</code>","text":"<p>Sets up the HTM database schema and runs all migrations.</p> <p>What it does: - Verifies required extensions (timescaledb, pgvector, pg_trgm) - Creates all HTM tables (nodes, tags, robots, operations_log) - Runs all pending migrations - Sets up hypertables for time-series optimization</p> <p>When to use: First-time setup or after dropping the database</p> <pre><code>$ rake htm:db:setup\n\u2713 TimescaleDB version: 2.22.1\n\u2713 pgvector version: 0.8.1\nCreating HTM schema...\n\u2713 Schema created\nRunning migration: 001_support_variable_dimensions\n  \u2713 Migration 001_support_variable_dimensions applied\nRunning migration: 002_ontology_topic_extraction\n  \u2713 Migration 002_ontology_topic_extraction applied\n\u2713 Created hypertable for operations_log\n\u2713 HTM database schema created successfully\n</code></pre>"},{"location":"database_rake_tasks/#migrations","title":"Migrations","text":""},{"location":"database_rake_tasks/#rake-htmdbmigrate","title":"<code>rake htm:db:migrate</code>","text":"<p>Runs pending database migrations only.</p> <p>What it does: - Checks which migrations have been applied - Runs any new migrations in <code>sql/migrations/</code> - Updates the <code>schema_migrations</code> table</p> <p>When to use: After pulling new code with migrations</p> <pre><code>$ rake htm:db:migrate\nRunning migration: 002_ontology_topic_extraction\n  \u2713 Migration 002_ontology_topic_extraction applied\n\u2713 Database migrations completed\n</code></pre>"},{"location":"database_rake_tasks/#rake-htmdbstatus","title":"<code>rake htm:db:status</code>","text":"<p>Shows which migrations have been applied and which are pending.</p> <p>Example output: <pre><code>$ rake htm:db:status\n\nMigration Status\n================================================================================\n\u2713 001_support_variable_dimensions (applied: 2025-10-26 04:27:15.428951+00)\n\u2713 002_ontology_topic_extraction (applied: 2025-10-27 03:44:15.012345+00)\n\nSummary: 2 applied, 0 pending\n================================================================================\n</code></pre></p>"},{"location":"database_rake_tasks/#information","title":"Information","text":""},{"location":"database_rake_tasks/#rake-htmdbinfo","title":"<code>rake htm:db:info</code>","text":"<p>Shows comprehensive database information.</p> <p>Example output: <pre><code>$ rake htm:db:info\n\nHTM Database Information\n================================================================================\n\nConnection:\n  Host: cw7rxj91bm.srbbwwxn56.tsdb.cloud.timescale.com\n  Port: 37807\n  Database: tsdb\n  User: tsdbadmin\n\nPostgreSQL Version:\n  PostgreSQL 17.6 (Ubuntu 17.6-2.pgdg22.04+1) on aarch64-unknown-linux-gnu\n\nExtensions:\n  ai (0.11.2)\n  pg_stat_statements (1.11)\n  pg_trgm (1.6)\n  plpgsql (1.0)\n  timescaledb (2.22.1)\n  timescaledb_toolkit (1.21.0)\n  vector (0.8.1)\n  vectorscale (0.8.0)\n\nHTM Tables:\n  nodes: 42 rows\n  tags: 156 rows\n  robots: 3 rows\n  operations_log: 289 rows\n  schema_migrations: 2 rows\n\nDatabase Size: 14 MB\n================================================================================\n</code></pre></p>"},{"location":"database_rake_tasks/#rake-htmdbtest","title":"<code>rake htm:db:test</code>","text":"<p>Tests database connection by running <code>test_connection.rb</code>.</p> <p>Example output: <pre><code>$ rake htm:db:test\nConnecting to TimescaleDB...\n\u2713 Connected successfully!\n\u2713 TimescaleDB Extension: Version 2.22.1\n\u2713 pgvector Extension: Version 0.8.1\n\u2713 pg_trgm Extension: Version 1.6\n</code></pre></p>"},{"location":"database_rake_tasks/#utilities","title":"Utilities","text":""},{"location":"database_rake_tasks/#rake-htmdbconsole","title":"<code>rake htm:db:console</code>","text":"<p>Opens an interactive PostgreSQL console (psql).</p> <p>What it does: - Launches <code>psql</code> connected to your HTM database - Uses connection parameters from <code>HTM_DBURL</code> or <code>.envrc</code> - Allows you to run SQL queries directly</p> <p>Example: <pre><code>$ rake htm:db:console\npsql (17.6)\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off, ALPN: none)\nType \"help\" for help.\n\ntsdb=&gt; SELECT COUNT(*) FROM nodes;\n count\n-------\n    42\n(1 row)\n\ntsdb=&gt; \\d nodes\ntsdb=&gt; \\q\n</code></pre></p>"},{"location":"database_rake_tasks/#rake-htmdbseed","title":"<code>rake htm:db:seed</code>","text":"<p>Seeds the database with sample data.</p> <p>What it does: - Creates a sample robot - Adds 3 sample nodes with different topics - Useful for testing or demos</p> <p>Example: <pre><code>$ rake htm:db:seed\nSeeding database with sample data...\n  Creating sample nodes...\n\u2713 Database seeded with 3 sample nodes\n</code></pre></p>"},{"location":"database_rake_tasks/#destructive-operations","title":"Destructive Operations","text":"<p>\u26a0\ufe0f WARNING: These tasks delete data and cannot be undone!</p>"},{"location":"database_rake_tasks/#rake-htmdbdrop","title":"<code>rake htm:db:drop</code>","text":"<p>Drops all HTM tables, functions, triggers, and views.</p> <p>What it does: - Drops tables: nodes, tags, robots, operations_log, schema_migrations - Drops ontology functions and triggers - Drops views: ontology_structure, topic_relationships</p> <p>Safety: Prompts for confirmation (\"yes\" must be typed)</p> <pre><code>$ rake htm:db:drop\nAre you sure you want to drop all tables? This cannot be undone! (yes/no): yes\nDropping HTM tables...\n  \u2713 Dropped nodes\n  \u2713 Dropped tags\n  \u2713 Dropped robots\n  \u2713 Dropped operations_log\n  \u2713 Dropped schema_migrations\n  \u2713 Dropped ontology functions and triggers\n  \u2713 Dropped ontology views\n\u2713 All HTM tables dropped\n</code></pre>"},{"location":"database_rake_tasks/#rake-htmdbreset","title":"<code>rake htm:db:reset</code>","text":"<p>Drops and recreates the entire database (equivalent to <code>drop</code> + <code>setup</code>).</p> <p>When to use: Development only, to start fresh</p> <pre><code>$ rake htm:db:reset\n# Runs drop (with confirmation) then setup\n</code></pre>"},{"location":"database_rake_tasks/#environment-variables","title":"Environment Variables","text":"<p>All tasks require database configuration via environment variables. Use one of these methods:</p>"},{"location":"database_rake_tasks/#method-1-direnv-recommended","title":"Method 1: direnv (Recommended)","text":"<pre><code># One-time setup\ncd /path/to/htm\ndirenv allow\n\n# Variables are automatically loaded from .envrc\nrake htm:db:info\n</code></pre>"},{"location":"database_rake_tasks/#method-2-manual-export","title":"Method 2: Manual Export","text":"<pre><code>export HTM_DBURL=\"postgresql://user:password@host:port/dbname?sslmode=require\"\nrake htm:db:info\n</code></pre>"},{"location":"database_rake_tasks/#method-3-source-tiger-credentials","title":"Method 3: Source Tiger Credentials","text":"<pre><code>source ~/.bashrc__tiger    # If using TimescaleDB Cloud\nrake htm:db:info\n</code></pre>"},{"location":"database_rake_tasks/#common-workflows","title":"Common Workflows","text":""},{"location":"database_rake_tasks/#initial-project-setup","title":"Initial Project Setup","text":"<pre><code>cd /path/to/htm\ndirenv allow\nbundle install\nrake htm:db:setup\nrake htm:db:seed          # Optional: add sample data\nrake htm:db:info          # Verify setup\n</code></pre>"},{"location":"database_rake_tasks/#after-pulling-new-code","title":"After Pulling New Code","text":"<pre><code>git pull\nrake htm:db:status        # Check for new migrations\nrake htm:db:migrate       # Run pending migrations\n</code></pre>"},{"location":"database_rake_tasks/#development-reset","title":"Development Reset","text":"<pre><code>rake htm:db:reset         # Drop and recreate (type 'yes' to confirm)\nrake htm:db:seed          # Re-add sample data\n</code></pre>"},{"location":"database_rake_tasks/#debugging","title":"Debugging","text":"<pre><code>rake htm:db:info          # Check database state\nrake htm:db:status        # Check migration status\nrake htm:db:console       # Open psql for SQL queries\n</code></pre>"},{"location":"database_rake_tasks/#production-deployment","title":"Production Deployment","text":"<pre><code># NEVER use reset or drop in production!\nrake htm:db:migrate       # Run new migrations only\n</code></pre>"},{"location":"database_rake_tasks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"database_rake_tasks/#database-configuration-not-found","title":"\"Database configuration not found\"","text":"<ul> <li>Run <code>direnv allow</code> in the project directory</li> <li>Or manually export <code>HTM_DBURL</code></li> <li>Verify: <code>echo $HTM_DBURL</code></li> </ul>"},{"location":"database_rake_tasks/#connection-refused","title":"\"Connection refused\"","text":"<ul> <li>Check database is running</li> <li>Verify host/port in <code>HTM_DBURL</code></li> <li>Test: <code>rake htm:db:test</code></li> </ul>"},{"location":"database_rake_tasks/#extension-not-found","title":"\"Extension not found\"","text":"<ul> <li>Ensure TimescaleDB Cloud instance has required extensions</li> <li>Check with: <code>rake htm:db:info</code></li> <li>Extensions needed: timescaledb, pgvector, pg_trgm</li> </ul>"},{"location":"database_rake_tasks/#migrations-not-running","title":"Migrations not running","text":"<ul> <li>Check migration files exist: <code>ls -la sql/migrations/</code></li> <li>Verify migrations table: <code>rake htm:db:console</code> then <code>SELECT * FROM schema_migrations;</code></li> <li>Force re-run: <code>rake htm:db:drop</code> then <code>rake htm:db:setup</code></li> </ul>"},{"location":"database_rake_tasks/#legacy-tasks-deprecated","title":"Legacy Tasks (Deprecated)","text":"<p>These tasks still work but will be removed in a future version:</p> <ul> <li><code>rake db_setup</code> \u2192 Use <code>rake htm:db:setup</code></li> <li><code>rake db_test</code> \u2192 Use <code>rake htm:db:test</code></li> </ul>"},{"location":"multi_framework_support/","title":"HTM Multi-Framework Support","text":"<p>HTM works seamlessly in three types of applications: 1. CLI Applications - Command-line tools with synchronous execution 2. Sinatra Applications - Web apps with Sidekiq background jobs 3. Rails Applications - Full Rails integration with ActiveJob</p>"},{"location":"multi_framework_support/#quick-start-by-framework","title":"Quick Start by Framework","text":""},{"location":"multi_framework_support/#cli-applications","title":"CLI Applications","text":"<pre><code>#!/usr/bin/env ruby\nrequire 'htm'\n\n# Configure for CLI (synchronous execution)\nHTM.configure do |config|\n  config.job_backend = :inline  # Jobs run immediately\nend\n\nhtm = HTM.new(robot_name: \"cli_assistant\")\n\n# Store information (waits for embedding + tags)\nnode_id = htm.remember(\"PostgreSQL is great for time-series data\")\nputs \"Stored as node #{node_id}\"\n\n# Search memories\nmemories = htm.recall(\"PostgreSQL\", limit: 10)\nputs \"Found #{memories.length} memories\"\n</code></pre> <p>Example: <code>examples/cli_app/htm_cli.rb</code></p>"},{"location":"multi_framework_support/#sinatra-applications","title":"Sinatra Applications","text":"<pre><code>require 'sinatra'\nrequire 'htm'\nrequire 'htm/integrations/sinatra'\n\nclass MyApp &lt; Sinatra::Base\n  # Automatically configures HTM with Sidekiq\n  register_htm\n\n  enable :sessions\n\n  before do\n    init_htm(robot_name: session[:user_id] || 'guest')\n  end\n\n  post '/remember' do\n    node_id = remember(params[:content])\n    json status: 'ok', node_id: node_id\n  end\n\n  get '/recall' do\n    memories = recall(params[:topic], limit: 10)\n    json memories: memories\n  end\nend\n</code></pre> <p>Example: <code>examples/sinatra_app/app.rb</code></p>"},{"location":"multi_framework_support/#rails-applications","title":"Rails Applications","text":"<pre><code># HTM automatically configures itself in Rails\n\n# app/controllers/memories_controller.rb\nclass MemoriesController &lt; ApplicationController\n  def create\n    htm = HTM.new(robot_name: \"user_#{current_user.id}\")\n    node_id = htm.remember(params[:content], source: 'user')\n\n    render json: { status: 'ok', node_id: node_id }\n  end\n\n  def index\n    htm = HTM.new(robot_name: \"user_#{current_user.id}\")\n    memories = htm.recall(params[:topic], limit: 10)\n\n    render json: { memories: memories }\n  end\nend\n</code></pre> <p>Rails auto-configuration happens via <code>HTM::Railtie</code>: - Uses Rails.logger - Uses ActiveJob for background jobs - Inline jobs in test environment - Rake tasks auto-loaded</p>"},{"location":"multi_framework_support/#job-backend-comparison","title":"Job Backend Comparison","text":"Backend Best For Speed Infrastructure Use Case <code>:inline</code> CLI, Tests Slow (synchronous) None Development, testing, CLI tools <code>:thread</code> Simple apps Fast None Quick prototypes, standalone <code>:sidekiq</code> Sinatra Fast Redis required Microservices, Sinatra apps <code>:active_job</code> Rails Fast Rails required Rails applications"},{"location":"multi_framework_support/#performance-characteristics","title":"Performance Characteristics","text":"<p>:inline (Synchronous) <pre><code># User waits for completion\nnode_id = htm.remember(\"text\")  # ~1-3 seconds\n# Embedding and tags already generated\n</code></pre></p> <p>:sidekiq/:active_job (Asynchronous) <pre><code># User gets immediate response\nnode_id = htm.remember(\"text\")  # ~15ms\n# Embedding and tags generated in background (~1 second)\n</code></pre></p>"},{"location":"multi_framework_support/#configuration","title":"Configuration","text":""},{"location":"multi_framework_support/#auto-detection","title":"Auto-Detection","text":"<p>HTM automatically detects the appropriate backend:</p> <pre><code># Test environment \u2192 :inline\nENV['RAILS_ENV'] = 'test'\nHTM.configuration.job_backend  # =&gt; :inline\n\n# Rails app \u2192 :active_job\ndefined?(ActiveJob)\nHTM.configuration.job_backend  # =&gt; :active_job\n\n# Sidekiq available \u2192 :sidekiq\ndefined?(Sidekiq)\nHTM.configuration.job_backend  # =&gt; :sidekiq\n\n# Default \u2192 :thread\nHTM.configuration.job_backend  # =&gt; :thread\n</code></pre>"},{"location":"multi_framework_support/#manual-override","title":"Manual Override","text":"<pre><code>HTM.configure do |config|\n  config.job_backend = :inline  # Force synchronous\nend\n</code></pre>"},{"location":"multi_framework_support/#environment-variable","title":"Environment Variable","text":"<pre><code>export HTM_JOB_BACKEND=inline  # Override auto-detection\n</code></pre>"},{"location":"multi_framework_support/#framework-specific-features","title":"Framework-Specific Features","text":""},{"location":"multi_framework_support/#cli-applications_1","title":"CLI Applications","text":"<p>Features: - Synchronous execution (<code>:inline</code> backend) - Progress feedback in terminal - No background infrastructure needed - Simple error handling</p> <p>Best Practices: <pre><code># Use inline backend\nHTM.configure do |config|\n  config.job_backend = :inline\n\n  # CLI-friendly logging\n  config.logger.formatter = proc do |severity, datetime, progname, msg|\n    case severity\n    when 'INFO'  then \"[\u2713] #{msg}\\n\"\n    when 'ERROR' then \"[\u2717] #{msg}\\n\"\n    else \"[\u2022] #{msg}\\n\"\n    end\n  end\nend\n</code></pre></p>"},{"location":"multi_framework_support/#sinatra-applications_1","title":"Sinatra Applications","text":"<p>Features: - Sidekiq background jobs - Session-based robot identification - Thread-safe request handling - RESTful API integration</p> <p>Setup: <pre><code># Gemfile\ngem 'sinatra'\ngem 'sidekiq'\ngem 'redis'\ngem 'htm'\n\n# app.rb\nrequire 'htm/integrations/sinatra'\n\nclass MyApp &lt; Sinatra::Base\n  register_htm  # Auto-configures HTM\n\n  enable :sessions\n\n  before do\n    robot_name = session[:user_id] || 'guest'\n    init_htm(robot_name: \"user_#{robot_name}\")\n  end\nend\n</code></pre></p> <p>Deployment: <pre><code># Start Redis\nredis-server\n\n# Start Sidekiq worker\nbundle exec sidekiq -r ./app.rb\n\n# Start web server\nbundle exec ruby app.rb\n</code></pre></p>"},{"location":"multi_framework_support/#rails-applications_1","title":"Rails Applications","text":"<p>Features: - Automatic configuration via Railtie - ActiveJob integration - Rails logger integration - Rake tasks loaded automatically - Test environment auto-configured</p> <p>Setup: <pre><code># Gemfile\ngem 'htm'\n\n# config/initializers/htm.rb (optional)\nHTM.configure do |config|\n  config.embedding_model = 'nomic-embed-text'\n  config.tag_model = 'llama3'\nend\n</code></pre></p> <p>Usage in Controllers: <pre><code>class MemoriesController &lt; ApplicationController\n  def create\n    htm = HTM.new(robot_name: \"user_#{current_user.id}\")\n    node_id = htm.remember(params[:content])\n\n    # Job enqueued via ActiveJob\n    # Returns immediately\n\n    render json: { node_id: node_id }\n  end\nend\n</code></pre></p> <p>Testing: <pre><code># test/test_helper.rb or spec/rails_helper.rb\n\n# Jobs run synchronously in tests (auto-configured)\nRSpec.describe MemoriesController do\n  it \"creates memory\" do\n    post :create, params: { content: \"Test memory\" }\n\n    # Embedding and tags already generated (inline in tests)\n    node = HTM::Models::Node.last\n    expect(node.embedding).to be_present\n    expect(node.tags).not_to be_empty\n  end\nend\n</code></pre></p>"},{"location":"multi_framework_support/#thread-safety","title":"Thread Safety","text":"<p>HTM is thread-safe for concurrent web requests:</p> <p>\u2705 Thread-Safe Components: - <code>HTM::WorkingMemory</code> - Per-instance state - <code>HTM::LongTermMemory</code> - Connection pooling - Database operations - PostgreSQL ACID compliance - Job enqueueing - Atomic operations</p> <p>\u26a0\ufe0f Considerations: - Each HTM instance is independent - Connection pool sized appropriately (<code>db_pool_size</code>) - Concurrent node creation is safe - Shared memory across robots (by design)</p> <p>Example: Concurrent Requests <pre><code># Sinatra/Rails - Multiple requests simultaneously\n# Request 1:\nhtm1 = HTM.new(robot_name: \"user_123\")\nhtm1.remember(\"Message 1\")  # \u2713 Thread-safe\n\n# Request 2 (concurrent):\nhtm2 = HTM.new(robot_name: \"user_456\")\nhtm2.remember(\"Message 2\")  # \u2713 Thread-safe\n\n# Separate instances, no conflicts\n</code></pre></p>"},{"location":"multi_framework_support/#database-connection-management","title":"Database Connection Management","text":""},{"location":"multi_framework_support/#cli-applications_2","title":"CLI Applications","text":"<pre><code># Single connection, simple usage\nhtm = HTM.new\n# Connection established once\n</code></pre>"},{"location":"multi_framework_support/#sinatra-applications_2","title":"Sinatra Applications","text":"<pre><code># Connection pooling handled by middleware\nclass MyApp &lt; Sinatra::Base\n  use HTM::Sinatra::Middleware  # Manages connections\nend\n</code></pre>"},{"location":"multi_framework_support/#rails-applications_2","title":"Rails Applications","text":"<pre><code># Rails manages connections automatically\n# HTM shares Rails' connection pool\n</code></pre>"},{"location":"multi_framework_support/#connection-pool-settings","title":"Connection Pool Settings","text":"<pre><code>htm = HTM.new(\n  db_pool_size: 10,         # Max connections\n  db_query_timeout: 30_000  # 30 seconds\n)\n</code></pre>"},{"location":"multi_framework_support/#troubleshooting","title":"Troubleshooting","text":""},{"location":"multi_framework_support/#jobs-not-running-sinatra","title":"Jobs Not Running (Sinatra)","text":"<p>Problem: Memories created but no embeddings/tags</p> <p>Solution: <pre><code># Check Sidekiq is running\nps aux | grep sidekiq\n\n# Start Sidekiq worker\nbundle exec sidekiq -r ./app.rb -q htm\n\n# Check Redis\nredis-cli ping\n</code></pre></p>"},{"location":"multi_framework_support/#jobs-not-running-rails","title":"Jobs Not Running (Rails)","text":"<p>Problem: Background jobs not processing</p> <p>Solution: <pre><code># Check ActiveJob backend configured\n# config/application.rb\nconfig.active_job.queue_adapter = :sidekiq\n\n# Start Sidekiq\nbundle exec sidekiq\n</code></pre></p>"},{"location":"multi_framework_support/#slow-cli-performance","title":"Slow CLI Performance","text":"<p>Problem: CLI operations take too long</p> <p>Solution: <pre><code># Use faster/smaller models\nHTM.configure do |config|\n  config.embedding_model = 'all-minilm'  # Smaller, faster\n  config.tag_model = 'gemma2:2b'         # Smaller model\nend\n\n# Or disable features\nHTM.configure do |config|\n  config.tag_extractor = -&gt;(_text, _ontology) { [] }  # Skip tags\nend\n</code></pre></p>"},{"location":"multi_framework_support/#thread-safety-issues","title":"Thread Safety Issues","text":"<p>Problem: Concurrent request errors</p> <p>Solution: <pre><code># Increase connection pool\nhtm = HTM.new(db_pool_size: 20)\n\n# Check for shared state (anti-pattern)\n# DON'T:\n$htm = HTM.new  # Global shared instance\n\n# DO:\ndef htm\n  @htm ||= HTM.new(robot_name: current_user.id)\nend\n</code></pre></p>"},{"location":"multi_framework_support/#migration-guide","title":"Migration Guide","text":""},{"location":"multi_framework_support/#existing-cli-apps","title":"Existing CLI Apps","text":"<pre><code># Before (blocking):\n# Jobs run in threads (may not complete)\n\n# After (explicit inline):\nHTM.configure do |config|\n  config.job_backend = :inline\nend\n# Jobs run synchronously, guaranteed to complete\n</code></pre>"},{"location":"multi_framework_support/#existing-sinatra-apps","title":"Existing Sinatra Apps","text":"<pre><code># Before:\nrequire 'htm'\n# Threads used (not production-ready)\n\n# After:\nrequire 'htm/integrations/sinatra'\nregister_htm  # Auto-configures Sidekiq\n# Production-ready background jobs\n</code></pre>"},{"location":"multi_framework_support/#existing-rails-apps","title":"Existing Rails Apps","text":"<pre><code># Before:\n# Manual configuration required\n\n# After:\n# Just add gem 'htm' - auto-configures via Railtie\n# Uses ActiveJob automatically\n</code></pre>"},{"location":"multi_framework_support/#best-practices","title":"Best Practices","text":""},{"location":"multi_framework_support/#cli-applications_3","title":"CLI Applications","text":"<ol> <li>Use <code>:inline</code> backend for predictability</li> <li>Add progress indicators for user feedback</li> <li>Handle Ollama connection errors gracefully</li> <li>Consider caching for repeated queries</li> </ol>"},{"location":"multi_framework_support/#sinatra-applications_3","title":"Sinatra Applications","text":"<ol> <li>Use <code>register_htm</code> for auto-configuration</li> <li>Always use sessions for robot identification</li> <li>Run Sidekiq workers in production</li> <li>Monitor Redis memory usage</li> </ol>"},{"location":"multi_framework_support/#rails-applications_3","title":"Rails Applications","text":"<ol> <li>Create initializer for custom configuration</li> <li>Use per-user robot names</li> <li>Let Rails manage database connections</li> <li>Use ActiveJob for all background processing</li> </ol>"},{"location":"multi_framework_support/#examples","title":"Examples","text":"<p>See working examples in the repository:</p> <ul> <li>CLI: <code>examples/cli_app/htm_cli.rb</code></li> <li>Sinatra: <code>examples/sinatra_app/app.rb</code></li> <li>Rails: <code>examples/rails_app/</code> (full Rails 7 app)</li> </ul>"},{"location":"multi_framework_support/#summary","title":"Summary","text":"Feature CLI Sinatra Rails Job Backend inline sidekiq active_job Setup Complexity Low Medium Low (auto) Infrastructure Database only +Redis +Rails Response Time Slow (1-3s) Fast (15ms) Fast (15ms) Production Ready \u2713 (small scale) \u2713 \u2713 Background Jobs No Yes Yes Auto-Configuration Manual <code>register_htm</code> Railtie <p>Recommendation: - CLI tools \u2192 Use <code>:inline</code> backend - Sinatra apps \u2192 Use <code>:sidekiq</code> backend - Rails apps \u2192 Use <code>:active_job</code> backend (default)</p>"},{"location":"setup_local_database/","title":"Setting Up Local PostgreSQL Database for HTM","text":"<p>This guide walks through setting up a local PostgreSQL database with all required extensions for HTM development.</p>"},{"location":"setup_local_database/#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS with Homebrew installed</li> <li>PostgreSQL 14+ (PostgreSQL 17.6 recommended)</li> <li>Ollama running locally at <code>http://localhost:11434</code></li> </ul>"},{"location":"setup_local_database/#why-local-database","title":"Why Local Database?","text":"<p>For local development, you'll want a PostgreSQL database on your machine for faster development and testing. HTM generates embeddings client-side using Ollama before inserting into the database.</p>"},{"location":"setup_local_database/#step-1-install-postgresql","title":"Step 1: Install PostgreSQL","text":"<p>If you don't have PostgreSQL installed:</p> <pre><code>brew install postgresql@17\nbrew services start postgresql@17\n</code></pre> <p>Verify installation:</p> <pre><code>psql --version\n# Should show: psql (PostgreSQL) 17.x (Homebrew)\n</code></pre>"},{"location":"setup_local_database/#step-2-install-required-extensions","title":"Step 2: Install Required Extensions","text":""},{"location":"setup_local_database/#21-install-pgvector-vector-similarity-search","title":"2.1 Install pgvector (Vector Similarity Search)","text":"<pre><code>brew install pgvector\n</code></pre>"},{"location":"setup_local_database/#22-install-timescaledb-time-series-database","title":"2.2 Install TimescaleDB (Time-Series Database)","text":"<pre><code># Add TimescaleDB tap\nbrew tap timescale/tap\n\n# Install TimescaleDB\nbrew install timescaledb\n\n# Configure PostgreSQL for TimescaleDB\n# This updates your postgresql.conf with TimescaleDB settings\ntimescaledb-tune --quiet --yes\n\n# Restart PostgreSQL to load TimescaleDB\nbrew services restart postgresql@17\n</code></pre>"},{"location":"setup_local_database/#23-pg_trgm-trigram-matching","title":"2.3 pg_trgm (Trigram Matching)","text":"<p>This extension is included with PostgreSQL, no installation needed.</p>"},{"location":"setup_local_database/#step-3-configure-environment","title":"Step 3: Configure Environment","text":"<p>Update your <code>.envrc</code> file (already done):</p> <pre><code># Database connection - Localhost PostgreSQL\nexport HTM_DBHOST=localhost\nexport HTM_DBPORT=5432\nexport HTM_DBNAME=htm_development\nexport HTM_DBUSER=${USER}\nexport HTM_DBPASS=\nexport HTM_DBURL=\"postgresql://${HTM_DBUSER}@${HTM_DBHOST}:${HTM_DBPORT}/${HTM_DBNAME}?sslmode=prefer\"\n\n# Client-side embedding generation\nexport HTM_EMBEDDINGS_PROVIDER=ollama\nexport HTM_EMBEDDINGS_MODEL=embeddinggemma\nexport HTM_EMBEDDINGS_BASE_URL=http://localhost:11434\nexport HTM_EMBEDDINGS_DIMENSION=768\n</code></pre> <p>Reload environment:</p> <pre><code>cd /path/to/htm\ndirenv allow\n</code></pre>"},{"location":"setup_local_database/#step-4-create-database","title":"Step 4: Create Database","text":"<pre><code>createdb htm_development\n</code></pre>"},{"location":"setup_local_database/#step-5-enable-extensions","title":"Step 5: Enable Extensions","text":"<pre><code># Enable pgvector\npsql -d htm_development -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n\n# Enable TimescaleDB\npsql -d htm_development -c \"CREATE EXTENSION IF NOT EXISTS timescaledb;\"\n\n# Enable pg_trgm (trigram matching)\npsql -d htm_development -c \"CREATE EXTENSION IF NOT EXISTS pg_trgm;\"\n</code></pre>"},{"location":"setup_local_database/#step-6-run-htm-database-setup","title":"Step 6: Run HTM Database Setup","text":"<pre><code>be rake htm:db:setup\n</code></pre> <p>This will: 1. Verify extensions are available 2. Create HTM schema (tables, indexes, triggers) 3. Set up TimescaleDB hypertables 4. Run any pending migrations</p> <p>Expected output:</p> <pre><code>\u2713 TimescaleDB version: X.X.X\n\u2713 pgvector version: X.X.X\n\u2713 pg_trgm version: X.X.X\nCreating HTM schema...\n\u2713 Schema created\n\u2713 Created hypertable for operations_log\n\u2713 Created hypertable for nodes\n\u2713 Enabled compression for nodes older than 30 days\n\u2713 HTM database schema created successfully\n</code></pre>"},{"location":"setup_local_database/#step-7-test-with-sample-data","title":"Step 7: Test with Sample Data","text":"<pre><code>be rake htm:db:seed\n</code></pre> <p>This will: 1. Initialize HTM with real EmbeddingService 2. Create 6 sample conversation messages 3. Generate embeddings client-side using your local Ollama</p> <p>Expected output:</p> <pre><code>Seeding database with sample data...\nNote: This requires Ollama to be running locally for embedding generation.\n\n  Creating sample conversation...\n\u2713 Database seeded with 6 conversation messages (3 exchanges)\n</code></pre>"},{"location":"setup_local_database/#available-rake-tasks","title":"Available Rake Tasks","text":"<pre><code>rake htm:db:setup      # Set up database schema and run migrations\nrake htm:db:migrate    # Run pending migrations\nrake htm:db:status     # Show migration status\nrake htm:db:drop       # Drop all HTM tables (WARNING: destructive!)\nrake htm:db:reset      # Drop and recreate database\nrake htm:db:test       # Test database connection\nrake htm:db:console    # Open PostgreSQL console\nrake htm:db:seed       # Seed database with sample data\nrake htm:db:info       # Show database info (size, tables, extensions)\n</code></pre>"},{"location":"setup_local_database/#troubleshooting","title":"Troubleshooting","text":""},{"location":"setup_local_database/#error-type-vector-does-not-exist","title":"Error: \"type 'vector' does not exist\"","text":"<p>Problem: pgvector extension not installed or not enabled.</p> <p>Solution: <pre><code>brew install pgvector\npsql -d htm_development -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n</code></pre></p>"},{"location":"setup_local_database/#error-timescaledb-extension-not-found","title":"Error: \"TimescaleDB extension not found\"","text":"<p>Problem: TimescaleDB not installed or not enabled.</p> <p>Solution: <pre><code>brew tap timescale/tap\nbrew install timescaledb\ntimescaledb-tune --quiet --yes\nbrew services restart postgresql@17\npsql -d htm_development -c \"CREATE EXTENSION IF NOT EXISTS timescaledb;\"\n</code></pre></p>"},{"location":"setup_local_database/#error-connection-refused-to-ollama","title":"Error: \"Connection refused\" to Ollama","text":"<p>Problem: Ollama not running or not accessible.</p> <p>Solution: <pre><code># Check Ollama is running\ncurl http://localhost:11434/api/tags\n\n# Start Ollama if not running\nollama serve\n</code></pre></p>"},{"location":"setup_local_database/#error-database-configuration-not-found","title":"Error: \"Database configuration not found\"","text":"<p>Problem: Environment variables not loaded.</p> <p>Solution: <pre><code>direnv allow\necho $HTM_DBURL  # Verify it's set\n</code></pre></p>"},{"location":"setup_local_database/#switching-back-to-timescaledb-cloud","title":"Switching Back to TimescaleDB Cloud","text":"<p>To switch back to TimescaleDB Cloud (production), edit <code>.envrc</code>:</p> <pre><code># Comment out localhost config\n# export HTM_DBHOST=localhost\n# export HTM_DBPORT=5432\n# export HTM_DBNAME=htm_development\n# export HTM_DBUSER=${USER}\n# export HTM_DBPASS=\n# export HTM_DBURL=\"postgresql://${HTM_DBUSER}@${HTM_DBHOST}:${HTM_DBPORT}/${HTM_DBNAME}?sslmode=prefer\"\n\n# Uncomment TimescaleDB Cloud config\nexport HTM_SERVICE_NAME=$TIGER_SERVICE_NAME\nexport HTM_DBURL=$TIGER_DBURL\nexport HTM_DBNAME=$TIGER_DBNAME\nexport HTM_DBUSER=$TIGER_DBUSER\nexport HTM_DBPASS=$TIGER_DBPASS\nexport HTM_DBHOST=$TIGER_DBHOST\nexport HTM_DBPORT=$TIGER_DBPORT\n</code></pre> <p>Then reload: <pre><code>direnv allow\n</code></pre></p>"},{"location":"setup_local_database/#verifying-setup","title":"Verifying Setup","text":"<p>Check database info:</p> <pre><code>be rake htm:db:info\n</code></pre> <p>Should show:</p> <pre><code>HTM Database Information\n================================================================================\n\nConnection:\n  Host: localhost\n  Port: 5432\n  Database: htm_development\n  User: dewayne\n\nPostgreSQL Version:\n  PostgreSQL 17.6\n\nExtensions:\n  pg_trgm (X.X.X)\n  plpgsql (X.X.X)\n  timescaledb (X.X.X)\n  vector (X.X.X)\n\nHTM Tables:\n  nodes: X rows\n  tags: X rows\n  robots: X rows\n  operations_log: X rows\n  schema_migrations: X rows\n\nDatabase Size: XX MB\n================================================================================\n</code></pre>"},{"location":"setup_local_database/#next-steps","title":"Next Steps","text":"<p>Once your local database is set up:</p> <ol> <li>Run tests: <code>rake test</code></li> <li>Start using HTM in your application</li> <li>Embeddings will be generated client-side using Ollama</li> <li>Check operations_log table to see all HTM operations</li> </ol>"},{"location":"setup_local_database/#architecture-notes","title":"Architecture Notes","text":"<p>With this setup:</p> <ul> <li>PostgreSQL runs on your localhost</li> <li>Ollama runs on your localhost at port 11434</li> <li>HTM Ruby client connects to both PostgreSQL and Ollama</li> <li>Embeddings are generated client-side before database insertion</li> <li>Simple, reliable architecture that works on all platforms</li> </ul> <p>This is the ideal development environment for HTM.</p>"},{"location":"using_rake_tasks_in_your_app/","title":"Using HTM Rake Tasks in Your Application","text":"<p>HTM provides a set of database management rake tasks that can be easily integrated into any application using the gem.</p>"},{"location":"using_rake_tasks_in_your_app/#quick-setup","title":"Quick Setup","text":"<p>Add this single line to your application's <code>Rakefile</code>:</p> <pre><code>require 'htm/tasks'\n</code></pre> <p>That's it! All HTM database tasks are now available in your application.</p>"},{"location":"using_rake_tasks_in_your_app/#example-rakefile","title":"Example Rakefile","text":"<pre><code># Your application's Rakefile\n\nrequire 'bundler/setup'\n\n# Load HTM database tasks\nrequire 'htm/tasks'\n\n# Your application's custom tasks\nnamespace :app do\n  desc \"Run your app\"\n  task :run do\n    # your code here\n  end\nend\n\ntask default: :run\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#available-tasks","title":"Available Tasks","text":"<p>Once <code>require 'htm/tasks'</code> is added, you have access to all HTM database tasks:</p> <pre><code># List all HTM tasks\nrake -T htm\n\n# Database management\nrake htm:db:setup      # Set up HTM database schema and run migrations\nrake htm:db:migrate    # Run pending database migrations\nrake htm:db:status     # Show migration status\nrake htm:db:info       # Show database info\nrake htm:db:test       # Test database connection\nrake htm:db:console    # Open PostgreSQL console\nrake htm:db:seed       # Seed database with sample data\nrake htm:db:drop       # Drop all HTM tables (destructive!)\nrake htm:db:reset      # Drop and recreate database (destructive!)\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#environment-configuration","title":"Environment Configuration","text":"<p>HTM tasks require database configuration via environment variables. You have several options:</p>"},{"location":"using_rake_tasks_in_your_app/#option-1-using-direnv-recommended","title":"Option 1: Using direnv (Recommended)","text":"<p>Create a <code>.envrc</code> file in your application's root:</p> <pre><code># .envrc\nexport HTM_DBURL=\"postgresql://user:password@host:port/dbname?sslmode=require\"\n\n# Or use individual parameters\nexport HTM_DBHOST=\"your-host.tsdb.cloud.timescale.com\"\nexport HTM_DBPORT=\"37807\"\nexport HTM_DBNAME=\"tsdb\"\nexport HTM_DBUSER=\"tsdbadmin\"\nexport HTM_DBPASS=\"your_password\"\n\n# Embedding configuration\nexport HTM_EMBEDDINGS_PROVIDER=ollama\nexport HTM_EMBEDDINGS_MODEL=nomic-embed-text\nexport HTM_EMBEDDINGS_BASE_URL=http://localhost:11434\nexport HTM_EMBEDDINGS_DIMENSION=768\n\n# Topic extraction configuration\nexport HTM_TOPIC_PROVIDER=ollama\nexport HTM_TOPIC_MODEL=llama3\nexport HTM_TOPIC_BASE_URL=http://localhost:11434\n</code></pre> <p>Then enable direnv:</p> <pre><code>direnv allow\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#option-2-export-in-shell","title":"Option 2: Export in Shell","text":"<pre><code>export HTM_DBURL=\"postgresql://user:password@host:port/dbname?sslmode=require\"\nrake htm:db:info\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#option-3-in-your-rakefile","title":"Option 3: In Your Rakefile","text":"<pre><code># Rakefile\n\n# Set environment variables programmatically\nENV['HTM_DBURL'] = \"postgresql://user:password@host:port/dbname?sslmode=require\"\n\n# Then load tasks\nrequire 'htm/tasks'\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#option-4-use-dotenv-gem","title":"Option 4: Use dotenv gem","text":"<pre><code># Gemfile\ngem 'dotenv'\n\n# Rakefile\nrequire 'dotenv/load'  # Loads .env file\nrequire 'htm/tasks'\n</code></pre> <pre><code># .env\nHTM_DBURL=postgresql://user:password@host:port/dbname?sslmode=require\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#real-world-example","title":"Real-World Example","text":"<p>Here's a complete example for a Rails-like application:</p> <pre><code># Rakefile for MyApp\n\nrequire 'bundler/setup'\nrequire 'dotenv/load'  # Load .env file\n\n# Load HTM database tasks\nrequire 'htm/tasks'\n\n# Application tasks\nnamespace :app do\n  desc \"Start the application\"\n  task :start do\n    require_relative 'lib/my_app'\n    MyApp.start\n  end\n\n  desc \"Run database migrations and start app\"\n  task :bootstrap =&gt; ['htm:db:setup', :start]\nend\n\n# Default task\ntask default: 'app:start'\n\n# Development helper\nnamespace :dev do\n  desc \"Reset database and restart (development only!)\"\n  task :reset =&gt; ['htm:db:reset', 'htm:db:seed', 'app:start']\nend\n</code></pre> <p>Usage:</p> <pre><code># First time setup\nrake app:bootstrap              # Sets up HTM database + starts app\n\n# Development reset\nrake dev:reset                  # Drops/recreates database + seeds data\n\n# Normal start\nrake                            # Runs default task (app:start)\n\n# Database management\nrake htm:db:info                # Check database status\nrake htm:db:migrate             # Run new migrations\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#task-composition","title":"Task Composition","text":"<p>You can compose HTM tasks with your own:</p> <pre><code># Your Rakefile\n\nrequire 'htm/tasks'\n\nnamespace :deploy do\n  desc \"Deploy application to production\"\n  task :production do\n    # Run HTM migrations first\n    Rake::Task['htm:db:migrate'].invoke\n\n    # Then deploy your app\n    sh \"git push production main\"\n    sh \"ssh production 'systemctl restart myapp'\"\n  end\nend\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#rails-integration","title":"Rails Integration","text":"<p>For Rails applications, add to your <code>Rakefile</code>:</p> <pre><code># Rakefile (Rails)\n\nrequire_relative 'config/application'\n\nRails.application.load_tasks\n\n# Load HTM tasks\nrequire 'htm/tasks'\n</code></pre> <p>Now you have both Rails tasks and HTM tasks:</p> <pre><code>rake db:migrate            # Rails migrations\nrake htm:db:migrate        # HTM migrations\n\nrake db:seed               # Rails seed\nrake htm:db:seed           # HTM seed\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#sinatra-integration","title":"Sinatra Integration","text":"<pre><code># Rakefile (Sinatra)\n\nrequire './app'  # Your Sinatra app\nrequire 'htm/tasks'\n\nnamespace :server do\n  desc \"Start Sinatra server\"\n  task :start do\n    App.run!\n  end\nend\n\ntask default: 'server:start'\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#testing-integration","title":"Testing Integration","text":"<p>Add HTM database setup to your test tasks:</p> <pre><code># Rakefile\n\nrequire 'rake/testtask'\nrequire 'htm/tasks'\n\nRake::TestTask.new(:test) do |t|\n  t.libs &lt;&lt; \"test\"\n  t.test_files = FileList['test/**/*_test.rb']\nend\n\n# Set up test database before running tests\ntask :test =&gt; 'htm:db:setup'\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>Example GitHub Actions workflow:</p> <pre><code># .github/workflows/test.yml\n\nname: Tests\non: [push]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: timescale/timescaledb-ha:pg17\n        env:\n          POSTGRES_PASSWORD: postgres\n        options: &gt;-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Set up Ruby\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: 3.3\n          bundler-cache: true\n\n      - name: Setup database\n        env:\n          HTM_DBURL: postgresql://postgres:postgres@localhost:5432/test\n        run: |\n          bundle exec rake htm:db:setup\n\n      - name: Run tests\n        env:\n          HTM_DBURL: postgresql://postgres:postgres@localhost:5432/test\n        run: |\n          bundle exec rake test\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#docker-integration","title":"Docker Integration","text":"<p>In your <code>docker-compose.yml</code>:</p> <pre><code>services:\n  app:\n    build: .\n    environment:\n      - HTM_DBURL=postgresql://postgres:postgres@db:5432/myapp\n    depends_on:\n      - db\n    command: bash -c \"rake htm:db:setup &amp;&amp; rake app:start\"\n\n  db:\n    image: timescale/timescaledb-ha:pg17\n    environment:\n      - POSTGRES_PASSWORD=postgres\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#troubleshooting","title":"Troubleshooting","text":""},{"location":"using_rake_tasks_in_your_app/#tasks-not-available","title":"Tasks not available","text":"<pre><code>rake -T\n# If you don't see htm:db tasks, check:\n</code></pre> <ol> <li>Verify <code>require 'htm/tasks'</code> is in your Rakefile</li> <li>Check that <code>htm</code> gem is in your Gemfile</li> <li>Run <code>bundle install</code></li> </ol>"},{"location":"using_rake_tasks_in_your_app/#database-not-configured","title":"Database not configured","text":"<pre><code>Error: Database configuration not found\n</code></pre> <p>Solution: Set <code>HTM_DBURL</code> environment variable</p> <pre><code>export HTM_DBURL=\"postgresql://user:password@host:port/dbname\"\nrake htm:db:info\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#permission-errors","title":"Permission errors","text":"<pre><code>Error: permission denied for table nodes\n</code></pre> <p>Solution: Ensure your database user has proper permissions</p> <pre><code>GRANT ALL ON ALL TABLES IN SCHEMA public TO your_user;\nGRANT ALL ON ALL SEQUENCES IN SCHEMA public TO your_user;\n</code></pre>"},{"location":"using_rake_tasks_in_your_app/#best-practices","title":"Best Practices","text":"<ol> <li>Always use environment variables for database configuration (never hardcode credentials)</li> <li>Use <code>htm:db:migrate</code> instead of <code>htm:db:setup</code> in production (setup drops tables)</li> <li>Run <code>htm:db:status</code> before deploying to check migration state</li> <li>Never use <code>htm:db:reset</code> or <code>htm:db:drop</code> in production</li> <li>Compose tasks rather than duplicating functionality</li> <li>Test migrations on staging before production</li> </ol>"},{"location":"using_rake_tasks_in_your_app/#see-also","title":"See Also","text":"<ul> <li>Database Rake Tasks Reference - Complete task documentation</li> <li>README.md - HTM gem overview</li> <li>SETUP.md - Initial setup guide</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for HTM (Hierarchical Temporary Memory).</p>"},{"location":"api/#overview","title":"Overview","text":"<p>HTM is a two-tier intelligent memory management system for LLM-based robots:</p> <ul> <li>Working Memory: Token-limited active context for immediate LLM use</li> <li>Long-term Memory: Durable PostgreSQL storage with RAG-based retrieval</li> </ul>"},{"location":"api/#class-hierarchy","title":"Class Hierarchy","text":""},{"location":"api/#class-diagram","title":"Class Diagram","text":"<p> <p> HTM + add_node() + recall() + retrieve() + forget() + create_context() + memory_stats() + which_robot_said() + conversation_timeline()</p> <p> WorkingMemory + add() + remove() + has_space?() + evict_to_make_space() + assemble_context() + token_count()</p> <p> LongTermMemory + add() + retrieve() + search() + search_fulltext() + search_hybrid() + add_relationship() + add_tag() + stats()</p> <p> EmbeddingService + embed() + count_tokens() - embed_ollama() - embed_openai()</p> <p> Database + setup() + default_config()</p> <p> uses</p> <p> uses</p> <p> uses</p> <p> config </p>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#core-classes","title":"Core Classes","text":"Class Purpose Key Methods HTM Main interface for memory management <code>add_node</code>, <code>recall</code>, <code>retrieve</code>, <code>forget</code>, <code>create_context</code> WorkingMemory Token-limited active context <code>add</code>, <code>evict_to_make_space</code>, <code>assemble_context</code> LongTermMemory Persistent PostgreSQL storage <code>add</code>, <code>search</code>, <code>search_fulltext</code>, <code>search_hybrid</code> EmbeddingService Vector embedding generation <code>embed</code>, <code>count_tokens</code> Database Schema setup and configuration <code>setup</code>, <code>default_config</code>"},{"location":"api/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"api/#basic-memory-operations","title":"Basic Memory Operations","text":"<pre><code># Initialize HTM\nhtm = HTM.new(robot_name: \"Assistant\")\n\n# Add memories\nhtm.add_node(\"fact_001\", \"PostgreSQL is our database\",\n  type: :fact, importance: 7.0, tags: [\"database\"])\n\n# Recall memories\nmemories = htm.recall(timeframe: \"last week\", topic: \"PostgreSQL\")\n\n# Create LLM context\ncontext = htm.create_context(strategy: :balanced)\n</code></pre>"},{"location":"api/#multi-robot-collaboration","title":"Multi-Robot Collaboration","text":"<pre><code># Find who discussed a topic\nrobots = htm.which_robot_said(\"deployment\")\n# =&gt; {\"robot-123\" =&gt; 5, \"robot-456\" =&gt; 3}\n\n# Get conversation timeline\ntimeline = htm.conversation_timeline(\"deployment\", limit: 20)\n</code></pre>"},{"location":"api/#advanced-retrieval","title":"Advanced Retrieval","text":"<pre><code># Vector similarity search\nmemories = htm.recall(\n  timeframe: \"last 30 days\",\n  topic: \"API design decisions\",\n  strategy: :vector,\n  limit: 10\n)\n\n# Hybrid search (fulltext + vector)\nmemories = htm.recall(\n  timeframe: \"this month\",\n  topic: \"security vulnerabilities\",\n  strategy: :hybrid,\n  limit: 20\n)\n</code></pre>"},{"location":"api/#memory-management","title":"Memory Management","text":"<pre><code># Get statistics\nstats = htm.memory_stats\n# =&gt; { total_nodes: 1234, working_memory: { current_tokens: 45000, ... }, ... }\n\n# Explicitly forget\nhtm.forget(\"temp_note\", confirm: :confirmed)\n</code></pre>"},{"location":"api/#search-strategies","title":"Search Strategies","text":"<p>HTM supports three search strategies for <code>recall</code>:</p> Strategy Description Use Case <code>:vector</code> Semantic similarity using embeddings Find conceptually related content <code>:fulltext</code> PostgreSQL full-text search Find exact terms and phrases <code>:hybrid</code> Combines fulltext + vector Best of both worlds - accurate and semantic"},{"location":"api/#memory-types","title":"Memory Types","text":"<p>When adding nodes, you can specify a type:</p> Type Purpose <code>:fact</code> Factual information <code>:context</code> Contextual background <code>:code</code> Code snippets <code>:preference</code> User preferences <code>:decision</code> Architectural decisions <code>:question</code> Questions and answers"},{"location":"api/#context-assembly-strategies","title":"Context Assembly Strategies","text":"<p>When creating context with <code>create_context</code>:</p> Strategy Behavior <code>:recent</code> Most recently accessed first <code>:important</code> Highest importance scores first <code>:balanced</code> Weighted by importance and recency"},{"location":"api/#api-documentation","title":"API Documentation","text":"<ul> <li>HTM - Main class reference</li> <li>WorkingMemory - Active context management</li> <li>LongTermMemory - Persistent storage</li> <li>EmbeddingService - Vector embeddings</li> <li>Database - Schema and configuration</li> </ul>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>HTM raises standard Ruby exceptions:</p> <pre><code># ArgumentError for invalid parameters\nhtm.forget(\"key\")  # Raises: Must pass confirm: :confirmed\n\n# PG::Error for database issues\nhtm.add_node(\"key\", \"value\")  # May raise PG connection errors\n\n# Invalid timeframe\nhtm.recall(timeframe: \"invalid\")  # Raises ArgumentError\n</code></pre>"},{"location":"api/#thread-safety","title":"Thread Safety","text":"<p>HTM is not thread-safe by default. Each instance maintains its own working memory state. For multi-threaded applications:</p> <ul> <li>Use separate HTM instances per thread</li> <li>Or implement external synchronization</li> <li>Database connections are created per operation (safe)</li> </ul>"},{"location":"api/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Working Memory: O(n) for eviction, O(1) for add/remove</li> <li>Vector Search: O(log n) with proper indexing</li> <li>Fulltext Search: O(log n) with GIN indexes</li> <li>Hybrid Search: Combines both overhead</li> </ul> <p>For large memory stores (&gt;100K nodes):</p> <ul> <li>Use hybrid search with appropriate <code>prefilter_limit</code></li> <li>Consider time-based partitioning (automatic with TimescaleDB)</li> <li>Enable compression for old data (configured in schema)</li> </ul>"},{"location":"api/#next-steps","title":"Next Steps","text":"<ul> <li>See HTM class reference for detailed API</li> <li>Review examples for common patterns</li> <li>Check database schema for advanced queries</li> </ul>"},{"location":"api/database/","title":"Database Class","text":"<p>Database schema setup and configuration utilities for HTM.</p>"},{"location":"api/database/#overview","title":"Overview","text":"<p><code>HTM::Database</code> provides class methods for setting up the HTM database schema, managing PostgreSQL connections, and configuring TimescaleDB hypertables.</p> <p>Key Features:</p> <ul> <li>Schema creation and migration</li> <li>TimescaleDB hypertable setup</li> <li>Extension verification (TimescaleDB, pgvector, pg_trgm)</li> <li>Connection configuration parsing</li> <li>Automatic compression policies</li> </ul>"},{"location":"api/database/#class-definition","title":"Class Definition","text":"<pre><code>class HTM::Database\n  # All methods are class methods\nend\n</code></pre>"},{"location":"api/database/#class-methods","title":"Class Methods","text":""},{"location":"api/database/#setup","title":"<code>setup(db_url = nil)</code>","text":"<p>Set up the HTM database schema and TimescaleDB hypertables.</p> <pre><code>HTM::Database.setup(db_url = nil)\n</code></pre>"},{"location":"api/database/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>db_url</code> String, nil <code>ENV['HTM_DBURL']</code> Database connection URL"},{"location":"api/database/#returns","title":"Returns","text":"<ul> <li><code>void</code></li> </ul>"},{"location":"api/database/#raises","title":"Raises","text":"<ul> <li><code>RuntimeError</code> - If database configuration not found</li> <li><code>PG::Error</code> - If database connection or schema creation fails</li> </ul>"},{"location":"api/database/#side-effects","title":"Side Effects","text":"<ul> <li>Connects to PostgreSQL database</li> <li>Verifies required extensions (TimescaleDB, pgvector, pg_trgm)</li> <li>Creates schema (tables, indexes, views)</li> <li>Converts tables to hypertables</li> <li>Sets up compression policies</li> <li>Prints status messages to stdout</li> </ul>"},{"location":"api/database/#examples","title":"Examples","text":"<pre><code># Use default configuration from environment\nHTM::Database.setup\n\n# Use specific database URL\nHTM::Database.setup('postgresql://user:pass@host:5432/dbname')\n\n# Use TimescaleDB Cloud\nurl = 'postgresql://tsdbadmin:pass@xxx.tsdb.cloud.timescale.com:37807/tsdb?sslmode=require'\nHTM::Database.setup(url)\n</code></pre>"},{"location":"api/database/#output","title":"Output","text":"<pre><code>\u2713 TimescaleDB version: 2.13.0\n\u2713 pgvector version: 0.5.1\n\u2713 pg_trgm version: 1.6\nCreating HTM schema...\n\u2713 Schema created\n\u2713 Created hypertable for operations_log\n\u2713 Created hypertable for nodes\n\u2713 Enabled compression for nodes older than 30 days\n\u2713 HTM database schema created successfully\n</code></pre>"},{"location":"api/database/#parse_connection_url","title":"<code>parse_connection_url(url)</code>","text":"<p>Parse a PostgreSQL connection URL into a configuration hash.</p> <pre><code>HTM::Database.parse_connection_url(url)\n</code></pre>"},{"location":"api/database/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>url</code> String PostgreSQL connection URL"},{"location":"api/database/#returns_1","title":"Returns","text":"<ul> <li><code>Hash</code> - Connection configuration</li> <li><code>nil</code> - If url is nil</li> </ul>"},{"location":"api/database/#hash-structure","title":"Hash Structure","text":"<pre><code>{\n  host: \"hostname\",\n  port: 5432,\n  dbname: \"database_name\",\n  user: \"username\",\n  password: \"password\",\n  sslmode: \"require\"  # or from URL params, default \"prefer\"\n}\n</code></pre>"},{"location":"api/database/#examples_1","title":"Examples","text":"<pre><code># Standard PostgreSQL URL\nurl = 'postgresql://user:pass@localhost:5432/mydb'\nconfig = HTM::Database.parse_connection_url(url)\n# =&gt; {\n#   host: \"localhost\",\n#   port: 5432,\n#   dbname: \"mydb\",\n#   user: \"user\",\n#   password: \"pass\",\n#   sslmode: \"prefer\"\n# }\n\n# With SSL mode\nurl = 'postgresql://user:pass@host:5432/db?sslmode=require'\nconfig = HTM::Database.parse_connection_url(url)\n# =&gt; { ..., sslmode: \"require\" }\n\n# TimescaleDB Cloud URL\nurl = 'postgresql://tsdbadmin:secret@xxx.tsdb.cloud.timescale.com:37807/tsdb?sslmode=require'\nconfig = HTM::Database.parse_connection_url(url)\n# =&gt; {\n#   host: \"xxx.tsdb.cloud.timescale.com\",\n#   port: 37807,\n#   dbname: \"tsdb\",\n#   user: \"tsdbadmin\",\n#   password: \"secret\",\n#   sslmode: \"require\"\n# }\n\n# Nil handling\nconfig = HTM::Database.parse_connection_url(nil)\n# =&gt; nil\n</code></pre>"},{"location":"api/database/#parse_connection_params","title":"<code>parse_connection_params()</code>","text":"<p>Build configuration from individual environment variables.</p> <pre><code>HTM::Database.parse_connection_params()\n</code></pre>"},{"location":"api/database/#returns_2","title":"Returns","text":"<ul> <li><code>Hash</code> - Connection configuration</li> <li><code>nil</code> - If <code>ENV['HTM_DBNAME']</code> not set</li> </ul>"},{"location":"api/database/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>HTM_DBHOST</code> Database hostname <code>'cw7rxj91bm.srbbwwxn56.tsdb.cloud.timescale.com'</code> <code>HTM_DBPORT</code> Database port <code>37807</code> <code>HTM_DBNAME</code> Database name required <code>HTM_DBUSER</code> Database user required <code>HTM_DBPASS</code> Database password required"},{"location":"api/database/#examples_2","title":"Examples","text":"<pre><code># Set environment variables\nENV['HTM_DBNAME'] = 'tsdb'\nENV['HTM_DBUSER'] = 'tsdbadmin'\nENV['HTM_DBPASS'] = 'secret'\n\nconfig = HTM::Database.parse_connection_params()\n# =&gt; {\n#   host: \"cw7rxj91bm.srbbwwxn56.tsdb.cloud.timescale.com\",\n#   port: 37807,\n#   dbname: \"tsdb\",\n#   user: \"tsdbadmin\",\n#   password: \"secret\",\n#   sslmode: \"require\"\n# }\n\n# Custom host and port\nENV['HTM_DBHOST'] = 'localhost'\nENV['HTM_DBPORT'] = '5432'\n\nconfig = HTM::Database.parse_connection_params()\n# =&gt; { host: \"localhost\", port: 5432, ... }\n\n# Without HTM_DBNAME\nENV.delete('HTM_DBNAME')\nconfig = HTM::Database.parse_connection_params()\n# =&gt; nil\n</code></pre>"},{"location":"api/database/#default_config","title":"<code>default_config()</code>","text":"<p>Get default database configuration from environment.</p> <pre><code>HTM::Database.default_config()\n</code></pre>"},{"location":"api/database/#returns_3","title":"Returns","text":"<ul> <li><code>Hash</code> - Connection configuration</li> <li><code>nil</code> - If no configuration found</li> </ul>"},{"location":"api/database/#priority-order","title":"Priority Order","text":"<ol> <li><code>ENV['HTM_DBURL']</code> - Parse connection URL</li> <li><code>ENV['HTM_DBNAME']</code> - Parse individual params</li> <li><code>nil</code> - No configuration available</li> </ol>"},{"location":"api/database/#examples_3","title":"Examples","text":"<pre><code># Using HTM_DBURL\nENV['HTM_DBURL'] = 'postgresql://user:pass@host/db'\nconfig = HTM::Database.default_config\n# =&gt; Parsed from URL\n\n# Using HTM_DBNAME\nENV.delete('HTM_DBURL')\nENV['HTM_DBNAME'] = 'mydb'\nENV['HTM_DBUSER'] = 'user'\nENV['HTM_DBPASS'] = 'pass'\nconfig = HTM::Database.default_config\n# =&gt; Parsed from params\n\n# No configuration\nENV.delete('HTM_DBURL')\nENV.delete('HTM_DBNAME')\nconfig = HTM::Database.default_config\n# =&gt; nil\n\n# Use in HTM initialization\nhtm = HTM.new(db_config: HTM::Database.default_config)\n</code></pre>"},{"location":"api/database/#database-schema","title":"Database Schema","text":"<p>For detailed database schema documentation, see:</p> <ul> <li>Database Schema Documentation - Query patterns, optimization tips, and best practices</li> <li>Database Tables Overview - Auto-generated table reference with ER diagram</li> </ul>"},{"location":"api/database/#quick-reference","title":"Quick Reference","text":"Table Purpose robots Robot registry for multi-robot tracking nodes Primary memory storage with vector embeddings tags Hierarchical tag names for categorization robot_nodes Robot-to-node associations (hive mind) node_tags Node-to-tag associations working_memories Per-robot working memory state"},{"location":"api/database/#required-extensions","title":"Required Extensions","text":"Extension Purpose <code>pgvector</code> Vector similarity search with HNSW indexes <code>pg_trgm</code> Trigram-based fuzzy text matching"},{"location":"api/database/#setup-process","title":"Setup Process","text":""},{"location":"api/database/#1-verify-extensions","title":"1. Verify Extensions","text":"<pre><code># Check TimescaleDB\ntimescale = conn.exec(\"SELECT extversion FROM pg_extension WHERE extname='timescaledb'\").first\n# =&gt; {\"extversion\"=&gt;\"2.13.0\"}\n\n# Check pgvector\npgvector = conn.exec(\"SELECT extversion FROM pg_extension WHERE extname='vector'\").first\n# =&gt; {\"extversion\"=&gt;\"0.5.1\"}\n\n# Check pg_trgm\npg_trgm = conn.exec(\"SELECT extversion FROM pg_extension WHERE extname='pg_trgm'\").first\n# =&gt; {\"extversion\"=&gt;\"1.6\"}\n</code></pre>"},{"location":"api/database/#2-run-schema","title":"2. Run Schema","text":"<p>Reads and executes <code>sql/schema.sql</code> from the repository:</p> <ul> <li>Creates tables</li> <li>Creates indexes</li> <li>Creates views</li> <li>Sets up constraints</li> </ul> <p>Note: <code>CREATE EXTENSION</code> lines are filtered out (extensions must be pre-installed).</p>"},{"location":"api/database/#3-setup-hypertables","title":"3. Setup Hypertables","text":"<p>Converts tables to hypertables:</p> <pre><code># operations_log\nconn.exec(\"SELECT create_hypertable('operations_log', 'timestamp', if_not_exists =&gt; TRUE, migrate_data =&gt; TRUE)\")\n\n# nodes (with compression)\nconn.exec(\"SELECT create_hypertable('nodes', 'created_at', if_not_exists =&gt; TRUE, migrate_data =&gt; TRUE)\")\nconn.exec(\"ALTER TABLE nodes SET (timescaledb.compress, timescaledb.compress_segmentby = 'robot_id,type')\")\nconn.exec(\"SELECT add_compression_policy('nodes', INTERVAL '30 days', if_not_exists =&gt; TRUE)\")\n</code></pre>"},{"location":"api/database/#environment-configuration","title":"Environment Configuration","text":""},{"location":"api/database/#timescaledb-cloud","title":"TimescaleDB Cloud","text":"<p>Using URL (recommended):</p> <pre><code># In ~/.bashrc__tiger\nexport HTM_DBURL='postgresql://tsdbadmin:PASSWORD@SERVICE.tsdb.cloud.timescale.com:37807/tsdb?sslmode=require'\n</code></pre> <p>Using individual variables:</p> <pre><code># In ~/.bashrc__tiger\nexport HTM_DBHOST='xxx.tsdb.cloud.timescale.com'\nexport HTM_DBPORT=37807\nexport HTM_DBNAME='tsdb'\nexport HTM_DBUSER='tsdbadmin'\nexport HTM_DBPASS='your_password'\n</code></pre>"},{"location":"api/database/#local-postgresql","title":"Local PostgreSQL","text":"<pre><code>export HTM_DBURL='postgresql://localhost/htm_dev'\n\n# Or with auth\nexport HTM_DBURL='postgresql://user:pass@localhost:5432/htm_dev'\n</code></pre>"},{"location":"api/database/#docker-postgresql","title":"Docker PostgreSQL","text":"<pre><code>export HTM_DBURL='postgresql://postgres:postgres@localhost:5432/htm'\n</code></pre>"},{"location":"api/database/#usage-examples","title":"Usage Examples","text":""},{"location":"api/database/#initial-setup","title":"Initial Setup","text":"<pre><code># First time setup\nrequire 'htm'\n\nHTM::Database.setup\n# Creates all tables, indexes, hypertables\n\n# Verify\nconfig = HTM::Database.default_config\nconn = PG.connect(config)\nresult = conn.exec(\"SELECT COUNT(*) FROM nodes\")\nconn.close\n</code></pre>"},{"location":"api/database/#configuration-management","title":"Configuration Management","text":"<pre><code># Get current config\nconfig = HTM::Database.default_config\n\nif config\n  puts \"Database: #{config[:dbname]}\"\n  puts \"Host: #{config[:host]}\"\n  puts \"Port: #{config[:port]}\"\nelse\n  puts \"No database configuration found\"\n  puts \"Please set HTM_DBURL or HTM_DBNAME environment variables\"\nend\n\n# Test connection\nbegin\n  conn = PG.connect(config)\n  version = conn.exec(\"SELECT version()\").first['version']\n  puts \"Connected: #{version}\"\n  conn.close\nrescue PG::Error =&gt; e\n  puts \"Connection failed: #{e.message}\"\nend\n</code></pre>"},{"location":"api/database/#schema-migration","title":"Schema Migration","text":"<pre><code># Check if schema exists\nconfig = HTM::Database.default_config\nconn = PG.connect(config)\n\ntables = conn.exec(&lt;&lt;~SQL).to_a\n  SELECT table_name\n  FROM information_schema.tables\n  WHERE table_schema = 'public'\n  AND table_name IN ('nodes', 'robots', 'relationships', 'tags', 'operations_log')\nSQL\n\nif tables.empty?\n  puts \"Schema not found, running setup...\"\n  HTM::Database.setup\nelse\n  puts \"Schema already exists:\"\n  tables.each { |t| puts \"  - #{t['table_name']}\" }\nend\n\nconn.close\n</code></pre>"},{"location":"api/database/#custom-database","title":"Custom Database","text":"<pre><code># Use non-standard database\ncustom_url = 'postgresql://app:secret@db.example.com:5432/production'\n\nHTM::Database.setup(custom_url)\n\n# Use with HTM\nconfig = HTM::Database.parse_connection_url(custom_url)\nhtm = HTM.new(db_config: config)\n</code></pre>"},{"location":"api/database/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/database/#extensions-not-available","title":"Extensions Not Available","text":"<pre><code>\u26a0 Warning: TimescaleDB extension not found\n\u26a0 Warning: pgvector extension not found\n</code></pre> <p>Solution: Install required extensions:</p> <pre><code># Ubuntu/Debian\nsudo apt install postgresql-15-timescaledb postgresql-15-pgvector\n\n# macOS with Homebrew\nbrew install timescaledb pgvector\n\n# Or use TimescaleDB Cloud (extensions pre-installed)\n</code></pre>"},{"location":"api/database/#connection-refused","title":"Connection Refused","text":"<pre><code>PG::ConnectionBad: could not connect to server: Connection refused\n</code></pre> <p>Solution: Verify PostgreSQL is running and connection details:</p> <pre><code># Check PostgreSQL status\npg_isready -h localhost -p 5432\n\n# Test connection\npsql -h localhost -U user -d dbname\n\n# Verify environment\necho $HTM_DBURL\n</code></pre>"},{"location":"api/database/#permission-denied","title":"Permission Denied","text":"<pre><code>PG::InsufficientPrivilege: ERROR:  permission denied for schema public\n</code></pre> <p>Solution: Grant necessary permissions:</p> <pre><code>GRANT ALL ON SCHEMA public TO your_user;\nGRANT ALL ON ALL TABLES IN SCHEMA public TO your_user;\n</code></pre>"},{"location":"api/database/#hypertable-already-exists","title":"Hypertable Already Exists","text":"<pre><code>Note: nodes hypertable: table \"nodes\" is already a hypertable\n</code></pre> <p>This is not an error - the schema setup is idempotent. Safe to ignore.</p>"},{"location":"api/database/#best-practices","title":"Best Practices","text":""},{"location":"api/database/#1-use-environment-variables","title":"1. Use Environment Variables","text":"<pre><code># Good: Use environment variables\nHTM::Database.setup\n\n# Avoid: Hardcoded credentials\nHTM::Database.setup('postgresql://user:password@host/db')\n</code></pre>"},{"location":"api/database/#2-verify-extensions-first","title":"2. Verify Extensions First","text":"<pre><code># Check extensions before setup\nconfig = HTM::Database.default_config\nconn = PG.connect(config)\n\nrequired = ['timescaledb', 'vector', 'pg_trgm']\nmissing = required.reject do |ext|\n  !conn.exec(\"SELECT 1 FROM pg_extension WHERE extname='#{ext}'\").first\nend\n\nif missing.any?\n  puts \"Missing extensions: #{missing.join(', ')}\"\n  puts \"Please install before running setup\"\n  exit 1\nend\n\nconn.close\nHTM::Database.setup\n</code></pre>"},{"location":"api/database/#3-run-setup-once","title":"3. Run Setup Once","text":"<pre><code># Run setup in a migration or initial deployment\n# Not on every application start\n\n# Bad:\ndef initialize\n  HTM::Database.setup  # Don't do this\n  @htm = HTM.new\nend\n\n# Good:\n# Run once during deployment:\n# rake db:setup -&gt; HTM::Database.setup\n</code></pre>"},{"location":"api/database/#4-handle-missing-configuration","title":"4. Handle Missing Configuration","text":"<pre><code>config = HTM::Database.default_config\n\nunless config\n  raise \"Database not configured. Please set HTM_DBURL environment variable. \" \\\n        \"See README.md for configuration instructions.\"\nend\n</code></pre>"},{"location":"api/database/#see-also","title":"See Also","text":"<ul> <li>HTM API - Main class that uses Database config</li> <li>LongTermMemory API - Uses database for storage</li> <li>Database Schema - Query patterns, optimization tips, and best practices</li> <li>Database Tables - Auto-generated table reference with ER diagram</li> <li>pgvector Documentation - Vector search</li> <li>pg_trgm Documentation - Trigram fuzzy matching</li> </ul>"},{"location":"api/embedding-service/","title":"EmbeddingService Class","text":"<p>Client-side embedding generation service for HTM.</p>"},{"location":"api/embedding-service/#overview","title":"Overview","text":"<p><code>HTM::EmbeddingService</code> generates vector embeddings for text content before database insertion. It supports multiple embedding providers:</p> <ul> <li>Ollama - Local embedding server (default, via <code>nomic-embed-text</code> model)</li> <li>OpenAI - OpenAI's <code>text-embedding-3-small</code> model</li> </ul> <p>The service also provides token counting for working memory management.</p> <p>Architecture: - Ruby application generates embeddings via HTTP call to Ollama/OpenAI - Embeddings are passed to PostgreSQL during INSERT - Simple, reliable, cross-platform operation</p>"},{"location":"api/embedding-service/#class-definition","title":"Class Definition","text":"<pre><code>class HTM::EmbeddingService\n  attr_reader :provider, :model, :dimensions\nend\n</code></pre>"},{"location":"api/embedding-service/#initialization","title":"Initialization","text":""},{"location":"api/embedding-service/#new","title":"<code>new(provider, **options)</code>","text":"<p>Create a new embedding service instance.</p> <pre><code>HTM::EmbeddingService.new(\n  provider = :ollama,\n  model: 'nomic-embed-text',\n  ollama_url: nil,\n  dimensions: nil\n)\n</code></pre>"},{"location":"api/embedding-service/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>provider</code> Symbol <code>:ollama</code> Embedding provider (<code>:ollama</code>, <code>:openai</code>) <code>model</code> String <code>'nomic-embed-text'</code> Model name for the provider <code>ollama_url</code> String, nil <code>ENV['OLLAMA_URL']</code> or <code>'http://localhost:11434'</code> Ollama server URL <code>dimensions</code> Integer, nil Auto-detected Expected embedding dimensions"},{"location":"api/embedding-service/#returns","title":"Returns","text":"<p><code>HTM::EmbeddingService</code> - Configured embedding service instance</p>"},{"location":"api/embedding-service/#raises","title":"Raises","text":"<ul> <li><code>HTM::EmbeddingError</code> - If provider is invalid or configuration fails</li> </ul>"},{"location":"api/embedding-service/#examples","title":"Examples","text":"<p>Default Ollama configuration:</p> <pre><code>service = HTM::EmbeddingService.new\n# Uses Ollama at http://localhost:11434 with nomic-embed-text (768 dimensions)\n</code></pre> <p>Custom Ollama model:</p> <pre><code>service = HTM::EmbeddingService.new(\n  :ollama,\n  model: 'mxbai-embed-large',\n  ollama_url: 'http://localhost:11434',\n  dimensions: 1024\n)\n</code></pre> <p>OpenAI configuration:</p> <pre><code># Requires OPENAI_API_KEY environment variable\nservice = HTM::EmbeddingService.new(\n  :openai,\n  model: 'text-embedding-3-small',\n  dimensions: 1536\n)\n</code></pre> <p>HTM automatically initializes EmbeddingService:</p> <pre><code>htm = HTM.new(\n  robot_name: \"Assistant\",\n  embedding_provider: :ollama,\n  embedding_model: 'nomic-embed-text'\n)\n# EmbeddingService configured automatically\n</code></pre>"},{"location":"api/embedding-service/#instance-methods","title":"Instance Methods","text":""},{"location":"api/embedding-service/#embed","title":"<code>embed(text)</code>","text":"<p>Generate embedding vector for text.</p> <pre><code>embed(text) \u2192 Array&lt;Float&gt;\n</code></pre>"},{"location":"api/embedding-service/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>text</code> String Text to embed"},{"location":"api/embedding-service/#returns_1","title":"Returns","text":"<p><code>Array&lt;Float&gt;</code> - Embedding vector (dimensions depend on model)</p>"},{"location":"api/embedding-service/#raises_1","title":"Raises","text":"<ul> <li><code>HTM::EmbeddingError</code> - If embedding generation fails</li> <li><code>ArgumentError</code> - If text is nil or empty</li> </ul>"},{"location":"api/embedding-service/#examples_1","title":"Examples","text":"<pre><code>service = HTM::EmbeddingService.new(:ollama)\n\n# Generate embedding\nembedding = service.embed(\"PostgreSQL with TimescaleDB\")\n# =&gt; [0.023, -0.441, 0.182, ..., 0.091]  # 768 dimensions\n\nputs embedding.length  # =&gt; 768 (for nomic-embed-text)\n</code></pre> <p>Error handling:</p> <pre><code>begin\n  embedding = service.embed(\"some text\")\nrescue HTM::EmbeddingError =&gt; e\n  puts \"Embedding failed: #{e.message}\"\n  # Check Ollama is running: curl http://localhost:11434/api/tags\nend\n</code></pre>"},{"location":"api/embedding-service/#implementation-details","title":"Implementation Details","text":"<p>Ollama provider: - Makes HTTP POST to <code>/api/embeddings</code> - Returns dense vector representation - Requires Ollama server running locally</p> <p>OpenAI provider: - Makes HTTP POST to OpenAI API - Requires <code>OPENAI_API_KEY</code> environment variable - API costs: $0.0001 per 1K tokens</p>"},{"location":"api/embedding-service/#count_tokens","title":"<code>count_tokens(text)</code>","text":"<p>Count tokens in text for working memory management.</p> <pre><code>count_tokens(text) \u2192 Integer\n</code></pre>"},{"location":"api/embedding-service/#parameters_2","title":"Parameters","text":"Parameter Type Description <code>text</code> String Text to count tokens for"},{"location":"api/embedding-service/#returns_2","title":"Returns","text":"<p><code>Integer</code> - Approximate token count</p>"},{"location":"api/embedding-service/#examples_2","title":"Examples","text":"<pre><code>service = HTM::EmbeddingService.new\n\ntokens = service.count_tokens(\"Hello, world!\")\n# =&gt; 4\n\ntokens = service.count_tokens(\"The quick brown fox jumps over the lazy dog\")\n# =&gt; 10\n</code></pre> <p>Used internally by HTM:</p> <pre><code>htm.add_message(\n  \"This is a long conversation message...\",\n  speaker: \"user\"\n)\n# HTM calls embedding_service.count_tokens() internally\n# to manage working memory token budget\n</code></pre>"},{"location":"api/embedding-service/#embedding-providers","title":"Embedding Providers","text":""},{"location":"api/embedding-service/#ollama-default","title":"Ollama (Default)","text":"<p>Status: \u2705 Fully implemented</p> <p>Local embedding server with various models, accessed via HTTP.</p> <p>Installation:</p> <pre><code># macOS/Linux\ncurl https://ollama.ai/install.sh | sh\n\n# Pull embedding model\nollama pull nomic-embed-text\n</code></pre> <p>Models:</p> Model Dimensions Speed Use Case <code>nomic-embed-text</code> 768 Fast General-purpose (default) <code>mxbai-embed-large</code> 1024 Medium Higher quality embeddings <code>all-minilm</code> 384 Very fast Lower quality, fast search <p>Configuration:</p> <pre><code>service = HTM::EmbeddingService.new(\n  :ollama,\n  model: 'nomic-embed-text',\n  ollama_url: 'http://localhost:11434'\n)\n\nembedding = service.embed(\"test text\")\n</code></pre> <p>Troubleshooting:</p> <p>If Ollama is unavailable, embedding generation will fail:</p> <pre><code># Check Ollama is running\nsystem(\"curl http://localhost:11434/api/tags\")\n\n# Start Ollama if needed\nsystem(\"ollama serve\")\n</code></pre> <p>Advantages: - \u2705 Free (no API costs) - \u2705 Private (data never leaves your machine) - \u2705 Fast (local generation) - \u2705 Works offline</p> <p>Disadvantages: - \u274c Requires local installation - \u274c Uses local compute resources - \u274c Slightly lower quality than OpenAI</p>"},{"location":"api/embedding-service/#openai","title":"OpenAI","text":"<p>Status: \u2705 Fully implemented</p> <p>Uses OpenAI's embedding API, accessed via HTTP.</p> <p>Configuration:</p> <pre><code>export OPENAI_API_KEY=\"sk-...\"\n</code></pre> <pre><code>service = HTM::EmbeddingService.new(\n  :openai,\n  model: 'text-embedding-3-small'\n)\n\n# Add message - embedding generated via OpenAI API\nembedding = service.embed(\"test text\")\n</code></pre> <p>Models:</p> Model Dimensions Speed Cost <code>text-embedding-3-small</code> 1536 Fast $0.0001/1K tokens <code>text-embedding-ada-002</code> 1536 Fast $0.0001/1K tokens <p>Error Handling:</p> <pre><code>begin\n  service = HTM::EmbeddingService.new(:openai)\n  embedding = service.embed(\"test\")\nrescue HTM::EmbeddingError =&gt; e\n  if e.message.include?(\"API key\")\n    puts \"Set OPENAI_API_KEY environment variable\"\n  end\nend\n</code></pre> <p>Advantages: - \u2705 High quality embeddings - \u2705 No local installation required - \u2705 Managed service</p> <p>Disadvantages: - \u274c API costs ($0.0001 per 1K tokens) - \u274c Requires internet connection - \u274c Data sent to OpenAI servers - \u274c Requires API key management</p>"},{"location":"api/embedding-service/#error-handling","title":"Error Handling","text":""},{"location":"api/embedding-service/#common-errors","title":"Common Errors","text":"<p>Ollama not running:</p> <pre><code># Error: Failed to connect to Ollama\n# Solution: Start Ollama\nsystem(\"ollama serve\")\n</code></pre> <p>OpenAI API key missing:</p> <pre><code># Error: OPENAI_API_KEY not set\n# Solution: Set environment variable\nENV['OPENAI_API_KEY'] = 'sk-...'\n</code></pre> <p>Invalid model:</p> <pre><code># Error: Model not found\n# Solution: Pull the model first\nsystem(\"ollama pull nomic-embed-text\")\n</code></pre>"},{"location":"api/embedding-service/#exception-types","title":"Exception Types","text":""},{"location":"api/embedding-service/#performance","title":"Performance","text":""},{"location":"api/embedding-service/#latency-benchmarks","title":"Latency Benchmarks","text":"<p>Based on typical production workloads:</p> Provider Model Latency (P50) Latency (P95) Cost per 1K embeds Ollama nomic-embed-text 20ms 40ms Free Ollama mxbai-embed-large 30ms 60ms Free OpenAI text-embedding-3-small 40ms 80ms $0.10 <p>Factors affecting latency: - Network latency (Ollama local vs OpenAI remote) - Text length (longer text = more tokens = slower) - Model size (larger models = slower) - System load (CPU/GPU utilization)</p>"},{"location":"api/embedding-service/#optimization-tips","title":"Optimization Tips","text":"<p>Use appropriate model size:</p> <pre><code># Fast but lower quality\nservice = HTM::EmbeddingService.new(:ollama, model: 'all-minilm')\n\n# Balanced (recommended)\nservice = HTM::EmbeddingService.new(:ollama, model: 'nomic-embed-text')\n\n# Slower but higher quality\nservice = HTM::EmbeddingService.new(:ollama, model: 'mxbai-embed-large')\n</code></pre> <p>Batch operations:</p> <pre><code># HTM automatically generates embeddings for each message\n# No special batching API needed\nmessages.each do |msg|\n  htm.add_message(msg, speaker: \"user\")\n  # Embedding generated for each message\nend\n</code></pre>"},{"location":"api/embedding-service/#integration-with-htm","title":"Integration with HTM","text":""},{"location":"api/embedding-service/#automatic-initialization","title":"Automatic Initialization","text":"<p>HTM initializes <code>EmbeddingService</code> automatically:</p> <pre><code>htm = HTM.new(\n  robot_name: \"Assistant\",\n  embedding_provider: :ollama,        # Optional, default\n  embedding_model: 'nomic-embed-text' # Optional, default\n)\n\n# EmbeddingService is ready to use internally\n</code></pre>"},{"location":"api/embedding-service/#embedding-generation-flow","title":"Embedding Generation Flow","text":"<pre><code>sequenceDiagram\n    participant App as Application\n    participant HTM as HTM\n    participant ES as EmbeddingService\n    participant Ollama as Ollama/OpenAI\n    participant DB as PostgreSQL\n\n    App-&gt;&gt;HTM: add_message(content)\n    HTM-&gt;&gt;ES: embed(content)\n    ES-&gt;&gt;Ollama: HTTP POST /api/embeddings\n    Ollama-&gt;&gt;ES: embedding vector\n    ES-&gt;&gt;HTM: Array&lt;Float&gt;\n    HTM-&gt;&gt;DB: INSERT with embedding\n    DB-&gt;&gt;HTM: node_id\n    HTM-&gt;&gt;App: node_id</code></pre>"},{"location":"api/embedding-service/#query-embedding","title":"Query Embedding","text":"<p>Search queries also generate embeddings:</p> <pre><code># User searches for \"database performance\"\nresults = htm.recall(\n  timeframe: \"last week\",\n  topic: \"database performance\",\n  strategy: :vector\n)\n\n# Internally:\n# 1. embedding_service.embed(\"database performance\")\n# 2. SQL vector search using embedding\n# 3. Return similar nodes\n</code></pre>"},{"location":"api/embedding-service/#examples_3","title":"Examples","text":""},{"location":"api/embedding-service/#basic-usage","title":"Basic Usage","text":"<pre><code>require 'htm'\n\n# Create service\nservice = HTM::EmbeddingService.new(:ollama)\n\n# Generate embedding\ntext = \"PostgreSQL with TimescaleDB handles time-series data efficiently\"\nembedding = service.embed(text)\n\nputs \"Embedding dimensions: #{embedding.length}\"\nputs \"First 5 values: #{embedding[0..4]}\"\n\n# Count tokens\ntokens = service.count_tokens(text)\nputs \"Token count: #{tokens}\"\n</code></pre>"},{"location":"api/embedding-service/#multiple-providers","title":"Multiple Providers","text":"<pre><code># Ollama for development\ndev_service = HTM::EmbeddingService.new(\n  :ollama,\n  model: 'nomic-embed-text'\n)\n\n# OpenAI for production\nprod_service = HTM::EmbeddingService.new(\n  :openai,\n  model: 'text-embedding-3-small'\n)\n\n# Same interface\ndev_embedding = dev_service.embed(\"test\")\nprod_embedding = prod_service.embed(\"test\")\n</code></pre>"},{"location":"api/embedding-service/#custom-model-dimensions","title":"Custom Model Dimensions","text":"<pre><code># Specify dimensions explicitly\nservice = HTM::EmbeddingService.new(\n  :ollama,\n  model: 'custom-model',\n  dimensions: 512\n)\n\nembedding = service.embed(\"text\")\n# Embedding will be padded/truncated to 512 dimensions\n</code></pre>"},{"location":"api/embedding-service/#see-also","title":"See Also","text":"<ul> <li>HTM API - Main HTM class</li> <li>LongTermMemory API - Storage layer</li> <li>ADR-003: Ollama Embeddings - Architecture decision</li> <li>Ollama Documentation - Ollama setup guide</li> <li>OpenAI Embeddings - OpenAI API docs</li> </ul>"},{"location":"api/htm/","title":"HTM Class","text":"<p>The main interface for HTM's intelligent memory management system.</p>"},{"location":"api/htm/#overview","title":"Overview","text":"<p><code>HTM</code> is the primary class that orchestrates the two-tier memory system:</p> <ul> <li>Working Memory: Token-limited active context for immediate LLM use</li> <li>Long-term Memory: Durable PostgreSQL storage with vector embeddings</li> </ul> <p>Key features:</p> <ul> <li>Never forgets unless explicitly told (<code>forget</code>)</li> <li>RAG-based retrieval (temporal + semantic search)</li> <li>Multi-robot \"hive mind\" - all robots share global memory via content deduplication</li> <li>Hierarchical tagging for knowledge organization</li> <li>Tag-enhanced hybrid search for improved relevance</li> </ul>"},{"location":"api/htm/#class-definition","title":"Class Definition","text":"<pre><code>class HTM\n  attr_reader :robot_id, :robot_name, :working_memory, :long_term_memory\nend\n</code></pre>"},{"location":"api/htm/#initialization","title":"Initialization","text":""},{"location":"api/htm/#new","title":"<code>new(**options)</code>","text":"<p>Create a new HTM instance.</p> <pre><code>HTM.new(\n  working_memory_size: 128_000,\n  robot_name: nil,\n  db_config: nil,\n  db_pool_size: 5,\n  db_query_timeout: 30_000,\n  db_cache_size: 1000,\n  db_cache_ttl: 300\n)\n</code></pre>"},{"location":"api/htm/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>working_memory_size</code> Integer <code>128_000</code> Maximum tokens for working memory <code>robot_name</code> String, nil <code>\"robot_#{uuid[0..7]}\"</code> Human-readable name <code>db_config</code> Hash, nil From <code>ENV['HTM_DBURL']</code> Database configuration <code>db_pool_size</code> Integer <code>5</code> Database connection pool size <code>db_query_timeout</code> Integer <code>30_000</code> Query timeout in milliseconds <code>db_cache_size</code> Integer <code>1000</code> Query cache size (0 to disable) <code>db_cache_ttl</code> Integer <code>300</code> Cache TTL in seconds"},{"location":"api/htm/#returns","title":"Returns","text":"<ul> <li><code>HTM</code> instance</li> </ul>"},{"location":"api/htm/#examples","title":"Examples","text":"<pre><code># Basic initialization\nhtm = HTM.new(robot_name: \"Code Assistant\")\n\n# Custom working memory size\nhtm = HTM.new(\n  robot_name: \"Large Context Bot\",\n  working_memory_size: 256_000\n)\n\n# Custom database configuration\nhtm = HTM.new(\n  db_config: {\n    host: 'localhost',\n    port: 5432,\n    dbname: 'htm_db',\n    user: 'postgres',\n    password: 'secret'\n  }\n)\n\n# With caching disabled\nhtm = HTM.new(\n  robot_name: \"No Cache Bot\",\n  db_cache_size: 0\n)\n</code></pre>"},{"location":"api/htm/#instance-attributes","title":"Instance Attributes","text":""},{"location":"api/htm/#robot_id","title":"<code>robot_id</code>","text":"<p>Unique integer identifier for this robot instance.</p> <ul> <li>Type: Integer</li> <li>Read-only: Yes</li> </ul> <pre><code>htm.robot_id  # =&gt; 42\n</code></pre>"},{"location":"api/htm/#robot_name","title":"<code>robot_name</code>","text":"<p>Human-readable name for this robot.</p> <ul> <li>Type: String</li> <li>Read-only: Yes</li> </ul> <pre><code>htm.robot_name  # =&gt; \"Code Assistant\"\n</code></pre>"},{"location":"api/htm/#working_memory","title":"<code>working_memory</code>","text":"<p>The working memory instance.</p> <ul> <li>Type: <code>HTM::WorkingMemory</code></li> <li>Read-only: Yes</li> </ul> <pre><code>htm.working_memory.token_count  # =&gt; 45234\n</code></pre>"},{"location":"api/htm/#long_term_memory","title":"<code>long_term_memory</code>","text":"<p>The long-term memory instance.</p> <ul> <li>Type: <code>HTM::LongTermMemory</code></li> <li>Read-only: Yes</li> </ul> <pre><code>htm.long_term_memory.stats  # =&gt; {...}\n</code></pre>"},{"location":"api/htm/#public-methods","title":"Public Methods","text":""},{"location":"api/htm/#remember","title":"<code>remember(content, tags:, metadata:)</code>","text":"<p>Remember new information by storing it in long-term memory.</p> <pre><code>remember(content, tags: [], metadata: {})\n</code></pre>"},{"location":"api/htm/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>content</code> String required The information to remember <code>tags</code> Array\\&lt;String&gt; <code>[]</code> Manual tags to assign (in addition to auto-extracted tags) <code>metadata</code> Hash <code>{}</code> Arbitrary key-value metadata stored as JSONB. Keys must be strings or symbols."},{"location":"api/htm/#returns_1","title":"Returns","text":"<ul> <li><code>Integer</code> - Database ID of the memory node</li> </ul>"},{"location":"api/htm/#side-effects","title":"Side Effects","text":"<ul> <li>Stores node in PostgreSQL with content deduplication (via SHA-256 hash)</li> <li>Creates/updates <code>robot_nodes</code> association for this robot</li> <li>Adds node to working memory (evicts if needed)</li> <li>Enqueues background job for embedding generation (new nodes only)</li> <li>Enqueues background job for tag extraction (new nodes only)</li> <li>Updates robot activity timestamp</li> </ul>"},{"location":"api/htm/#content-deduplication","title":"Content Deduplication","text":"<p>When <code>remember()</code> is called:</p> <ol> <li>A SHA-256 hash of the content is computed</li> <li>If a node with the same hash exists, the existing node is reused</li> <li>A new <code>robot_nodes</code> association is created (or <code>remember_count</code> is incremented)</li> <li>This ensures identical memories are stored once but can be \"remembered\" by multiple robots</li> </ol>"},{"location":"api/htm/#examples_1","title":"Examples","text":"<pre><code># Basic usage\nnode_id = htm.remember(\"PostgreSQL supports vector similarity search via pgvector\")\n\n# With manual tags\nnode_id = htm.remember(\n  \"Time-series data works great with hypertables\",\n  tags: [\"database:timescaledb\", \"performance\"]\n)\n\n# With metadata\nnode_id = htm.remember(\n  \"User prefers dark mode for all interfaces\",\n  metadata: { category: \"preference\", priority: \"high\", source_app: \"settings\" }\n)\n\n# With both tags and metadata\nnode_id = htm.remember(\n  \"API rate limit is 1000 requests per minute\",\n  tags: [\"api:rate-limiting\", \"infrastructure\"],\n  metadata: { environment: \"production\", version: 2 }\n)\n\n# Multiple robots remembering the same content\nrobot1 = HTM.new(robot_name: \"assistant_1\")\nrobot2 = HTM.new(robot_name: \"assistant_2\")\n\n# Both robots remember the same fact - stored once, linked to both\nrobot1.remember(\"Ruby 3.3 was released in December 2023\")\nrobot2.remember(\"Ruby 3.3 was released in December 2023\")\n# Same node_id returned, remember_count incremented for robot2\n</code></pre>"},{"location":"api/htm/#notes","title":"Notes","text":"<ul> <li>Embeddings and hierarchical tags are generated asynchronously via background jobs</li> <li>Empty content returns the ID of the most recent node without creating a duplicate</li> <li>Token count is calculated automatically using the configured token counter</li> <li>Metadata is stored in a JSONB column with a GIN index for efficient queries</li> </ul>"},{"location":"api/htm/#recall","title":"<code>recall(topic, **options)</code>","text":"<p>Recall memories from long-term storage using RAG-based retrieval.</p> <pre><code>recall(\n  topic,\n  timeframe: \"last 7 days\",\n  limit: 20,\n  strategy: :vector,\n  with_relevance: false,\n  query_tags: [],\n  metadata: {},\n  raw: false\n)\n</code></pre>"},{"location":"api/htm/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>topic</code> String required Topic to search for <code>timeframe</code> String, Range <code>\"last 7 days\"</code> Time range <code>limit</code> Integer <code>20</code> Maximum number of nodes to retrieve <code>strategy</code> Symbol <code>:vector</code> Search strategy (<code>:vector</code>, <code>:fulltext</code>, <code>:hybrid</code>) <code>with_relevance</code> Boolean <code>false</code> Include dynamic relevance scores <code>query_tags</code> Array\\&lt;String&gt; <code>[]</code> Tags to boost relevance <code>metadata</code> Hash <code>{}</code> Filter results by metadata (uses JSONB <code>@&gt;</code> containment) <code>raw</code> Boolean <code>false</code> Return full node hashes instead of content strings"},{"location":"api/htm/#timeframe-formats","title":"Timeframe Formats","text":"<p>String format (natural language):</p> <ul> <li><code>\"last week\"</code> - Last 7 days</li> <li><code>\"yesterday\"</code> - Previous day</li> <li><code>\"last N days\"</code> - Last N days (e.g., \"last 30 days\")</li> <li><code>\"this month\"</code> - Current month to now</li> <li><code>\"last month\"</code> - Previous calendar month</li> <li>Default (unrecognized) - Last 24 hours</li> </ul> <p>Range format:</p> <ul> <li><code>Time</code> range: <code>(Time.now - 7*24*3600)..Time.now</code></li> <li><code>Date</code> range: <code>Date.today-7..Date.today</code></li> </ul>"},{"location":"api/htm/#search-strategies","title":"Search Strategies","text":"Strategy Description Use Case <code>:vector</code> Semantic similarity using embeddings Find conceptually related content <code>:fulltext</code> PostgreSQL full-text search Find exact terms and phrases <code>:hybrid</code> Vector + fulltext + tag matching Best accuracy with tag boosting"},{"location":"api/htm/#tag-enhanced-hybrid-search","title":"Tag-Enhanced Hybrid Search","text":"<p>When using <code>:hybrid</code> strategy, the search automatically:</p> <ol> <li>Finds tags matching query terms (words 3+ chars)</li> <li>Includes nodes with matching tags in the candidate pool</li> <li>Calculates combined score: <code>(similarity \u00d7 0.7) + (tag_boost \u00d7 0.3)</code></li> <li>Returns results sorted by combined score</li> </ol>"},{"location":"api/htm/#returns_2","title":"Returns","text":"<ul> <li><code>Array&lt;String&gt;</code> - Content strings (when <code>raw: false</code>, default)</li> <li><code>Array&lt;Hash&gt;</code> - Full node hashes (when <code>raw: true</code>)</li> </ul> <p>When <code>raw: true</code>, each hash contains:</p> <pre><code>{\n  \"id\" =&gt; 123,                    # Database ID\n  \"content\" =&gt; \"...\",             # Node content\n  \"content_hash\" =&gt; \"abc123...\",  # SHA-256 hash\n  \"access_count\" =&gt; 5,            # Times accessed\n  \"created_at\" =&gt; \"2025-01-15...\", # Creation timestamp\n  \"token_count\" =&gt; 125,           # Token count\n  \"metadata\" =&gt; { \"category\" =&gt; \"preference\", \"priority\" =&gt; \"high\" },  # JSONB metadata\n  \"similarity\" =&gt; 0.87,           # Similarity score (hybrid/vector)\n  \"tag_boost\" =&gt; 0.3,             # Tag boost score (hybrid only)\n  \"combined_score\" =&gt; 0.79        # Combined score (hybrid only)\n}\n</code></pre>"},{"location":"api/htm/#side-effects_1","title":"Side Effects","text":"<ul> <li>Adds recalled nodes to working memory</li> <li>Evicts existing nodes if working memory is full</li> <li>Updates robot activity timestamp</li> </ul>"},{"location":"api/htm/#examples_2","title":"Examples","text":"<pre><code># Basic usage (returns content strings)\nmemories = htm.recall(\"PostgreSQL\")\n# =&gt; [\"PostgreSQL supports vector search...\", \"PostgreSQL with pgvector...\"]\n\n# Get full node hashes\nnodes = htm.recall(\"PostgreSQL\", raw: true)\n# =&gt; [{\"id\" =&gt; 1, \"content\" =&gt; \"...\", \"similarity\" =&gt; 0.92, ...}, ...]\n\n# Vector semantic search\nmemories = htm.recall(\n  \"database performance optimization\",\n  timeframe: \"last week\",\n  strategy: :vector\n)\n\n# Fulltext search for exact phrases\nmemories = htm.recall(\n  \"PostgreSQL connection pooling\",\n  timeframe: \"last 30 days\",\n  strategy: :fulltext,\n  limit: 10\n)\n\n# Hybrid search with tag boosting (recommended)\nmemories = htm.recall(\n  \"API rate limiting implementation\",\n  timeframe: \"this month\",\n  strategy: :hybrid,\n  limit: 15,\n  raw: true\n)\n\n# Check matching tags for a query\nmatching_tags = htm.long_term_memory.find_query_matching_tags(\"PostgreSQL\")\n# =&gt; [\"database:postgresql\", \"database:postgresql:extensions\"]\n\n# Custom time range\nstart_time = Time.new(2025, 1, 1)\nend_time = Time.now\n\nmemories = htm.recall(\n  \"security vulnerabilities\",\n  timeframe: start_time..end_time,\n  limit: 50\n)\n\n# Filter by metadata\nmemories = htm.recall(\n  \"user preferences\",\n  metadata: { category: \"preference\" }\n)\n# =&gt; Returns only nodes with metadata containing { category: \"preference\" }\n\n# Combine metadata with other filters\nmemories = htm.recall(\n  \"API configuration\",\n  timeframe: \"last month\",\n  strategy: :hybrid,\n  metadata: { environment: \"production\", version: 2 },\n  raw: true\n)\n# =&gt; Returns production configs with version 2, sorted by relevance\n</code></pre>"},{"location":"api/htm/#performance-notes","title":"Performance Notes","text":"<ul> <li>Vector search: Best for semantic understanding, requires embedding generation</li> <li>Fulltext search: Fastest for exact matches, no embedding overhead</li> <li>Hybrid search: Most accurate, combines vector + fulltext + tags with weighted scoring</li> </ul>"},{"location":"api/htm/#forget","title":"<code>forget(node_id, confirm:)</code>","text":"<p>Explicitly delete a memory node. Requires confirmation to prevent accidental deletion.</p> <pre><code>forget(node_id, confirm: :confirmed)\n</code></pre>"},{"location":"api/htm/#parameters_3","title":"Parameters","text":"Parameter Type Description <code>node_id</code> Integer ID of the node to delete <code>confirm</code> Symbol Must be <code>:confirmed</code> to proceed"},{"location":"api/htm/#returns_3","title":"Returns","text":"<ul> <li><code>true</code> - If successfully deleted</li> </ul>"},{"location":"api/htm/#raises","title":"Raises","text":"<ul> <li><code>ArgumentError</code> - If <code>confirm</code> is not <code>:confirmed</code></li> <li><code>ArgumentError</code> - If <code>node_id</code> is nil</li> <li><code>HTM::NotFoundError</code> - If node doesn't exist</li> </ul>"},{"location":"api/htm/#side-effects_2","title":"Side Effects","text":"<ul> <li>Deletes node from PostgreSQL</li> <li>Removes node from working memory</li> <li>Updates robot activity timestamp</li> </ul>"},{"location":"api/htm/#examples_3","title":"Examples","text":"<pre><code># Correct usage\nhtm.forget(123, confirm: :confirmed)\n\n# This will raise ArgumentError\nhtm.forget(123)  # Missing confirm parameter\n\n# Safe deletion with verification\nif htm.long_term_memory.exists?(node_id)\n  htm.forget(node_id, confirm: :confirmed)\n  puts \"Deleted node #{node_id}\"\nend\n</code></pre>"},{"location":"api/htm/#notes_1","title":"Notes","text":"<ul> <li>This is the only way to delete data from HTM</li> <li>Deletion is permanent and cannot be undone</li> <li>Related robot_nodes, node_tags are also deleted (CASCADE)</li> <li>Other robots' associations to this node are also removed</li> </ul>"},{"location":"api/htm/#load_file","title":"<code>load_file(path, force: false)</code>","text":"<p>Load a markdown file into long-term memory with automatic chunking and source tracking.</p> <pre><code>load_file(path, force: false)\n</code></pre>"},{"location":"api/htm/#parameters_4","title":"Parameters","text":"Parameter Type Default Description <code>path</code> String required Path to the markdown file to load <code>force</code> Boolean <code>false</code> Force re-sync even if file hasn't changed"},{"location":"api/htm/#returns_4","title":"Returns","text":"<ul> <li><code>Hash</code> with keys:</li> <li><code>file_source_id</code> - ID of the FileSource record</li> <li><code>chunks_created</code> - Number of new nodes created</li> <li><code>chunks_updated</code> - Number of existing nodes updated</li> <li><code>chunks_deleted</code> - Number of nodes soft-deleted</li> </ul>"},{"location":"api/htm/#side-effects_3","title":"Side Effects","text":"<ul> <li>Creates or updates a FileSource record for tracking</li> <li>Parses YAML frontmatter and stores as metadata</li> <li>Chunks content by paragraph, preserving code blocks</li> <li>Creates nodes for each chunk with <code>source_id</code> linking to file</li> <li>Triggers async embedding and tag extraction for new nodes</li> </ul>"},{"location":"api/htm/#examples_4","title":"Examples","text":"<pre><code># Load a file\nresult = htm.load_file(\"docs/guide.md\")\n# =&gt; { file_source_id: 1, chunks_created: 5, chunks_updated: 0, chunks_deleted: 0 }\n\n# Force reload even if unchanged\nresult = htm.load_file(\"docs/guide.md\", force: true)\n\n# File with frontmatter\n# ---\n# title: User Guide\n# tags: [documentation, tutorial]\n# ---\n# Content here...\nresult = htm.load_file(\"docs/guide.md\")\n# Frontmatter stored in FileSource.frontmatter\n</code></pre>"},{"location":"api/htm/#load_directory","title":"<code>load_directory(path, pattern: '**/*.md', force: false)</code>","text":"<p>Load all matching files in a directory into long-term memory.</p> <pre><code>load_directory(path, pattern: '**/*.md', force: false)\n</code></pre>"},{"location":"api/htm/#parameters_5","title":"Parameters","text":"Parameter Type Default Description <code>path</code> String required Directory path to scan <code>pattern</code> String <code>'**/*.md'</code> Glob pattern for matching files <code>force</code> Boolean <code>false</code> Force re-sync all files"},{"location":"api/htm/#returns_5","title":"Returns","text":"<ul> <li><code>Array&lt;Hash&gt;</code> - Results for each file loaded, each containing:</li> <li><code>file_path</code> - Path of the loaded file</li> <li><code>file_source_id</code> - ID of the FileSource record</li> <li><code>chunks_created</code> - Number of new nodes created</li> <li><code>chunks_updated</code> - Number of existing nodes updated</li> <li><code>chunks_deleted</code> - Number of nodes soft-deleted</li> </ul>"},{"location":"api/htm/#examples_5","title":"Examples","text":"<pre><code># Load all markdown files\nresults = htm.load_directory(\"docs/\")\n\n# Load with custom pattern\nresults = htm.load_directory(\"content/\", pattern: \"**/*.md\")\n\n# Force reload all\nresults = htm.load_directory(\"docs/\", force: true)\n</code></pre>"},{"location":"api/htm/#nodes_from_file","title":"<code>nodes_from_file(file_path)</code>","text":"<p>Get all nodes loaded from a specific file.</p> <pre><code>nodes_from_file(file_path)\n</code></pre>"},{"location":"api/htm/#parameters_6","title":"Parameters","text":"Parameter Type Description <code>file_path</code> String Path of the source file"},{"location":"api/htm/#returns_6","title":"Returns","text":"<ul> <li><code>Array&lt;HTM::Models::Node&gt;</code> - Nodes from the file, ordered by chunk position</li> </ul>"},{"location":"api/htm/#examples_6","title":"Examples","text":"<pre><code>nodes = htm.nodes_from_file(\"docs/guide.md\")\nnodes.each do |node|\n  puts \"Chunk #{node.chunk_position}: #{node.content[0..50]}...\"\nend\n</code></pre>"},{"location":"api/htm/#unload_file","title":"<code>unload_file(file_path)</code>","text":"<p>Remove a file from memory by soft-deleting all its chunks and the file source.</p> <pre><code>unload_file(file_path)\n</code></pre>"},{"location":"api/htm/#parameters_7","title":"Parameters","text":"Parameter Type Description <code>file_path</code> String Path of the source file to unload"},{"location":"api/htm/#returns_7","title":"Returns","text":"<ul> <li><code>true</code> if file was found and unloaded</li> <li><code>false</code> if file was not found</li> </ul>"},{"location":"api/htm/#side-effects_4","title":"Side Effects","text":"<ul> <li>Soft-deletes all nodes from the file (sets <code>deleted_at</code>)</li> <li>Destroys the FileSource record</li> </ul>"},{"location":"api/htm/#examples_7","title":"Examples","text":"<pre><code># Unload a file\nhtm.unload_file(\"docs/guide.md\")\n\n# Check if file is loaded\nif htm.nodes_from_file(\"docs/guide.md\").empty?\n  puts \"File not loaded\"\nend\n</code></pre>"},{"location":"api/htm/#error-handling","title":"Error Handling","text":""},{"location":"api/htm/#argumenterror","title":"ArgumentError","text":"<pre><code># Invalid confirm parameter\nhtm.forget(123)\n# =&gt; ArgumentError: Must pass confirm: :confirmed to delete\n\n# Nil node_id\nhtm.forget(nil, confirm: :confirmed)\n# =&gt; ArgumentError: node_id cannot be nil\n\n# Invalid timeframe\nhtm.recall(\"test\", timeframe: 123)\n# =&gt; ValidationError: Timeframe must be a Range or String\n</code></pre>"},{"location":"api/htm/#htmnotfounderror","title":"HTM::NotFoundError","text":"<pre><code># Node doesn't exist\nhtm.forget(999999, confirm: :confirmed)\n# =&gt; HTM::NotFoundError: Node not found: 999999\n</code></pre>"},{"location":"api/htm/#pgerror","title":"PG::Error","text":"<pre><code># Database connection issues\nhtm = HTM.new(db_config: { host: 'invalid' })\n# =&gt; PG::ConnectionBad: could not translate host name...\n</code></pre>"},{"location":"api/htm/#best-practices","title":"Best Practices","text":""},{"location":"api/htm/#content-organization","title":"Content Organization","text":"<pre><code># Use meaningful content that stands alone\nhtm.remember(\"PostgreSQL was chosen for its reliability and pgvector support\")\n\n# Add hierarchical tags for organization\nhtm.remember(\n  \"Rate limiting implemented using Redis sliding window algorithm\",\n  tags: [\"architecture:api:rate-limiting\", \"database:redis\"]\n)\n\n# Let the system extract tags automatically for most content\nhtm.remember(\"The authentication system uses JWT tokens with 1-hour expiry\")\n# Auto-extracted tags might include: security:authentication, technology:jwt\n</code></pre>"},{"location":"api/htm/#search-strategies_1","title":"Search Strategies","text":"<pre><code># Use hybrid for best results (recommended)\nmemories = htm.recall(\n  \"security vulnerability\",\n  strategy: :hybrid  # Combines vector + fulltext + tags\n)\n\n# Use vector for semantic understanding\nmemories = htm.recall(\n  \"performance issues\",\n  strategy: :vector  # Finds \"slow queries\", \"optimization\", etc.\n)\n\n# Use fulltext for exact terms\nmemories = htm.recall(\n  \"PostgreSQL EXPLAIN ANALYZE\",\n  strategy: :fulltext  # Exact match\n)\n</code></pre>"},{"location":"api/htm/#leveraging-tag-enhanced-search","title":"Leveraging Tag-Enhanced Search","text":"<pre><code># Check what tags exist for a topic\ntags = htm.long_term_memory.find_query_matching_tags(\"database\")\n# =&gt; [\"database:postgresql\", \"database:redis\", \"database:timescaledb\"]\n\n# Hybrid search automatically boosts nodes with matching tags\nmemories = htm.recall(\"database optimization\", strategy: :hybrid, raw: true)\nmemories.each do |m|\n  puts \"Score: #{m['combined_score']} (sim: #{m['similarity']}, tag: #{m['tag_boost']})\"\nend\n</code></pre>"},{"location":"api/htm/#multi-robot-memory-sharing","title":"Multi-Robot Memory Sharing","text":"<pre><code># Content is deduplicated across robots\nassistant = HTM.new(robot_name: \"assistant\")\nresearcher = HTM.new(robot_name: \"researcher\")\n\n# Both robots remember the same fact\nassistant.remember(\"Ruby 3.3 supports YJIT by default\")\nresearcher.remember(\"Ruby 3.3 supports YJIT by default\")\n# Node stored once, linked to both robots\n\n# Any robot can recall shared memories\nmemories = assistant.recall(\"Ruby YJIT\")\n# Returns the shared memory\n</code></pre>"},{"location":"api/htm/#resource-management","title":"Resource Management","text":"<pre><code># Check working memory before large operations\nstats = htm.working_memory.stats\nif stats[:utilization] &gt; 90\n  # Consider clearing working memory or using smaller limits\nend\n\n# Use appropriate limits\nhtm.recall(\"common_topic\", limit: 10)  # Not 1000\n\n# Monitor node counts\nnode_count = HTM::Models::Node.count\nif node_count &gt; 1_000_000\n  # Consider archival strategy\nend\n</code></pre>"},{"location":"api/htm/#see-also","title":"See Also","text":"<ul> <li>WorkingMemory API</li> <li>LongTermMemory API</li> <li>EmbeddingService API</li> <li>Database API</li> </ul>"},{"location":"api/long-term-memory/","title":"LongTermMemory Class","text":"<p>PostgreSQL-backed permanent memory storage with RAG-based retrieval.</p>"},{"location":"api/long-term-memory/#overview","title":"Overview","text":"<p><code>HTM::LongTermMemory</code> provides durable storage for all memory nodes with advanced search capabilities:</p> <ul> <li>Vector similarity search - Semantic understanding via embeddings</li> <li>Full-text search - Fast keyword and phrase matching</li> <li>Tag-enhanced hybrid search - Combines fulltext + vector + tag matching</li> <li>Content deduplication - SHA-256 based node deduplication</li> <li>Query result caching - LRU cache for frequent queries</li> <li>Hierarchical tagging - Colon-separated tag namespaces</li> </ul>"},{"location":"api/long-term-memory/#class-definition","title":"Class Definition","text":"<pre><code>class HTM::LongTermMemory\n  attr_reader :query_timeout\nend\n</code></pre>"},{"location":"api/long-term-memory/#initialization","title":"Initialization","text":""},{"location":"api/long-term-memory/#new","title":"<code>new(config, **options)</code>","text":"<p>Create a new long-term memory instance.</p> <pre><code>HTM::LongTermMemory.new(\n  config,\n  pool_size: nil,\n  query_timeout: 30_000,\n  cache_size: 1000,\n  cache_ttl: 300\n)\n</code></pre>"},{"location":"api/long-term-memory/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>config</code> Hash required PostgreSQL connection configuration <code>pool_size</code> Integer, nil <code>nil</code> Connection pool size (managed by ActiveRecord) <code>query_timeout</code> Integer <code>30_000</code> Query timeout in milliseconds <code>cache_size</code> Integer <code>1000</code> LRU cache size (0 to disable) <code>cache_ttl</code> Integer <code>300</code> Cache TTL in seconds"},{"location":"api/long-term-memory/#configuration-hash","title":"Configuration Hash","text":"<pre><code>{\n  host: \"hostname\",\n  port: 5432,\n  dbname: \"database_name\",\n  user: \"username\",\n  password: \"password\",\n  sslmode: \"require\"\n}\n</code></pre>"},{"location":"api/long-term-memory/#examples","title":"Examples","text":"<pre><code># From environment variable\nconfig = HTM::Database.default_config\nltm = HTM::LongTermMemory.new(config)\n\n# With custom timeout and cache\nltm = HTM::LongTermMemory.new(\n  config,\n  query_timeout: 60_000,  # 60 seconds\n  cache_size: 5000,\n  cache_ttl: 600\n)\n\n# Disable caching\nltm = HTM::LongTermMemory.new(config, cache_size: 0)\n</code></pre>"},{"location":"api/long-term-memory/#public-methods","title":"Public Methods","text":""},{"location":"api/long-term-memory/#add","title":"<code>add(**params)</code>","text":"<p>Add a node to long-term memory with content deduplication.</p> <pre><code>add(\n  content:,\n  token_count: 0,\n  robot_id:,\n  embedding: nil\n)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>content</code> String required Node content <code>token_count</code> Integer <code>0</code> Token count <code>robot_id</code> Integer required Robot identifier <code>embedding</code> Array\\&lt;Float&gt;, nil <code>nil</code> Pre-generated embedding vector"},{"location":"api/long-term-memory/#returns","title":"Returns","text":"<ul> <li><code>Hash</code> - <code>{ node_id:, is_new:, robot_node: }</code></li> </ul>"},{"location":"api/long-term-memory/#content-deduplication","title":"Content Deduplication","text":"<p>When <code>add()</code> is called:</p> <ol> <li>A SHA-256 hash of the content is computed</li> <li>If a node with the same hash exists:</li> <li>Links the robot to the existing node (or updates <code>remember_count</code>)</li> <li>Returns <code>is_new: false</code></li> <li>If no match:</li> <li>Creates a new node</li> <li>Links the robot to it</li> <li>Returns <code>is_new: true</code></li> </ol>"},{"location":"api/long-term-memory/#examples_1","title":"Examples","text":"<pre><code># Add new content\nresult = ltm.add(\n  content: \"PostgreSQL is our primary database\",\n  token_count: 8,\n  robot_id: 1\n)\n# =&gt; { node_id: 123, is_new: true, robot_node: &lt;RobotNode&gt; }\n\n# Add duplicate content (different robot)\nresult = ltm.add(\n  content: \"PostgreSQL is our primary database\",\n  token_count: 8,\n  robot_id: 2\n)\n# =&gt; { node_id: 123, is_new: false, robot_node: &lt;RobotNode&gt; }\n# Same node_id, robot_node tracks this robot's remember_count\n\n# With pre-generated embedding\nresult = ltm.add(\n  content: \"Vector search is powerful\",\n  token_count: 5,\n  robot_id: 1,\n  embedding: [0.1, 0.2, 0.3, ...]  # Will be padded to 2000 dims\n)\n</code></pre>"},{"location":"api/long-term-memory/#retrieve","title":"<code>retrieve(node_id)</code>","text":"<p>Retrieve a node by its database ID.</p> <pre><code>retrieve(node_id)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_2","title":"Parameters","text":"Parameter Type Description <code>node_id</code> Integer Node database ID"},{"location":"api/long-term-memory/#returns_1","title":"Returns","text":"<ul> <li><code>Hash</code> - Node attributes if found</li> <li><code>nil</code> - If node doesn't exist</li> </ul>"},{"location":"api/long-term-memory/#side-effects","title":"Side Effects","text":"<ul> <li>Increments <code>access_count</code></li> <li>Updates <code>last_accessed</code> timestamp</li> </ul>"},{"location":"api/long-term-memory/#examples_2","title":"Examples","text":"<pre><code>node = ltm.retrieve(123)\n\nif node\n  puts node['content']\n  puts \"Accessed #{node['access_count']} times\"\n  puts \"Created: #{node['created_at']}\"\nelse\n  puts \"Node not found\"\nend\n</code></pre>"},{"location":"api/long-term-memory/#exists","title":"<code>exists?(node_id)</code>","text":"<p>Check if a node exists.</p> <pre><code>exists?(node_id)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_3","title":"Parameters","text":"Parameter Type Description <code>node_id</code> Integer Node database ID"},{"location":"api/long-term-memory/#returns_2","title":"Returns","text":"<ul> <li><code>Boolean</code> - True if node exists</li> </ul>"},{"location":"api/long-term-memory/#examples_3","title":"Examples","text":"<pre><code>if ltm.exists?(123)\n  ltm.delete(123)\nend\n</code></pre>"},{"location":"api/long-term-memory/#delete","title":"<code>delete(node_id)</code>","text":"<p>Delete a node permanently.</p> <pre><code>delete(node_id)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_4","title":"Parameters","text":"Parameter Type Description <code>node_id</code> Integer Node database ID"},{"location":"api/long-term-memory/#side-effects_1","title":"Side Effects","text":"<ul> <li>Deletes node from database</li> <li>Cascades to robot_nodes and node_tags</li> <li>Invalidates query cache</li> </ul>"},{"location":"api/long-term-memory/#warning","title":"Warning","text":"<p>Deletion is permanent and cannot be undone. Use <code>HTM#forget</code> for proper confirmation flow.</p>"},{"location":"api/long-term-memory/#search","title":"<code>search(**params)</code>","text":"<p>Vector similarity search using embeddings.</p> <pre><code>search(\n  timeframe:,\n  query:,\n  limit:,\n  embedding_service:\n)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_5","title":"Parameters","text":"Parameter Type Description <code>timeframe</code> Range Time range to search (Time..Time) <code>query</code> String Search query text <code>limit</code> Integer Maximum results <code>embedding_service</code> Object Service to generate query embedding"},{"location":"api/long-term-memory/#returns_3","title":"Returns","text":"<ul> <li><code>Array&lt;Hash&gt;</code> - Matching nodes sorted by similarity (highest first)</li> </ul>"},{"location":"api/long-term-memory/#hash-structure","title":"Hash Structure","text":"<pre><code>{\n  \"id\" =&gt; 123,\n  \"content\" =&gt; \"content...\",\n  \"access_count\" =&gt; 5,\n  \"created_at\" =&gt; \"2025-01-15 10:30:00\",\n  \"token_count\" =&gt; 50,\n  \"similarity\" =&gt; 0.8745  # 0.0-1.0, higher = more similar\n}\n</code></pre>"},{"location":"api/long-term-memory/#examples_4","title":"Examples","text":"<pre><code>timeframe = (Time.now - 7*24*3600)..Time.now\n\nresults = ltm.search(\n  timeframe: timeframe,\n  query: \"database performance optimization\",\n  limit: 20,\n  embedding_service: HTM\n)\n\nresults.each do |node|\n  puts \"[#{node['similarity']}] #{node['content']}\"\nend\n</code></pre>"},{"location":"api/long-term-memory/#search_fulltext","title":"<code>search_fulltext(**params)</code>","text":"<p>Full-text search using PostgreSQL's text search.</p> <pre><code>search_fulltext(\n  timeframe:,\n  query:,\n  limit:\n)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_6","title":"Parameters","text":"Parameter Type Description <code>timeframe</code> Range Time range to search <code>query</code> String Search query <code>limit</code> Integer Maximum results"},{"location":"api/long-term-memory/#returns_4","title":"Returns","text":"<ul> <li><code>Array&lt;Hash&gt;</code> - Matching nodes sorted by rank (highest first)</li> </ul>"},{"location":"api/long-term-memory/#hash-structure_1","title":"Hash Structure","text":"<pre><code>{\n  ...,\n  \"rank\" =&gt; 0.456  # Higher = better match\n}\n</code></pre>"},{"location":"api/long-term-memory/#examples_5","title":"Examples","text":"<pre><code>results = ltm.search_fulltext(\n  timeframe: (Time.now - 30*24*3600)..Time.now,\n  query: \"PostgreSQL connection pooling\",\n  limit: 10\n)\n</code></pre>"},{"location":"api/long-term-memory/#search_hybrid","title":"<code>search_hybrid(**params)</code>","text":"<p>Tag-enhanced hybrid search combining fulltext, vector, and tag matching.</p> <pre><code>search_hybrid(\n  timeframe:,\n  query:,\n  limit:,\n  embedding_service:,\n  prefilter_limit: 100\n)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_7","title":"Parameters","text":"Parameter Type Default Description <code>timeframe</code> Range required Time range to search <code>query</code> String required Search query <code>limit</code> Integer required Maximum final results <code>embedding_service</code> Object required Service for embeddings <code>prefilter_limit</code> Integer <code>100</code> Candidates to consider"},{"location":"api/long-term-memory/#returns_5","title":"Returns","text":"<ul> <li><code>Array&lt;Hash&gt;</code> - Matching nodes with combined scores</li> </ul>"},{"location":"api/long-term-memory/#hash-structure_2","title":"Hash Structure","text":"<pre><code>{\n  \"id\" =&gt; 123,\n  \"content\" =&gt; \"...\",\n  \"similarity\" =&gt; 0.87,       # Vector similarity (0-1)\n  \"tag_boost\" =&gt; 0.3,         # Tag match score (0-1)\n  \"combined_score\" =&gt; 0.79    # (similarity \u00d7 0.7) + (tag_boost \u00d7 0.3)\n}\n</code></pre>"},{"location":"api/long-term-memory/#strategy","title":"Strategy","text":"<ol> <li>Find matching tags: Searches tags for query term matches</li> <li>Build candidate pool: Fulltext matches + tag-matching nodes</li> <li>Score candidates: Vector similarity + tag boost</li> <li>Return top results: Sorted by combined_score</li> </ol>"},{"location":"api/long-term-memory/#examples_6","title":"Examples","text":"<pre><code>results = ltm.search_hybrid(\n  timeframe: (Time.now - 30*24*3600)..Time.now,\n  query: \"PostgreSQL performance\",\n  limit: 15,\n  embedding_service: HTM\n)\n\nresults.each do |node|\n  puts \"#{node['content']}\"\n  puts \"  Similarity: #{node['similarity']}\"\n  puts \"  Tag boost: #{node['tag_boost']}\"\n  puts \"  Combined: #{node['combined_score']}\"\nend\n</code></pre>"},{"location":"api/long-term-memory/#find_query_matching_tags","title":"<code>find_query_matching_tags(query)</code>","text":"<p>Find tags that match terms in the query.</p> <pre><code>find_query_matching_tags(query)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_8","title":"Parameters","text":"Parameter Type Description <code>query</code> String Search query"},{"location":"api/long-term-memory/#returns_6","title":"Returns","text":"<ul> <li><code>Array&lt;String&gt;</code> - Matching tag names</li> </ul>"},{"location":"api/long-term-memory/#how-it-works","title":"How It Works","text":"<ol> <li>Extracts words from query (3+ chars, lowercase)</li> <li>Searches tags where any hierarchy level matches (ILIKE)</li> <li>Returns all matching tag names</li> </ol>"},{"location":"api/long-term-memory/#examples_7","title":"Examples","text":"<pre><code># Query: \"PostgreSQL database optimization\"\n# Might return: [\"database:postgresql\", \"database:optimization\", \"database:sql\"]\n\nmatching_tags = ltm.find_query_matching_tags(\"PostgreSQL database\")\n# =&gt; [\"database:postgresql\", \"database:postgresql:extensions\"]\n</code></pre>"},{"location":"api/long-term-memory/#add_tag","title":"<code>add_tag(node_id:, tag:)</code>","text":"<p>Add a tag to a node.</p> <pre><code>add_tag(node_id:, tag:)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_9","title":"Parameters","text":"Parameter Type Description <code>node_id</code> Integer Node database ID <code>tag</code> String Tag name"},{"location":"api/long-term-memory/#examples_8","title":"Examples","text":"<pre><code>ltm.add_tag(node_id: 123, tag: \"database:postgresql\")\nltm.add_tag(node_id: 123, tag: \"architecture:decision\")\n</code></pre>"},{"location":"api/long-term-memory/#get_node_tags","title":"<code>get_node_tags(node_id)</code>","text":"<p>Get tags for a specific node.</p> <pre><code>get_node_tags(node_id)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_10","title":"Parameters","text":"Parameter Type Description <code>node_id</code> Integer Node database ID"},{"location":"api/long-term-memory/#returns_7","title":"Returns","text":"<ul> <li><code>Array&lt;String&gt;</code> - Tag names</li> </ul>"},{"location":"api/long-term-memory/#examples_9","title":"Examples","text":"<pre><code>tags = ltm.get_node_tags(123)\n# =&gt; [\"database:postgresql\", \"architecture:decision\"]\n</code></pre>"},{"location":"api/long-term-memory/#node_topics","title":"<code>node_topics(node_id)</code>","text":"<p>Alias for <code>get_node_tags</code> - returns topics/tags for a node.</p> <pre><code>node_topics(node_id)\n</code></pre>"},{"location":"api/long-term-memory/#nodes_by_topic","title":"<code>nodes_by_topic(topic_path, exact:, limit:)</code>","text":"<p>Retrieve nodes by tag/topic.</p> <pre><code>nodes_by_topic(topic_path, exact: false, limit: 50)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_11","title":"Parameters","text":"Parameter Type Default Description <code>topic_path</code> String required Topic hierarchy path <code>exact</code> Boolean <code>false</code> Exact match or prefix match <code>limit</code> Integer <code>50</code> Maximum results"},{"location":"api/long-term-memory/#returns_8","title":"Returns","text":"<ul> <li><code>Array&lt;Hash&gt;</code> - Matching node attributes</li> </ul>"},{"location":"api/long-term-memory/#examples_10","title":"Examples","text":"<pre><code># Prefix match (default) - finds all database-related nodes\nnodes = ltm.nodes_by_topic(\"database\")\n\n# Exact match - only nodes tagged with exactly \"database:postgresql\"\nnodes = ltm.nodes_by_topic(\"database:postgresql\", exact: true)\n</code></pre>"},{"location":"api/long-term-memory/#search_by_tags","title":"<code>search_by_tags(**params)</code>","text":"<p>Search nodes by tags with relevance scoring.</p> <pre><code>search_by_tags(\n  tags:,\n  match_all: false,\n  timeframe: nil,\n  limit: 20\n)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_12","title":"Parameters","text":"Parameter Type Default Description <code>tags</code> Array\\&lt;String&gt; required Tags to search for <code>match_all</code> Boolean <code>false</code> Match ALL tags or ANY tag <code>timeframe</code> Range, nil <code>nil</code> Optional time range filter <code>limit</code> Integer <code>20</code> Maximum results"},{"location":"api/long-term-memory/#returns_9","title":"Returns","text":"<ul> <li><code>Array&lt;Hash&gt;</code> - Nodes with relevance scores and tags</li> </ul>"},{"location":"api/long-term-memory/#examples_11","title":"Examples","text":"<pre><code># Match ANY tag\nnodes = ltm.search_by_tags(tags: [\"database\", \"api\"])\n\n# Match ALL tags\nnodes = ltm.search_by_tags(\n  tags: [\"database:postgresql\", \"architecture\"],\n  match_all: true\n)\n\n# With timeframe\nnodes = ltm.search_by_tags(\n  tags: [\"security\"],\n  timeframe: (Time.now - 7*24*3600)..Time.now\n)\n</code></pre>"},{"location":"api/long-term-memory/#popular_tags","title":"<code>popular_tags(limit:, timeframe:)</code>","text":"<p>Get most frequently used tags.</p> <pre><code>popular_tags(limit: 20, timeframe: nil)\n</code></pre>"},{"location":"api/long-term-memory/#returns_10","title":"Returns","text":"<ul> <li><code>Array&lt;Hash&gt;</code> - <code>[{ name: \"tag_name\", usage_count: 42 }, ...]</code></li> </ul>"},{"location":"api/long-term-memory/#examples_12","title":"Examples","text":"<pre><code>top_tags = ltm.popular_tags(limit: 10)\ntop_tags.each do |tag|\n  puts \"#{tag[:name]}: #{tag[:usage_count]} nodes\"\nend\n</code></pre>"},{"location":"api/long-term-memory/#topic_relationships","title":"<code>topic_relationships(min_shared_nodes:, limit:)</code>","text":"<p>Get tag co-occurrence relationships.</p> <pre><code>topic_relationships(min_shared_nodes: 2, limit: 50)\n</code></pre>"},{"location":"api/long-term-memory/#returns_11","title":"Returns","text":"<ul> <li><code>Array&lt;Hash&gt;</code> - <code>[{ topic1:, topic2:, shared_nodes: }, ...]</code></li> </ul>"},{"location":"api/long-term-memory/#examples_13","title":"Examples","text":"<pre><code>related = ltm.topic_relationships(min_shared_nodes: 3)\nrelated.each do |r|\n  puts \"#{r['topic1']} &lt;-&gt; #{r['topic2']}: #{r['shared_nodes']} shared\"\nend\n</code></pre>"},{"location":"api/long-term-memory/#register_robot","title":"<code>register_robot(robot_name)</code>","text":"<p>Register a robot in the system.</p> <pre><code>register_robot(robot_name)\n</code></pre>"},{"location":"api/long-term-memory/#returns_12","title":"Returns","text":"<ul> <li><code>Integer</code> - Robot ID</li> </ul>"},{"location":"api/long-term-memory/#examples_14","title":"Examples","text":"<pre><code>robot_id = ltm.register_robot(\"Code Assistant\")\n</code></pre>"},{"location":"api/long-term-memory/#update_robot_activity","title":"<code>update_robot_activity(robot_id)</code>","text":"<p>Update robot's last activity timestamp.</p> <pre><code>update_robot_activity(robot_id)\n</code></pre>"},{"location":"api/long-term-memory/#mark_evicted","title":"<code>mark_evicted(node_ids)</code>","text":"<p>Mark nodes as evicted from working memory.</p> <pre><code>mark_evicted(node_ids)\n</code></pre>"},{"location":"api/long-term-memory/#parameters_13","title":"Parameters","text":"Parameter Type Description <code>node_ids</code> Array\\&lt;Integer&gt; Node IDs to mark"},{"location":"api/long-term-memory/#track_access","title":"<code>track_access(node_ids)</code>","text":"<p>Track access for multiple nodes (bulk update).</p> <pre><code>track_access(node_ids)\n</code></pre> <p>Updates <code>access_count</code> and <code>last_accessed</code> for all specified nodes.</p>"},{"location":"api/long-term-memory/#stats","title":"<code>stats()</code>","text":"<p>Get comprehensive memory statistics.</p> <pre><code>stats()\n</code></pre>"},{"location":"api/long-term-memory/#returns_13","title":"Returns","text":"<pre><code>{\n  total_nodes: 1234,\n  nodes_by_robot: { 1 =&gt; 500, 2 =&gt; 734 },\n  total_tags: 890,\n  oldest_memory: Time,\n  newest_memory: Time,\n  active_robots: 3,\n  robot_activity: [{ id:, name:, last_active: }, ...],\n  database_size: 12345678,  # bytes\n  cache: {                  # Only if cache enabled\n    hits: 150,\n    misses: 50,\n    hit_rate: 75.0,\n    size: 200\n  }\n}\n</code></pre>"},{"location":"api/long-term-memory/#database-schema","title":"Database Schema","text":""},{"location":"api/long-term-memory/#tables-used","title":"Tables Used","text":""},{"location":"api/long-term-memory/#nodes","title":"<code>nodes</code>","text":"<p>Primary memory storage:</p> <ul> <li><code>id</code> - BIGSERIAL primary key</li> <li><code>content</code> - TEXT (the memory content)</li> <li><code>content_hash</code> - VARCHAR(64) UNIQUE (SHA-256 for deduplication)</li> <li><code>access_count</code> - INTEGER (retrieval count)</li> <li><code>token_count</code> - INTEGER</li> <li><code>embedding</code> - vector(2000)</li> <li><code>embedding_dimension</code> - INTEGER</li> <li><code>created_at</code>, <code>updated_at</code>, <code>last_accessed</code> - TIMESTAMPTZ</li> <li><code>in_working_memory</code> - BOOLEAN</li> </ul>"},{"location":"api/long-term-memory/#robot_nodes","title":"<code>robot_nodes</code>","text":"<p>Robot-node associations (many-to-many):</p> <ul> <li><code>id</code> - BIGSERIAL primary key</li> <li><code>robot_id</code> - BIGINT FK</li> <li><code>node_id</code> - BIGINT FK</li> <li><code>first_remembered_at</code>, <code>last_remembered_at</code> - TIMESTAMPTZ</li> <li><code>remember_count</code> - INTEGER</li> </ul>"},{"location":"api/long-term-memory/#tags","title":"<code>tags</code>","text":"<p>Hierarchical tag registry:</p> <ul> <li><code>id</code> - BIGSERIAL primary key</li> <li><code>name</code> - TEXT UNIQUE (colon-separated hierarchy)</li> <li><code>created_at</code> - TIMESTAMPTZ</li> </ul>"},{"location":"api/long-term-memory/#node_tags","title":"<code>node_tags</code>","text":"<p>Node-tag associations (many-to-many):</p> <ul> <li><code>node_id</code> - BIGINT FK</li> <li><code>tag_id</code> - BIGINT FK</li> </ul>"},{"location":"api/long-term-memory/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/long-term-memory/#query-caching","title":"Query Caching","text":"<p>Results are cached in an LRU cache with TTL:</p> <pre><code># Check cache stats\nstats = ltm.stats\nputs \"Cache hit rate: #{stats[:cache][:hit_rate]}%\"\n</code></pre> <p>Cache is automatically invalidated when: - Nodes are added - Nodes are deleted</p>"},{"location":"api/long-term-memory/#indexing","title":"Indexing","text":"<p>Automatic indexes:</p> <ul> <li><code>content_hash</code> - UNIQUE index for deduplication</li> <li><code>embedding</code> - HNSW index for vector search</li> <li><code>content</code> - GIN indexes for fulltext and trigram search</li> <li><code>created_at</code> - B-tree for time-range queries</li> <li><code>robot_nodes</code> and <code>node_tags</code> - Indexes on foreign keys</li> </ul>"},{"location":"api/long-term-memory/#query-optimization","title":"Query Optimization","text":"<pre><code># Good: Time-limited searches\nltm.search(timeframe: (Time.now - 7*24*3600)..Time.now, ...)\n\n# Bad: All-time searches (slow)\nltm.search(timeframe: (Time.at(0)..Time.now), ...)\n\n# Good: Reasonable limits\nltm.search_fulltext(query: \"...\", limit: 20)\n</code></pre>"},{"location":"api/long-term-memory/#error-handling","title":"Error Handling","text":""},{"location":"api/long-term-memory/#pgerror","title":"PG::Error","text":"<pre><code># Connection errors\nltm = HTM::LongTermMemory.new(invalid_config)\n# =&gt; PG::ConnectionBad\n\n# Unique constraint violations (rare with deduplication)\n# =&gt; PG::UniqueViolation\n</code></pre>"},{"location":"api/long-term-memory/#best-practices","title":"Best Practices","text":"<pre><code># Check existence before operations\nif ltm.exists?(node_id)\n  ltm.delete(node_id)\nend\n\n# Use HTM#forget for safe deletion with confirmation\nhtm.forget(node_id, confirm: :confirmed)\n</code></pre>"},{"location":"api/long-term-memory/#see-also","title":"See Also","text":"<ul> <li>HTM API - Main class that uses LongTermMemory</li> <li>WorkingMemory API - Token-limited active context</li> <li>Database Schema - Full schema documentation</li> </ul>"},{"location":"api/working-memory/","title":"WorkingMemory Class","text":"<p>Token-limited active context for immediate LLM use.</p>"},{"location":"api/working-memory/#overview","title":"Overview","text":"<p><code>HTM::WorkingMemory</code> manages the active conversation context within strict token limits. When capacity is reached, it intelligently evicts less important or older nodes back to long-term storage.</p> <p>Key Features:</p> <ul> <li>Token-based capacity management</li> <li>LRU (Least Recently Used) tracking</li> <li>Importance-weighted eviction</li> <li>Multiple context assembly strategies</li> <li>Real-time utilization monitoring</li> </ul>"},{"location":"api/working-memory/#class-definition","title":"Class Definition","text":"<pre><code>class HTM::WorkingMemory\n  attr_reader :max_tokens\nend\n</code></pre>"},{"location":"api/working-memory/#initialization","title":"Initialization","text":""},{"location":"api/working-memory/#new","title":"<code>new(max_tokens:)</code>","text":"<p>Create a new working memory instance.</p> <pre><code>HTM::WorkingMemory.new(max_tokens: 128_000)\n</code></pre>"},{"location":"api/working-memory/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>max_tokens</code> Integer required Maximum tokens allowed in working memory"},{"location":"api/working-memory/#returns","title":"Returns","text":"<ul> <li><code>HTM::WorkingMemory</code> instance</li> </ul>"},{"location":"api/working-memory/#examples","title":"Examples","text":"<pre><code># Standard working memory (128K tokens)\nwm = HTM::WorkingMemory.new(max_tokens: 128_000)\n\n# Large context window (256K tokens)\nwm = HTM::WorkingMemory.new(max_tokens: 256_000)\n\n# Small working memory (32K tokens)\nwm = HTM::WorkingMemory.new(max_tokens: 32_000)\n</code></pre>"},{"location":"api/working-memory/#instance-attributes","title":"Instance Attributes","text":""},{"location":"api/working-memory/#max_tokens","title":"<code>max_tokens</code>","text":"<p>Maximum token capacity for this working memory.</p> <ul> <li>Type: Integer</li> <li>Read-only: Yes</li> </ul> <pre><code>wm.max_tokens  # =&gt; 128000\n</code></pre>"},{"location":"api/working-memory/#public-methods","title":"Public Methods","text":""},{"location":"api/working-memory/#add","title":"<code>add(key, value, **options)</code>","text":"<p>Add a node to working memory.</p> <pre><code>add(key, value,\n  token_count:,\n  importance: 1.0,\n  from_recall: false\n)\n</code></pre>"},{"location":"api/working-memory/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>key</code> String required Node identifier <code>value</code> String required Node content <code>token_count</code> Integer required Number of tokens in this node <code>importance</code> Float <code>1.0</code> Importance score (0.0-10.0) <code>from_recall</code> Boolean <code>false</code> Whether this node was recalled from long-term memory"},{"location":"api/working-memory/#returns_1","title":"Returns","text":"<ul> <li><code>void</code></li> </ul>"},{"location":"api/working-memory/#side-effects","title":"Side Effects","text":"<ul> <li>Adds node to internal hash</li> <li>Updates access order (LRU tracking)</li> <li>Records timestamp</li> </ul>"},{"location":"api/working-memory/#examples_1","title":"Examples","text":"<pre><code># Add a simple node\nwm.add(\"fact_001\", \"PostgreSQL is our database\",\n  token_count: 50,\n  importance: 7.0\n)\n\n# Add recalled node\nwm.add(\"decision_001\", \"Use microservices architecture\",\n  token_count: 120,\n  importance: 9.0,\n  from_recall: true\n)\n\n# Add low-importance temporary note\nwm.add(\"temp_note\", \"Check deployment at 3pm\",\n  token_count: 30,\n  importance: 2.0\n)\n</code></pre>"},{"location":"api/working-memory/#notes","title":"Notes","text":"<ul> <li>Does not check capacity - use <code>has_space?</code> first</li> <li>Updates existing node if key already exists</li> <li>Automatically updates access order for LRU</li> </ul>"},{"location":"api/working-memory/#remove","title":"<code>remove(key)</code>","text":"<p>Remove a node from working memory.</p> <pre><code>remove(key)\n</code></pre>"},{"location":"api/working-memory/#parameters_2","title":"Parameters","text":"Parameter Type Description <code>key</code> String Node identifier"},{"location":"api/working-memory/#returns_2","title":"Returns","text":"<ul> <li><code>void</code></li> </ul>"},{"location":"api/working-memory/#side-effects_1","title":"Side Effects","text":"<ul> <li>Removes node from internal hash</li> <li>Removes from access order tracking</li> </ul>"},{"location":"api/working-memory/#examples_2","title":"Examples","text":"<pre><code># Remove a node\nwm.remove(\"temp_note_123\")\n\n# Safe removal (checks existence)\nif wm.respond_to?(:has_key?) &amp;&amp; wm.has_key?(\"old_key\")\n  wm.remove(\"old_key\")\nend\n</code></pre>"},{"location":"api/working-memory/#has_space","title":"<code>has_space?(token_count)</code>","text":"<p>Check if there's space for a node.</p> <pre><code>has_space?(token_count)\n</code></pre>"},{"location":"api/working-memory/#parameters_3","title":"Parameters","text":"Parameter Type Description <code>token_count</code> Integer Number of tokens needed"},{"location":"api/working-memory/#returns_3","title":"Returns","text":"<ul> <li><code>Boolean</code> - <code>true</code> if space available, <code>false</code> otherwise</li> </ul>"},{"location":"api/working-memory/#examples_3","title":"Examples","text":"<pre><code># Check before adding\nif wm.has_space?(500)\n  wm.add(\"new_node\", \"content...\", token_count: 500)\nelse\n  puts \"Working memory full, need to evict\"\nend\n\n# Check capacity for large addition\nlarge_content_tokens = 5000\nif wm.has_space?(large_content_tokens)\n  wm.add(\"large_node\", large_content, token_count: large_content_tokens)\nelse\n  # Evict to make space\n  evicted = wm.evict_to_make_space(large_content_tokens)\n  wm.add(\"large_node\", large_content, token_count: large_content_tokens)\nend\n</code></pre>"},{"location":"api/working-memory/#evict_to_make_space","title":"<code>evict_to_make_space(needed_tokens)</code>","text":"<p>Evict nodes to free up space for new content.</p> <pre><code>evict_to_make_space(needed_tokens)\n</code></pre>"},{"location":"api/working-memory/#parameters_4","title":"Parameters","text":"Parameter Type Description <code>needed_tokens</code> Integer Number of tokens needed"},{"location":"api/working-memory/#returns_4","title":"Returns","text":"<ul> <li><code>Array&lt;Hash&gt;</code> - Evicted nodes</li> </ul> <p>Each hash contains:</p> <pre><code>{\n  key: \"node_key\",\n  value: \"node content...\"\n}\n</code></pre>"},{"location":"api/working-memory/#eviction-strategy","title":"Eviction Strategy","text":"<p>Nodes are sorted by:</p> <ol> <li>Importance (ascending) - Lower importance evicted first</li> <li>Recency (descending) - Older nodes evicted first</li> </ol> <p>Formula: <code>[importance, -recency_in_seconds]</code></p>"},{"location":"api/working-memory/#side-effects_2","title":"Side Effects","text":"<ul> <li>Removes evicted nodes from working memory</li> <li>Updates access order tracking</li> </ul>"},{"location":"api/working-memory/#examples_4","title":"Examples","text":"<pre><code># Make space for 10,000 tokens\nevicted = wm.evict_to_make_space(10_000)\n\nputs \"Evicted #{evicted.length} nodes:\"\nevicted.each do |node|\n  puts \"  - #{node[:key]}\"\nend\n\n# Make space and log to long-term memory\nevicted = wm.evict_to_make_space(needed_tokens)\nevicted_keys = evicted.map { |n| n[:key] }\nlong_term_memory.mark_evicted(evicted_keys)\n\n# Check how much was freed\ntokens_freed = evicted.sum { |n| n[:token_count] }\nputs \"Freed #{tokens_freed} tokens\"\n</code></pre>"},{"location":"api/working-memory/#notes_1","title":"Notes","text":"<ul> <li>Evicts just enough to meet <code>needed_tokens</code></li> <li>May evict more than one node</li> <li>Preserves high-importance and recent nodes</li> </ul>"},{"location":"api/working-memory/#assemble_context","title":"<code>assemble_context(strategy:, max_tokens:)</code>","text":"<p>Assemble context string for LLM consumption.</p> <pre><code>assemble_context(strategy:, max_tokens: nil)\n</code></pre>"},{"location":"api/working-memory/#parameters_5","title":"Parameters","text":"Parameter Type Default Description <code>strategy</code> Symbol required Assembly strategy (<code>:recent</code>, <code>:important</code>, <code>:balanced</code>) <code>max_tokens</code> Integer, nil <code>@max_tokens</code> Optional token limit"},{"location":"api/working-memory/#assembly-strategies","title":"Assembly Strategies","text":"<p><code>:recent</code> - Most recently accessed first</p> <pre><code># Ordered by access time (newest first)\n# Good for: Conversational continuity\n</code></pre> <p><code>:important</code> - Highest importance scores first</p> <pre><code># Ordered by importance score (highest first)\n# Good for: Critical information prioritization\n</code></pre> <p><code>:balanced</code> - Weighted by importance and recency</p> <pre><code># Formula: importance \u00d7 (1.0 / (1 + age_in_hours))\n# Good for: General-purpose use (recommended)\n</code></pre>"},{"location":"api/working-memory/#returns_5","title":"Returns","text":"<ul> <li><code>String</code> - Assembled context with nodes separated by <code>\"\\n\\n\"</code></li> </ul>"},{"location":"api/working-memory/#examples_5","title":"Examples","text":"<pre><code># Balanced context (recommended)\ncontext = wm.assemble_context(strategy: :balanced)\n\n# Recent conversations only\ncontext = wm.assemble_context(strategy: :recent)\n\n# Most important information\ncontext = wm.assemble_context(strategy: :important)\n\n# Limited tokens\ncontext = wm.assemble_context(\n  strategy: :balanced,\n  max_tokens: 50_000\n)\n\n# Use in LLM prompt\nprompt = &lt;&lt;~PROMPT\n  Context:\n  #{context}\n\n  Question: #{user_question}\nPROMPT\n</code></pre>"},{"location":"api/working-memory/#balanced-strategy-details","title":"Balanced Strategy Details","text":"<p>The balanced strategy uses a decay function:</p> <pre><code>score = importance \u00d7 (1.0 / (1 + recency_in_hours))\n</code></pre> <p>Examples:</p> Importance Age Score 10.0 1 hour 5.0 10.0 5 hours 1.67 5.0 1 hour 2.5 5.0 5 hours 0.83"},{"location":"api/working-memory/#token_count","title":"<code>token_count()</code>","text":"<p>Get current total tokens in working memory.</p> <pre><code>token_count()\n</code></pre>"},{"location":"api/working-memory/#returns_6","title":"Returns","text":"<ul> <li><code>Integer</code> - Total tokens across all nodes</li> </ul>"},{"location":"api/working-memory/#examples_6","title":"Examples","text":"<pre><code>current = wm.token_count\nputs \"Using #{current} tokens\"\n\n# Check if near capacity\nif wm.token_count &gt; wm.max_tokens * 0.9\n  puts \"Warning: Working memory 90% full\"\nend\n\n# Calculate available space\navailable = wm.max_tokens - wm.token_count\nputs \"#{available} tokens available\"\n</code></pre>"},{"location":"api/working-memory/#utilization_percentage","title":"<code>utilization_percentage()</code>","text":"<p>Get working memory utilization as a percentage.</p> <pre><code>utilization_percentage()\n</code></pre>"},{"location":"api/working-memory/#returns_7","title":"Returns","text":"<ul> <li><code>Float</code> - Percentage of capacity used (0.0-100.0, rounded to 2 decimals)</li> </ul>"},{"location":"api/working-memory/#examples_7","title":"Examples","text":"<pre><code>util = wm.utilization_percentage\nputs \"Working memory: #{util}% full\"\n\n# Capacity warnings\ncase util\nwhen 0..50\n  puts \"Plenty of space\"\nwhen 51..80\n  puts \"Getting full\"\nwhen 81..95\n  puts \"Warning: High utilization\"\nelse\n  puts \"Critical: Near capacity\"\nend\n\n# Progress bar\nbar_length = (util / 2).round\nbar = \"\u2588\" * bar_length + \"\u2591\" * (50 - bar_length)\nputs \"[#{bar}] #{util}%\"\n</code></pre>"},{"location":"api/working-memory/#node_count","title":"<code>node_count()</code>","text":"<p>Get the number of nodes in working memory.</p> <pre><code>node_count()\n</code></pre>"},{"location":"api/working-memory/#returns_8","title":"Returns","text":"<ul> <li><code>Integer</code> - Number of nodes currently stored</li> </ul>"},{"location":"api/working-memory/#examples_8","title":"Examples","text":"<pre><code>count = wm.node_count\nputs \"#{count} nodes in working memory\"\n\n# Average tokens per node\nif count &gt; 0\n  avg = wm.token_count / count\n  puts \"Average: #{avg} tokens per node\"\nend\n\n# Density check\nif wm.node_count &gt; 100\n  puts \"Warning: Many small nodes, consider consolidation\"\nend\n</code></pre>"},{"location":"api/working-memory/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/working-memory/#capacity-management","title":"Capacity Management","text":"<pre><code># Check capacity before operations\ndef add_with_eviction(wm, ltm, key, value, token_count, importance)\n  unless wm.has_space?(token_count)\n    # Evict and mark in long-term memory\n    evicted = wm.evict_to_make_space(token_count)\n    evicted_keys = evicted.map { |n| n[:key] }\n    ltm.mark_evicted(evicted_keys) unless evicted_keys.empty?\n  end\n\n  wm.add(key, value,\n    token_count: token_count,\n    importance: importance\n  )\nend\n</code></pre>"},{"location":"api/working-memory/#monitoring","title":"Monitoring","text":"<pre><code># Working memory dashboard\ndef print_wm_status(wm)\n  puts \"Working Memory Status:\"\n  puts \"  Nodes: #{wm.node_count}\"\n  puts \"  Tokens: #{wm.token_count} / #{wm.max_tokens}\"\n  puts \"  Utilization: #{wm.utilization_percentage}%\"\n\n  available = wm.max_tokens - wm.token_count\n  puts \"  Available: #{available} tokens\"\nend\n</code></pre>"},{"location":"api/working-memory/#strategic-context-assembly","title":"Strategic Context Assembly","text":"<pre><code># Different contexts for different tasks\nclass ContextManager\n  def initialize(wm)\n    @wm = wm\n  end\n\n  def for_conversation\n    # Recent context for chat continuity\n    @wm.assemble_context(strategy: :recent, max_tokens: 8000)\n  end\n\n  def for_analysis\n    # Important context for deep analysis\n    @wm.assemble_context(strategy: :important, max_tokens: 32000)\n  end\n\n  def for_general_task\n    # Balanced for most tasks\n    @wm.assemble_context(strategy: :balanced)\n  end\nend\n</code></pre>"},{"location":"api/working-memory/#importance-based-retention","title":"Importance-Based Retention","text":"<pre><code># Critical memories persist longer\nwm.add(\"critical_security_alert\",\n  \"SQL injection vulnerability found in user input\",\n  token_count: 150,\n  importance: 10.0  # Maximum importance\n)\n\n# Temporary notes evicted first\nwm.add(\"temp_reminder\",\n  \"Check logs after deployment\",\n  token_count: 50,\n  importance: 1.0  # Minimum importance\n)\n</code></pre>"},{"location":"api/working-memory/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"api/working-memory/#time-complexity","title":"Time Complexity","text":"Operation Complexity Notes <code>add</code> O(1) Hash insertion + array append <code>remove</code> O(n) Array deletion requires scan <code>has_space?</code> O(n) Sums all token counts <code>evict_to_make_space</code> O(n log n) Sorting nodes <code>assemble_context</code> O(n log n) Sorting + concatenation <code>token_count</code> O(n) Sums all nodes <code>node_count</code> O(1) Hash size"},{"location":"api/working-memory/#space-complexity","title":"Space Complexity","text":"<ul> <li>O(n) where n is the number of nodes</li> <li>Each node stores: key, value, token_count, importance, timestamp, from_recall flag</li> </ul>"},{"location":"api/working-memory/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Batch Operations: Add multiple nodes before checking space</li> <li>Importance Scoring: Use meaningful scores (1-10) for effective eviction</li> <li>Token Limits: Set <code>max_tokens</code> based on your LLM's context window</li> <li>Strategy Selection: Use <code>:recent</code> for speed, <code>:balanced</code> for quality</li> </ol>"},{"location":"api/working-memory/#internal-data-structures","title":"Internal Data Structures","text":""},{"location":"api/working-memory/#node-storage","title":"Node Storage","text":"<pre><code>@nodes = {\n  \"key1\" =&gt; {\n    value: \"content...\",\n    token_count: 150,\n    importance: 7.0,\n    added_at: Time.now,\n    from_recall: false\n  },\n  \"key2\" =&gt; { ... }\n}\n</code></pre>"},{"location":"api/working-memory/#access-order","title":"Access Order","text":"<pre><code>@access_order = [\"key1\", \"key3\", \"key2\"]\n# Most recently accessed at the end\n</code></pre>"},{"location":"api/working-memory/#best-practices","title":"Best Practices","text":""},{"location":"api/working-memory/#1-check-space-before-adding","title":"1. Check Space Before Adding","text":"<pre><code># Good\nif wm.has_space?(tokens)\n  wm.add(key, value, token_count: tokens)\nelse\n  wm.evict_to_make_space(tokens)\n  wm.add(key, value, token_count: tokens)\nend\n\n# Bad - may exceed capacity\nwm.add(key, value, token_count: tokens)\n</code></pre>"},{"location":"api/working-memory/#2-use-appropriate-importance-scores","title":"2. Use Appropriate Importance Scores","text":"<pre><code># Critical architectural decisions\nimportance: 10.0\n\n# Important facts\nimportance: 7.0-8.0\n\n# Normal information\nimportance: 4.0-6.0\n\n# Temporary notes\nimportance: 1.0-3.0\n</code></pre>"},{"location":"api/working-memory/#3-monitor-utilization","title":"3. Monitor Utilization","text":"<pre><code># Set up alerts\nif wm.utilization_percentage &gt; 90\n  warn \"Working memory critically full\"\nend\n</code></pre>"},{"location":"api/working-memory/#4-choose-right-strategy","title":"4. Choose Right Strategy","text":"<pre><code># For chat/conversation\ncontext = wm.assemble_context(strategy: :recent)\n\n# For analysis/reasoning\ncontext = wm.assemble_context(strategy: :important)\n\n# For general use\ncontext = wm.assemble_context(strategy: :balanced)\n</code></pre>"},{"location":"api/working-memory/#see-also","title":"See Also","text":"<ul> <li>HTM API - Main class that uses WorkingMemory</li> <li>LongTermMemory API - Persistent storage for evicted nodes</li> <li>EmbeddingService API - Token counting</li> </ul>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>HTM (Hierarchical Temporary Memory) implements a sophisticated two-tier memory system designed specifically for LLM-based applications (\"robots\"). This architecture enables robots to maintain long-term context across sessions while managing token budgets efficiently.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>HTM provides intelligent memory management through five core components that work together to deliver persistent, searchable, and context-aware memory for AI agents.</p> <p> <p> HTM Coordination Layer</p> <p> Working Memory Token-Limited In-Memory LRU Eviction</p> <p> Long-Term Memory PostgreSQL Unlimited Capacity Durable Storage</p> <p> Embedding Service Ollama/OpenAI Vector Embeddings Semantic Search</p> <p> Database PostgreSQL 16+ TimescaleDB pgvector + pg_trgm</p> <p> </p> <p>manages persists generates stores</p> <p> </p> <p>Data Flow: Add Memory \u2192 Working Memory \u2192 Long-Term Memory (persistent) Recall \u2192 Long-Term (RAG search) \u2192 Working Memory (evict if needed) </p>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#htm-main-interface","title":"HTM (Main Interface)","text":"<p>The HTM class is the primary interface for memory operations. It coordinates between working memory, long-term memory, and embedding services to provide a unified API.</p> <p>Key Responsibilities:</p> <ul> <li>Initialize and coordinate all memory subsystems</li> <li>Manage robot identification and registration</li> <li>Generate embeddings for new memories</li> <li>Orchestrate recall operations with RAG-based retrieval</li> <li>Assemble context for LLM consumption</li> <li>Track memory statistics and robot activity</li> </ul> <p>Related ADRs: ADR-002, ADR-008</p>"},{"location":"architecture/#working-memory","title":"Working Memory","text":"<p>Token-limited, in-memory storage for active conversation context. Working memory acts as a fast cache for recently accessed or highly important memories that the LLM needs immediate access to.</p> <p>Characteristics:</p> <ul> <li>Capacity: Token-limited (default: 128,000 tokens)</li> <li>Storage: Ruby Hash (in-memory)</li> <li>Eviction: Hybrid importance + recency (LRU-based)</li> <li>Lifetime: Process lifetime</li> <li>Access Time: O(1) hash lookups</li> </ul> <p>Related ADRs: ADR-002, ADR-007</p>"},{"location":"architecture/#long-term-memory","title":"Long-Term Memory","text":"<p>Durable PostgreSQL storage for permanent knowledge retention. All memories are stored here permanently unless explicitly deleted.</p> <p>Characteristics:</p> <ul> <li>Capacity: Effectively unlimited</li> <li>Storage: PostgreSQL with TimescaleDB extension</li> <li>Retention: Permanent (explicit deletion only)</li> <li>Access Pattern: RAG-based retrieval (semantic + temporal)</li> <li>Lifetime: Forever</li> </ul> <p>Related ADRs: ADR-001, ADR-005</p>"},{"location":"architecture/#embedding-service","title":"Embedding Service","text":"<p>Generates vector embeddings for semantic search and manages token counting for memory management.</p> <p>Supported Providers:</p> <ul> <li>Ollama (default): Local embedding models (gpt-oss, nomic-embed-text, mxbai-embed-large)</li> <li>OpenAI: text-embedding-3-small, text-embedding-3-large</li> <li>Cohere: embed-english-v3.0, embed-multilingual-v3.0</li> <li>Local: Transformers.js for browser/edge deployment</li> </ul> <p>Related ADRs: ADR-003</p>"},{"location":"architecture/#database","title":"Database","text":"<p>PostgreSQL 16+ with extensions for time-series optimization, vector similarity search, and full-text search.</p> <p>Key Extensions:</p> <ul> <li>TimescaleDB: Hypertable partitioning, compression policies, time-range optimization</li> <li>pgvector: Vector similarity search with HNSW indexing</li> <li>pg_trgm: Trigram-based fuzzy text matching</li> </ul> <p>Related ADRs: ADR-001</p>"},{"location":"architecture/#component-interaction-flow","title":"Component Interaction Flow","text":""},{"location":"architecture/#adding-a-memory","title":"Adding a Memory","text":"<pre><code>sequenceDiagram\n    participant User\n    participant HTM\n    participant EmbeddingService\n    participant LongTermMemory\n    participant WorkingMemory\n    participant Database\n\n    User-&gt;&gt;HTM: add_node(key, value, ...)\n    HTM-&gt;&gt;EmbeddingService: embed(value)\n    EmbeddingService--&gt;&gt;HTM: embedding vector\n    HTM-&gt;&gt;EmbeddingService: count_tokens(value)\n    EmbeddingService--&gt;&gt;HTM: token_count\n    HTM-&gt;&gt;LongTermMemory: add(key, value, embedding, ...)\n    LongTermMemory-&gt;&gt;Database: INSERT INTO nodes\n    Database--&gt;&gt;LongTermMemory: node_id\n    LongTermMemory--&gt;&gt;HTM: node_id\n    HTM-&gt;&gt;WorkingMemory: add(key, value, token_count, ...)\n    Note over WorkingMemory: Evict if needed\n    WorkingMemory--&gt;&gt;HTM: success\n    HTM--&gt;&gt;User: node_id</code></pre>"},{"location":"architecture/#recalling-memories","title":"Recalling Memories","text":"<pre><code>sequenceDiagram\n    participant User\n    participant HTM\n    participant LongTermMemory\n    participant EmbeddingService\n    participant Database\n    participant WorkingMemory\n\n    User-&gt;&gt;HTM: recall(timeframe, topic, ...)\n    HTM-&gt;&gt;EmbeddingService: embed(topic)\n    EmbeddingService--&gt;&gt;HTM: query_embedding\n    HTM-&gt;&gt;LongTermMemory: search(timeframe, embedding, ...)\n    LongTermMemory-&gt;&gt;Database: SELECT with vector similarity\n    Database--&gt;&gt;LongTermMemory: matching nodes\n    LongTermMemory--&gt;&gt;HTM: recalled_memories\n    loop For each recalled memory\n        HTM-&gt;&gt;WorkingMemory: add(memory)\n        Note over WorkingMemory: Evict old memories if needed\n    end\n    HTM--&gt;&gt;User: recalled_memories</code></pre>"},{"location":"architecture/#key-architectural-principles","title":"Key Architectural Principles","text":""},{"location":"architecture/#1-never-forget-unless-told","title":"1. Never Forget (Unless Told)","text":"<p>HTM implements a \"never forget\" philosophy. Eviction from working memory moves data to long-term storage, it doesn't delete it. Only explicit <code>forget(key, confirm: :confirmed)</code> operations delete data.</p> <p>Design Principle</p> <p>Memory eviction is about managing working memory tokens, not data deletion. All evicted memories remain searchable and recallable from long-term storage.</p> <p>Related ADRs: ADR-009</p>"},{"location":"architecture/#2-two-tier-memory-hierarchy","title":"2. Two-Tier Memory Hierarchy","text":"<p>Working memory provides fast O(1) access to recent/important context, while long-term memory provides unlimited durable storage with RAG-based retrieval.</p> <p>Performance Benefit</p> <p>This architecture balances the competing needs of fast access (working memory) and unlimited retention (long-term memory).</p> <p>Related ADRs: ADR-002</p>"},{"location":"architecture/#3-hive-mind-architecture","title":"3. Hive Mind Architecture","text":"<p>All robots share a global long-term memory database, enabling cross-robot learning and context continuity. Each robot maintains its own working memory for process isolation.</p> <p>Multi-Robot Collaboration</p> <p>Knowledge gained by one robot benefits all robots. Users never need to repeat information across sessions or robots.</p> <p>Related ADRs: ADR-004</p>"},{"location":"architecture/#4-rag-based-retrieval","title":"4. RAG-Based Retrieval","text":"<p>HTM uses Retrieval-Augmented Generation patterns with hybrid search strategies combining semantic similarity (vector search) and temporal relevance (time-range filtering).</p> <p>Search Strategies</p> <ul> <li>Vector: Pure semantic similarity</li> <li>Full-text: Keyword-based search</li> <li>Hybrid: Combines both with RRF scoring</li> </ul> <p>Related ADRs: ADR-005</p>"},{"location":"architecture/#5-importance-weighted-eviction","title":"5. Importance-Weighted Eviction","text":"<p>Working memory eviction prioritizes low-importance older memories first, preserving critical context even if it's old.</p> <p>Token Budget Management</p> <p>Eviction is inevitable with finite token limits. The hybrid importance + recency strategy ensures the most valuable memories stay in working memory.</p> <p>Related ADRs: ADR-007</p>"},{"location":"architecture/#memory-lifecycle","title":"Memory Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Created: add_node()\n    Created --&gt; InWorkingMemory: Add to WM\n    Created --&gt; InLongTermMemory: Persist to LTM\n\n    InWorkingMemory --&gt; Evicted: Token limit reached\n    Evicted --&gt; InLongTermMemory: Mark as evicted\n\n    InLongTermMemory --&gt; Recalled: recall()\n    Recalled --&gt; InWorkingMemory: Add back to WM\n\n    InWorkingMemory --&gt; [*]: Process ends\n    InLongTermMemory --&gt; Forgotten: forget(confirm: :confirmed)\n    Forgotten --&gt; [*]: Permanently deleted\n\n    note right of InWorkingMemory\n        Fast O(1) access\n        Token-limited\n        Process-local\n    end note\n\n    note right of InLongTermMemory\n        Durable PostgreSQL\n        Unlimited capacity\n        RAG retrieval\n    end note</code></pre>"},{"location":"architecture/#architecture-documents","title":"Architecture Documents","text":"<p>Explore detailed architecture documentation:</p> <ul> <li>Detailed Architecture - Deep dive into system architecture, data flows, and performance characteristics</li> <li>Two-Tier Memory System - Working memory and long-term memory design, eviction strategies, and context assembly</li> <li>Hive Mind Architecture - Multi-robot shared memory, robot identification, and cross-robot knowledge sharing</li> </ul>"},{"location":"architecture/#technology-stack","title":"Technology Stack","text":"Layer Technology Purpose Language Ruby 3.2+ Core implementation Database PostgreSQL 16+ Relational storage Time-Series TimescaleDB Hypertable partitioning, compression Vector Search pgvector Semantic similarity (HNSW) Full-Text pg_trgm Fuzzy text matching Embeddings Ollama/OpenAI Vector generation Connection Pool connection_pool gem Database connection management Testing Minitest Test framework"},{"location":"architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/#working-memory_1","title":"Working Memory","text":"<ul> <li>Add: O(1) amortized (eviction is O(n log n) when needed)</li> <li>Retrieve: O(1) hash lookup</li> <li>Context Assembly: O(n log n) for sorting, O(k) for selecting</li> <li>Typical Size: 50-200 nodes (~128K tokens)</li> </ul>"},{"location":"architecture/#long-term-memory_1","title":"Long-Term Memory","text":"<ul> <li>Add: O(log n) with PostgreSQL indexes</li> <li>Vector Search: O(log n) with HNSW (approximate)</li> <li>Full-Text Search: O(log n) with GIN indexes</li> <li>Hybrid Search: O(log n) + merge</li> <li>Typical Size: Thousands to millions of nodes</li> </ul>"},{"location":"architecture/#overall-system","title":"Overall System","text":"<ul> <li>Memory Addition: &lt; 100ms (including embedding generation)</li> <li>Recall Operation: &lt; 200ms (typical hybrid search)</li> <li>Context Assembly: &lt; 10ms (working memory sort)</li> <li>Eviction: &lt; 10ms (rare, only when working memory full)</li> </ul>"},{"location":"architecture/#scalability-considerations","title":"Scalability Considerations","text":""},{"location":"architecture/#vertical-scaling","title":"Vertical Scaling","text":"<ul> <li>Working Memory: Limited by process RAM (~1-2GB for 128K tokens)</li> <li>Database: PostgreSQL scales to TBs with proper indexing</li> <li>Embeddings: Local models (Ollama) bounded by GPU/CPU</li> </ul>"},{"location":"architecture/#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>Multiple Robots: Each robot process has independent working memory</li> <li>Database: Single shared PostgreSQL instance (can add replicas)</li> <li>Read Replicas: For query scaling (future consideration)</li> <li>Sharding: By robot_id or timeframe (future consideration)</li> </ul> <p>Scaling Strategy</p> <p>Start with single PostgreSQL instance. Add read replicas when query load increases. Consider partitioning by robot_id for multi-tenant scenarios.</p>"},{"location":"architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>Installation Guide - Setup PostgreSQL, TimescaleDB, and dependencies</li> <li>Quick Start - Get started with HTM in 5 minutes</li> <li>API Reference - Complete API documentation</li> <li>Architecture Decision Records - Detailed decision history</li> </ul>"},{"location":"architecture/#architecture-reviews","title":"Architecture Reviews","text":"<p>All architecture decisions are documented in ADRs and reviewed by domain experts:</p> <ul> <li>Systems Architect: Overall system design and scalability</li> <li>Database Architect: PostgreSQL schema and query optimization</li> <li>AI Engineer: Embedding strategies and RAG implementation</li> <li>Performance Specialist: Latency and throughput analysis</li> <li>Ruby Expert: Idiomatic Ruby patterns and best practices</li> <li>Security Specialist: Data privacy and access control</li> </ul> <p>See Architecture Decision Records for complete review notes.</p>"},{"location":"architecture/hive-mind/","title":"Hive Mind Architecture: Multi-Robot Shared Memory","text":"<p>HTM implements a \"hive mind\" architecture where multiple robots (AI agents) share a global memory database. This enables cross-robot learning, context continuity across sessions, and collaborative knowledge building without requiring users to repeat information.</p>"},{"location":"architecture/hive-mind/#overview","title":"Overview","text":"<p>In the hive mind model, all robots access a single shared long-term memory database while maintaining independent working memory for process isolation. This design provides the best of both worlds: global knowledge sharing with local performance optimization.</p> <p> <p>Hive Mind: Shared Long-Term Memory</p> <p> Long-Term Memory PostgreSQL Shared Global Database All Robots Access Here</p> <p> Robot 1: Code Helper ID: robot-abc123 Own Working Memory</p> <p> Robot 2: Research Bot ID: robot-xyz789 Own Working Memory</p> <p> Robot 3: Chat Bot ID: robot-def456 Own Working Memory</p> <p> Robot 4: Designer ID: robot-ghi012 Own Working Memory</p> <p> </p> <p>read/write read/write read/write read/write</p> <p> Knowledge Sharing: All robots see all memories </p> <p>Related ADR</p> <p>See ADR-004: Multi-Robot Shared Memory (Hive Mind) for the complete architectural decision record.</p>"},{"location":"architecture/hive-mind/#why-hive-mind","title":"Why Hive Mind?","text":""},{"location":"architecture/hive-mind/#problems-with-isolated-memory","title":"Problems with Isolated Memory","text":"<p>When each robot has independent memory:</p> <ul> <li>Users repeat information across robots</li> <li>Context lost when switching robots</li> <li>No cross-robot learning</li> <li>Fragmented conversation history</li> <li>Architectural decisions made by one robot unknown to others</li> </ul>"},{"location":"architecture/hive-mind/#benefits-of-shared-memory","title":"Benefits of Shared Memory","text":"<p>With the hive mind architecture:</p> <ul> <li>Context continuity: User never repeats themselves</li> <li>Cross-robot learning: Knowledge compounds across agents</li> <li>Seamless switching: Switch robots without losing context</li> <li>Unified knowledge base: Single source of truth</li> <li>Collaborative development: Robots build on each other's work</li> </ul> <p>User Experience</p> <p>With shared memory, users can switch from a code helper to a research assistant without explaining the project context again. The research assistant already knows what the code helper learned.</p>"},{"location":"architecture/hive-mind/#architecture-design","title":"Architecture Design","text":""},{"location":"architecture/hive-mind/#memory-topology","title":"Memory Topology","text":"<p>HTM uses a hybrid memory topology:</p> <ul> <li>Long-Term Memory: Shared globally across all robots</li> <li>Working Memory: Per-robot, process-local</li> </ul> <p> <p>Memory Topology: Shared LTM + Local WM</p> <p> Shared (Global) Per-Robot (Local)</p> <p> Robot 1 (Process 1) Working Memory In-memory, token-limited Independent </p> <p> Robot 2 (Process 2) Working Memory In-memory, token-limited Independent </p> <p> Long-Term Memory (Shared) PostgreSQL All robots read/write here Memories attributed with robot_id Single Source of Truth</p> <p> </p> <p>read/write read/write</p> <p> </p> <p> Each robot has fast local cache (WM) + access to global knowledge (LTM) </p>"},{"location":"architecture/hive-mind/#why-this-design","title":"Why This Design?","text":"<p>Shared Long-Term Memory:</p> <ul> <li>Global knowledge base accessible to all robots</li> <li>Cross-robot context continuity</li> <li>Simplified architecture (single database)</li> <li>Unified search across all conversations</li> </ul> <p>Per-Robot Working Memory:</p> <ul> <li>Fast O(1) local access without network overhead</li> <li>Process isolation (no distributed state synchronization)</li> <li>Independent token budgets per robot</li> <li>Simple implementation (Ruby Hash, no Redis needed)</li> </ul> <p>Design Trade-off</p> <p>This architecture optimizes for single-user, multi-robot scenarios. For multi-tenant deployments, add row-level security or database sharding by tenant_id.</p>"},{"location":"architecture/hive-mind/#robot-identification-system","title":"Robot Identification System","text":"<p>Every robot in the hive mind has a unique identity for attribution tracking and activity monitoring.</p>"},{"location":"architecture/hive-mind/#dual-identifier-system","title":"Dual Identifier System","text":"<p>HTM uses two identifiers for each robot:</p>"},{"location":"architecture/hive-mind/#1-robot-id-robot_id","title":"1. Robot ID (<code>robot_id</code>)","text":"<ul> <li>Type: UUID v4 (RFC 4122)</li> <li>Format: <code>\"f47ac10b-58cc-4372-a567-0e02b2c3d479\"</code></li> <li>Generation: <code>SecureRandom.uuid</code> if not provided</li> <li>Purpose: Primary key, foreign key references, attribution</li> <li>Uniqueness: Guaranteed (collision probability: ~10^-36)</li> </ul>"},{"location":"architecture/hive-mind/#2-robot-name-robot_name","title":"2. Robot Name (<code>robot_name</code>)","text":"<ul> <li>Type: String (human-readable)</li> <li>Format: Any descriptive string (e.g., \"Code Helper\", \"Research Assistant\")</li> <li>Generation: <code>\"robot_#{robot_id[0..7]}\"</code> if not provided</li> <li>Purpose: Display, debugging, logging</li> <li>Uniqueness: Not enforced (names can collide)</li> </ul>"},{"location":"architecture/hive-mind/#robot-initialization","title":"Robot Initialization","text":"<pre><code># Option 1: Auto-generated identity (ephemeral robot)\nhtm = HTM.new(robot_name: \"Code Helper\")\n# robot_id: auto-generated UUID (new each session)\n# robot_name: \"Code Helper\"\n\n# Option 2: Persistent identity (stable robot)\nROBOT_ID = ENV['ROBOT_ID'] || \"f47ac10b-58cc-4372-a567-0e02b2c3d479\"\nhtm = HTM.new(\n  robot_id: ROBOT_ID,\n  robot_name: \"Code Helper\"\n)\n# robot_id: same across sessions\n# robot_name: \"Code Helper\"\n\n# Option 3: Minimal (auto-generate everything)\nhtm = HTM.new\n# robot_id: auto-generated UUID\n# robot_name: \"robot_f47ac10b\" (derived from UUID)\n</code></pre> <p>Persistent vs Ephemeral Robots</p> <ul> <li>Ephemeral: New UUID every session. Useful for testing or one-off tasks.</li> <li>Persistent: Store robot_id in config/environment. Recommended for production robots with stable identities.</li> </ul>"},{"location":"architecture/hive-mind/#robot-registry","title":"Robot Registry","text":"<p>All robots are registered in the <code>robots</code> table on first initialization:</p> <pre><code>CREATE TABLE robots (\n  id TEXT PRIMARY KEY,              -- robot_id (UUID)\n  name TEXT,                         -- robot_name (human-readable)\n  created_at TIMESTAMP DEFAULT NOW(),\n  last_active TIMESTAMP DEFAULT NOW(),\n  metadata JSONB                     -- future extensibility\n);\n</code></pre> <p>Registration flow:</p> <pre><code>def register_robot\n  @long_term_memory.register_robot(@robot_id, @robot_name)\nend\n\n# SQL: UPSERT semantics\n# INSERT INTO robots (id, name) VALUES ($1, $2)\n# ON CONFLICT (id) DO UPDATE\n# SET name = $2, last_active = CURRENT_TIMESTAMP\n</code></pre> <p>Related ADR</p> <p>See ADR-008: Robot Identification System for detailed design decisions.</p>"},{"location":"architecture/hive-mind/#memory-attribution-and-deduplication","title":"Memory Attribution and Deduplication","text":"<p>HTM uses a many-to-many relationship between robots and nodes, enabling both content deduplication and attribution tracking.</p>"},{"location":"architecture/hive-mind/#attribution-schema","title":"Attribution Schema","text":"<pre><code>-- Nodes are content-deduplicated via SHA-256 hash\nCREATE TABLE nodes (\n  id BIGSERIAL PRIMARY KEY,\n  content TEXT NOT NULL,\n  content_hash VARCHAR(64) UNIQUE,  -- SHA-256 for deduplication\n  ...\n);\n\n-- Robot-node relationships tracked in join table\nCREATE TABLE robot_nodes (\n  id BIGSERIAL PRIMARY KEY,\n  robot_id BIGINT NOT NULL REFERENCES robots(id),\n  node_id BIGINT NOT NULL REFERENCES nodes(id),\n  first_remembered_at TIMESTAMPTZ,  -- When robot first saw this content\n  last_remembered_at TIMESTAMPTZ,   -- When robot last tried to remember\n  remember_count INTEGER DEFAULT 1  -- How many times robot remembered this\n);\n\n-- Indexes for efficient queries\nCREATE UNIQUE INDEX idx_robot_nodes_unique ON robot_nodes(robot_id, node_id);\nCREATE INDEX idx_robot_nodes_robot_id ON robot_nodes(robot_id);\nCREATE INDEX idx_robot_nodes_node_id ON robot_nodes(node_id);\n</code></pre>"},{"location":"architecture/hive-mind/#content-deduplication","title":"Content Deduplication","text":"<p>When a robot remembers content:</p> <pre><code>def remember(content, tags: [])\n  # 1. Compute SHA-256 hash of content\n  content_hash = Digest::SHA256.hexdigest(content)\n\n  # 2. Check if node with same hash exists\n  existing_node = HTM::Models::Node.find_by(content_hash: content_hash)\n\n  if existing_node\n    # 3a. Link robot to existing node (or update remember_count)\n    link_robot_to_node(robot_id: @robot_id, node: existing_node)\n    return existing_node.id\n  else\n    # 3b. Create new node and link robot\n    node = create_new_node(content, content_hash)\n    link_robot_to_node(robot_id: @robot_id, node: node)\n    return node.id\n  end\nend\n</code></pre>"},{"location":"architecture/hive-mind/#attribution-queries","title":"Attribution Queries","text":""},{"location":"architecture/hive-mind/#which-robots-remember-this-content","title":"Which robots remember this content?","text":"<pre><code># Find all robots that have remembered a specific node\ndef robots_for_node(node_id)\n  HTM::Models::RobotNode\n    .where(node_id: node_id)\n    .includes(:robot)\n    .map do |rn|\n      {\n        robot_name: rn.robot.name,\n        first_remembered_at: rn.first_remembered_at,\n        remember_count: rn.remember_count\n      }\n    end\nend\n\n# Example\nrobots_for_node(123)\n# =&gt; [\n#   { robot_name: \"Code Helper\", first_remembered_at: \"2025-01-15\", remember_count: 3 },\n#   { robot_name: \"Research Bot\", first_remembered_at: \"2025-01-16\", remember_count: 1 }\n# ]\n</code></pre>"},{"location":"architecture/hive-mind/#nodes-shared-by-multiple-robots","title":"Nodes shared by multiple robots","text":"<pre><code># Find content that multiple robots have remembered\ndef shared_memories(min_robots: 2, limit: 50)\n  HTM::Models::Node\n    .joins(:robot_nodes)\n    .group('nodes.id')\n    .having('COUNT(DISTINCT robot_nodes.robot_id) &gt;= ?', min_robots)\n    .order('COUNT(DISTINCT robot_nodes.robot_id) DESC')\n    .limit(limit)\n    .map(&amp;:attributes)\nend\n\n# Example\nshared_memories(min_robots: 2)\n# =&gt; Nodes that 2+ robots have remembered\n</code></pre>"},{"location":"architecture/hive-mind/#robot-activity","title":"Robot activity","text":"<pre><code>-- Which robots have been active?\nSELECT id, name, last_active\nFROM robots\nORDER BY last_active DESC;\n\n-- Which robot has remembered the most nodes?\nSELECT r.name, COUNT(rn.node_id) as memory_count\nFROM robots r\nLEFT JOIN robot_nodes rn ON rn.robot_id = r.id\nGROUP BY r.id, r.name\nORDER BY memory_count DESC;\n\n-- What has a specific robot remembered recently?\nSELECT n.content, rn.first_remembered_at, rn.remember_count\nFROM robot_nodes rn\nJOIN nodes n ON n.id = rn.node_id\nWHERE rn.robot_id = 1\nORDER BY rn.last_remembered_at DESC\nLIMIT 50;\n</code></pre>"},{"location":"architecture/hive-mind/#cross-robot-knowledge-sharing","title":"Cross-Robot Knowledge Sharing","text":"<p>The power of the hive mind lies in automatic knowledge sharing across robots.</p>"},{"location":"architecture/hive-mind/#use-case-1-cross-session-context","title":"Use Case 1: Cross-Session Context","text":"<p>A user works with Robot A in one session, then Robot B in another session. Robot B automatically knows what Robot A learned.</p> <pre><code># Session 1 - Robot A (Code Helper)\nhtm_a = HTM.new(robot_name: \"Code Helper A\")\nhtm_a.remember(\"User prefers debug_me over puts for debugging\")\n# Stored in long-term memory, linked to robot A via robot_nodes\n\n# === User logs out, logs in next day ===\n\n# Session 2 - Robot B (different process, same or different machine)\nhtm_b = HTM.new(robot_name: \"Code Helper B\")\n\n# Robot B recalls preferences\nmemories = htm_b.recall(\"debugging preference\", timeframe: \"last week\")\n# =&gt; Finds preference from Robot A!\n\n# Robot B knows user preference without being told\n</code></pre>"},{"location":"architecture/hive-mind/#use-case-2-collaborative-development-with-deduplication","title":"Use Case 2: Collaborative Development with Deduplication","text":"<p>Different robots working on the same content automatically share nodes.</p> <pre><code># Robot A (Architecture discussion)\nhtm_a = HTM.new(robot_name: \"Architect Bot\")\nnode_id = htm_a.remember(\n  \"We decided to use PostgreSQL with pgvector for HTM storage\",\n  tags: [\"architecture\", \"database\"]\n)\n# =&gt; node_id: 123 (new node created)\n\n# Robot B learns the same fact independently\nhtm_b = HTM.new(robot_name: \"Code Bot\")\nnode_id = htm_b.remember(\n  \"We decided to use PostgreSQL with pgvector for HTM storage\"\n)\n# =&gt; node_id: 123 (same node! Content hash matched)\n\n# Both robots now linked to the same node\n# Robot A: remember_count = 1\n# Robot B: remember_count = 1\n\n# Check shared ownership\nrns = HTM::Models::RobotNode.where(node_id: 123)\nrns.each { |rn| puts \"Robot #{rn.robot_id}: #{rn.remember_count} times\" }\n# =&gt; Robot 1: 1 times\n# =&gt; Robot 2: 1 times\n</code></pre>"},{"location":"architecture/hive-mind/#use-case-3-finding-shared-knowledge","title":"Use Case 3: Finding Shared Knowledge","text":"<p>Analyze what content is shared across robots:</p> <pre><code># Find nodes remembered by multiple robots\nshared_nodes = HTM::Models::Node\n  .joins(:robot_nodes)\n  .group('nodes.id')\n  .having('COUNT(DISTINCT robot_nodes.robot_id) &gt;= 2')\n  .select('nodes.*, COUNT(DISTINCT robot_nodes.robot_id) as robot_count')\n\nshared_nodes.each do |node|\n  puts \"Node #{node.id}: #{node.robot_count} robots\"\n  puts \"  Content: #{node.content[0..80]}...\"\n\n  # Show which robots\n  node.robot_nodes.each do |rn|\n    puts \"  - Robot #{rn.robot.name}: remembered #{rn.remember_count}x\"\n  end\nend\n</code></pre>"},{"location":"architecture/hive-mind/#robot-activity-tracking","title":"Robot Activity Tracking","text":"<p>HTM automatically tracks robot activity through:</p>"},{"location":"architecture/hive-mind/#1-robot-registry-updates","title":"1. Robot Registry Updates","text":"<p>Every HTM operation updates the robot's <code>last_active</code> timestamp:</p> <pre><code>def update_robot_activity\n  @long_term_memory.update_robot_activity(@robot_id)\nend\n\n# SQL\n# UPDATE robots\n# SET last_active = CURRENT_TIMESTAMP\n# WHERE id = $1\n</code></pre>"},{"location":"architecture/hive-mind/#2-operations-log","title":"2. Operations Log","text":"<p>Every operation is logged with robot attribution:</p> <pre><code>def add_node(key, value, ...)\n  # ... add node ...\n\n  # Log operation\n  @long_term_memory.log_operation(\n    operation: 'add',\n    node_id: node_id,\n    robot_id: @robot_id,\n    details: { key: key, type: type }\n  )\nend\n</code></pre> <pre><code>CREATE TABLE operations_log (\n  id BIGSERIAL PRIMARY KEY,\n  timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n  operation TEXT NOT NULL,  -- add, retrieve, recall, forget, evict\n  node_id BIGINT REFERENCES nodes(id),\n  robot_id TEXT NOT NULL REFERENCES robots(id),\n  details JSONB\n);\n</code></pre>"},{"location":"architecture/hive-mind/#3-activity-queries","title":"3. Activity Queries","text":"<pre><code>-- Active robots in last 24 hours\nSELECT id, name, last_active\nFROM robots\nWHERE last_active &gt; NOW() - INTERVAL '24 hours'\nORDER BY last_active DESC;\n\n-- Operations by robot\nSELECT r.name, COUNT(ol.id) as operation_count\nFROM robots r\nJOIN operations_log ol ON ol.robot_id = r.id\nWHERE ol.timestamp &gt; NOW() - INTERVAL '7 days'\nGROUP BY r.name\nORDER BY operation_count DESC;\n\n-- Most active robots by memory contributions\nSELECT r.name, COUNT(n.id) as memory_count\nFROM robots r\nJOIN nodes n ON n.robot_id = r.id\nWHERE n.created_at &gt; NOW() - INTERVAL '30 days'\nGROUP BY r.name\nORDER BY memory_count DESC;\n</code></pre>"},{"location":"architecture/hive-mind/#privacy-considerations","title":"Privacy Considerations","text":"<p>The hive mind architecture has important privacy implications:</p>"},{"location":"architecture/hive-mind/#current-design-single-user-assumption","title":"Current Design: Single-User Assumption","text":"<p>HTM v1 assumes a single-user scenario where all robots work for the same user:</p> <ul> <li>All robots see all memories</li> <li>No isolation between robots</li> <li>No access control or permissions</li> <li>Simple, performant, easy to use</li> </ul> <p>Privacy Warning</p> <p>In the current design, all robots can access all memories. This is intentional for single-user scenarios but unsuitable for multi-user or multi-tenant deployments without additional security layers.</p>"},{"location":"architecture/hive-mind/#future-multi-tenancy-support","title":"Future: Multi-Tenancy Support","text":"<p>For multi-user scenarios, consider these privacy enhancements:</p>"},{"location":"architecture/hive-mind/#1-row-level-security-rls","title":"1. Row-Level Security (RLS)","text":"<p>PostgreSQL's RLS can enforce tenant isolation:</p> <pre><code>-- Enable RLS on nodes table\nALTER TABLE nodes ENABLE ROW LEVEL SECURITY;\n\n-- Policy: Users can only see their own tenant's nodes\nCREATE POLICY tenant_isolation ON nodes\n  FOR ALL\n  TO PUBLIC\n  USING (tenant_id = current_setting('app.tenant_id')::TEXT);\n\n-- Set tenant context per connection\nSET app.tenant_id = 'user-123';\n</code></pre>"},{"location":"architecture/hive-mind/#2-robot-visibility-levels","title":"2. Robot Visibility Levels","text":"<p>Add visibility controls to nodes:</p> <pre><code>ALTER TABLE nodes ADD COLUMN visibility TEXT DEFAULT 'shared';\n-- Values: 'private' (robot-only), 'shared' (all robots), 'team' (robot group)\n\n-- Private memory (only this robot)\nhtm.add_node(\"private_key\", \"sensitive data\", visibility: :private)\n\n-- Shared with specific robots\nhtm.add_node(\"team_key\", \"team data\", visibility: { team: ['robot-a', 'robot-b'] })\n</code></pre>"},{"location":"architecture/hive-mind/#3-robot-groupsteams","title":"3. Robot Groups/Teams","text":"<p>Organize robots into teams with shared memory:</p> <pre><code>CREATE TABLE robot_teams (\n  id BIGSERIAL PRIMARY KEY,\n  name TEXT NOT NULL,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE robot_team_members (\n  robot_id TEXT REFERENCES robots(id),\n  team_id BIGINT REFERENCES robot_teams(id),\n  PRIMARY KEY (robot_id, team_id)\n);\n\n-- Query memories by team\nSELECT n.*\nFROM nodes n\nJOIN robot_team_members rtm ON rtm.robot_id = n.robot_id\nWHERE rtm.team_id = $1;\n</code></pre>"},{"location":"architecture/hive-mind/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/hive-mind/#shared-long-term-memory","title":"Shared Long-Term Memory","text":"Aspect Performance Notes Concurrent reads Excellent PostgreSQL read scaling Concurrent writes Good MVCC handles concurrent inserts Attribution queries Fast Indexed on <code>robot_id</code> Cross-robot search Fast Same as single-robot search Registry updates Minimal overhead Simple UPDATE per operation"},{"location":"architecture/hive-mind/#per-robot-working-memory","title":"Per-Robot Working Memory","text":"Aspect Performance Notes Memory isolation O(1) No synchronization needed Process independence Excellent No shared state Eviction O(n log n) Per-robot, doesn't affect others Context assembly O(n log n) Per-robot, fast"},{"location":"architecture/hive-mind/#scalability","title":"Scalability","text":""},{"location":"architecture/hive-mind/#vertical-scaling","title":"Vertical Scaling","text":"<ul> <li>Database connections: Use connection pooling (default)</li> <li>Robot count: Limited by database connections (~100-200 concurrent)</li> <li>Memory size: Each robot process uses ~1-2GB RAM for working memory</li> </ul>"},{"location":"architecture/hive-mind/#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>Multi-process: Each robot process is independent</li> <li>Multi-host: All hosts share PostgreSQL database</li> <li>Read replicas: Route reads to replicas, writes to primary</li> <li>Sharding: Partition by <code>robot_id</code> or <code>tenant_id</code> for massive scale</li> </ul>"},{"location":"architecture/hive-mind/#code-examples","title":"Code Examples","text":""},{"location":"architecture/hive-mind/#example-1-persistent-robot-with-named-identity","title":"Example 1: Persistent Robot with Named Identity","text":"<pre><code># Initialize with persistent name\nhtm = HTM.new(\n  robot_name: \"Code Helper\",\n  working_memory_size: 128_000\n)\n\n# Add memories (linked to this robot via robot_nodes)\nhtm.remember(\"Use PostgreSQL for ACID guarantees and pgvector support\")\n\n# Robot ID is stored in database, robot_name is human-readable\nputs \"Robot ID: #{htm.robot_id}\"\nputs \"Robot name: #{htm.robot_name}\"\n</code></pre>"},{"location":"architecture/hive-mind/#example-2-multi-robot-collaboration-with-deduplication","title":"Example 2: Multi-Robot Collaboration with Deduplication","text":"<pre><code># Robot A: Architecture discussion\nrobot_a = HTM.new(robot_name: \"Architect\")\nnode_id = robot_a.remember(\n  \"PostgreSQL chosen for ACID guarantees and pgvector support\",\n  tags: [\"architecture:database\", \"decision\"]\n)\n\n# Robot B: Implementation (different process, accesses same LTM)\nrobot_b = HTM.new(robot_name: \"Coder\")\ndecisions = robot_b.recall(\"database decision\", timeframe: \"today\")\n# =&gt; Finds Robot A's decision automatically\n\n# If Robot B remembers the same content, it links to existing node\nsame_node_id = robot_b.remember(\n  \"PostgreSQL chosen for ACID guarantees and pgvector support\"\n)\n# =&gt; same_node_id == node_id (deduplication!)\n\nrobot_b.remember(\n  \"Implemented Database class with connection pooling\",\n  tags: [\"implementation:database\", \"code:ruby\"]\n)\n</code></pre>"},{"location":"architecture/hive-mind/#example-3-robot-activity-dashboard","title":"Example 3: Robot Activity Dashboard","text":"<pre><code># Get all robots and their activity\nstats = []\n\nHTM::Models::Robot.order(last_active: :desc).each do |robot|\n  # Count memories via robot_nodes\n  memory_count = robot.robot_nodes.count\n\n  # Get remember statistics\n  remember_stats = robot.robot_nodes.group(:node_id).count.size  # Unique nodes\n  total_remembers = robot.robot_nodes.sum(:remember_count)       # Total remembers\n\n  stats &lt;&lt; {\n    name: robot.name,\n    id: robot.id,\n    last_active: robot.last_active,\n    unique_memories: memory_count,\n    total_remembers: total_remembers\n  }\nend\n\n# Display dashboard\nputs \"=\" * 60\nputs \"Robot Activity Dashboard\"\nputs \"=\" * 60\nstats.each do |data|\n  puts \"#{data[:name]} (ID: #{data[:id]})\"\n  puts \"  Last active: #{data[:last_active]}\"\n  puts \"  Unique memories: #{data[:unique_memories]}\"\n  puts \"  Total remembers: #{data[:total_remembers]}\"\n  puts\nend\n</code></pre>"},{"location":"architecture/hive-mind/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Index - System overview and component summary</li> <li>Architecture Overview - Detailed architecture and data flows</li> <li>Two-Tier Memory System - Working memory and long-term memory design</li> <li>ADR-004: Multi-Robot Shared Memory (Hive Mind)</li> <li>ADR-008: Robot Identification System</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"architecture/overview/","title":"Detailed Architecture","text":"<p>This document provides a comprehensive deep dive into HTM's system architecture, component interactions, data flows, database schema, and performance characteristics.</p>"},{"location":"architecture/overview/#table-of-contents","title":"Table of Contents","text":"<ul> <li>System Architecture</li> <li>Component Diagrams</li> <li>Data Flow Diagrams</li> <li>Memory Lifecycle</li> <li>Database Schema</li> <li>Technology Stack</li> <li>Performance Characteristics</li> <li>Scalability Considerations</li> </ul>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<p>HTM implements a layered architecture with clear separation of concerns between presentation (API), business logic (memory management), and data access (database).</p>"},{"location":"architecture/overview/#architecture-layers","title":"Architecture Layers","text":""},{"location":"architecture/overview/#component-responsibilities","title":"Component Responsibilities","text":""},{"location":"architecture/overview/#api-layer-htm-class","title":"API Layer (HTM class)","text":"<ul> <li>Public interface for all memory operations</li> <li>Robot identification and initialization</li> <li>Request routing to appropriate subsystems</li> <li>Response aggregation and formatting</li> <li>Activity logging and statistics</li> </ul>"},{"location":"architecture/overview/#coordination-layer","title":"Coordination Layer","text":"<ul> <li>Robot Management: Registration, activity tracking, metadata</li> <li>Embedding Coordination: Generate embeddings for new memories and search queries</li> <li>Memory Orchestration: Coordinate between working and long-term memory</li> <li>Context Assembly: Build LLM context strings from working memory</li> <li>Token Management: Count tokens and enforce limits</li> </ul>"},{"location":"architecture/overview/#memory-management-layer","title":"Memory Management Layer","text":""},{"location":"architecture/overview/#working-memory","title":"Working Memory","text":"<ul> <li>In-Memory Store: Fast Ruby Hash-based storage</li> <li>Token Budget: Enforce maximum token limit (default 128K)</li> <li>Eviction Policy: Hybrid importance + recency eviction</li> <li>Access Tracking: LRU-style access order for recency</li> <li>Context Assembly: Three strategies (recent, important, balanced)</li> </ul>"},{"location":"architecture/overview/#long-term-memory","title":"Long-Term Memory","text":"<ul> <li>Persistence: Write all memories to PostgreSQL</li> <li>RAG Search: Vector + temporal + full-text search</li> <li>Relationship Management: Store and query node relationships</li> <li>Robot Registry: Track all robots using the system</li> <li>Eviction Marking: Mark which nodes are in working memory</li> </ul>"},{"location":"architecture/overview/#services-layer","title":"Services Layer","text":""},{"location":"architecture/overview/#embedding-service","title":"Embedding Service","text":"<ul> <li>Client-Side Generation: Generate embeddings before database insertion</li> <li>Token Counting: Estimate token counts for strings</li> <li>Model Management: Handle different models per provider</li> <li>Provider Support: Ollama (default) and OpenAI</li> </ul> <p>Architecture Change (October 2025)</p> <p>Embeddings are generated client-side in Ruby before database insertion. This provides reliable, cross-platform operation without complex database extension dependencies.</p>"},{"location":"architecture/overview/#database-service","title":"Database Service","text":"<ul> <li>Connection Pooling: Manage PostgreSQL connections</li> <li>Query Execution: Execute parameterized queries safely</li> <li>Transaction Management: ACID guarantees for operations</li> <li>Error Handling: Retry logic and failure recovery</li> </ul>"},{"location":"architecture/overview/#data-layer","title":"Data Layer","text":"<ul> <li>PostgreSQL: Relational storage with ACID guarantees</li> <li>TimescaleDB: Time-series optimization and compression</li> <li>pgvector: Vector similarity search with HNSW</li> <li>pg_trgm: Fuzzy text matching for search</li> </ul>"},{"location":"architecture/overview/#component-diagrams","title":"Component Diagrams","text":""},{"location":"architecture/overview/#htm-core-components","title":"HTM Core Components","text":""},{"location":"architecture/overview/#data-flow-diagrams","title":"Data Flow Diagrams","text":""},{"location":"architecture/overview/#memory-addition-flow","title":"Memory Addition Flow","text":"<p>This diagram shows the complete flow of adding a new memory node to HTM with client-side embedding generation.</p> <p>Architecture Note</p> <p>With client-side generation (October 2025), embeddings are generated in Ruby before database insertion. This provides reliable, cross-platform operation.</p> <p></p>"},{"location":"architecture/overview/#memory-recall-flow","title":"Memory Recall Flow","text":"<p>This diagram illustrates the RAG-based retrieval process with client-side query embeddings.</p> <p>Architecture Note</p> <p>With client-side generation, query embeddings are generated in Ruby before being passed to SQL for vector similarity search.</p> <p></p>"},{"location":"architecture/overview/#context-assembly-flow","title":"Context Assembly Flow","text":"<p>This diagram shows how working memory assembles context for LLM consumption using different strategies.</p> <p></p>"},{"location":"architecture/overview/#memory-lifecycle","title":"Memory Lifecycle","text":""},{"location":"architecture/overview/#node-states","title":"Node States","text":"<p>A memory node transitions through several states during its lifetime in HTM:</p> <p></p>"},{"location":"architecture/overview/#eviction-process","title":"Eviction Process","text":"<p>When working memory reaches its token limit, the eviction process runs to free up space:</p> <p></p>"},{"location":"architecture/overview/#database-schema","title":"Database Schema","text":""},{"location":"architecture/overview/#entity-relationship-diagram","title":"Entity-Relationship Diagram","text":""},{"location":"architecture/overview/#table-details","title":"Table Details","text":""},{"location":"architecture/overview/#nodes","title":"nodes","text":"<p>The main table storing all memory nodes with vector embeddings, metadata, and timestamps.</p> Column Type Description <code>id</code> BIGSERIAL Primary key, auto-incrementing <code>key</code> TEXT Unique identifier for node (user-defined) <code>value</code> TEXT Content of the memory <code>type</code> TEXT Memory type (fact, context, code, preference, decision, question) <code>category</code> TEXT Optional category for organization <code>importance</code> REAL Importance score (0.0-10.0, default 1.0) <code>created_at</code> TIMESTAMP Creation timestamp <code>updated_at</code> TIMESTAMP Last update timestamp <code>last_accessed</code> TIMESTAMP Last access timestamp <code>token_count</code> INTEGER Number of tokens in value <code>in_working_memory</code> BOOLEAN Whether currently in working memory <code>robot_id</code> TEXT Foreign key to robots table <code>embedding</code> vector(1536) Vector embedding for semantic search <p>Indexes:</p> <ul> <li>Primary key on <code>id</code></li> <li>Unique index on <code>key</code></li> <li>B-tree indexes on <code>created_at</code>, <code>updated_at</code>, <code>last_accessed</code>, <code>type</code>, <code>category</code>, <code>robot_id</code></li> <li>HNSW index on <code>embedding</code> for vector similarity</li> <li>GIN indexes on <code>to_tsvector('english', value)</code> for full-text search</li> <li>GIN trigram index on <code>value</code> for fuzzy matching</li> </ul>"},{"location":"architecture/overview/#robots","title":"robots","text":"<p>Registry of all robots using the HTM system.</p> Column Type Description <code>id</code> TEXT Primary key, UUID v4 <code>name</code> TEXT Human-readable robot name <code>created_at</code> TIMESTAMP Registration timestamp <code>last_active</code> TIMESTAMP Last activity timestamp <code>metadata</code> JSONB Flexible robot configuration"},{"location":"architecture/overview/#relationships","title":"relationships","text":"<p>Graph edges connecting related nodes.</p> Column Type Description <code>id</code> BIGSERIAL Primary key <code>from_node_id</code> BIGINT Source node foreign key <code>to_node_id</code> BIGINT Target node foreign key <code>relationship_type</code> TEXT Type of relationship (e.g., \"related_to\", \"follows\") <code>strength</code> REAL Relationship strength (0.0-1.0) <code>created_at</code> TIMESTAMP Creation timestamp <p>Indexes:</p> <ul> <li>B-tree indexes on <code>from_node_id</code> and <code>to_node_id</code></li> <li>Unique constraint on <code>(from_node_id, to_node_id, relationship_type)</code></li> </ul>"},{"location":"architecture/overview/#tags","title":"tags","text":"<p>Flexible categorization system for nodes.</p> Column Type Description <code>id</code> BIGSERIAL Primary key <code>node_id</code> BIGINT Foreign key to nodes <code>tag</code> TEXT Tag name <code>created_at</code> TIMESTAMP Creation timestamp <p>Indexes:</p> <ul> <li>B-tree index on <code>node_id</code></li> <li>B-tree index on <code>tag</code></li> <li>Unique constraint on <code>(node_id, tag)</code></li> </ul>"},{"location":"architecture/overview/#operations_log","title":"operations_log","text":"<p>Audit trail of all memory operations for debugging and replay.</p> Column Type Description <code>id</code> BIGSERIAL Primary key <code>timestamp</code> TIMESTAMP Operation timestamp <code>operation</code> TEXT Operation type (add, retrieve, recall, forget, evict) <code>node_id</code> BIGINT Foreign key to nodes (nullable) <code>robot_id</code> TEXT Foreign key to robots <code>details</code> JSONB Flexible operation metadata <p>Indexes:</p> <ul> <li>B-tree indexes on <code>timestamp</code>, <code>robot_id</code>, <code>operation</code></li> </ul>"},{"location":"architecture/overview/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/overview/#core-technologies","title":"Core Technologies","text":"Technology Version Purpose Why Chosen Ruby 3.2+ Implementation language Readable, expressive, mature ecosystem PostgreSQL 16+ Relational database ACID guarantees, rich extensions, production-proven TimescaleDB 2.13+ Time-series extension Hypertable partitioning, automatic compression pgvector 0.5+ Vector similarity HNSW indexing, PostgreSQL-native, fast approximate search pg_trgm - Fuzzy text search Built-in PostgreSQL extension for trigram matching"},{"location":"architecture/overview/#ruby-dependencies","title":"Ruby Dependencies","text":"<pre><code># Core dependencies\ngem 'pg', '~&gt; 1.5'                    # PostgreSQL client\ngem 'pgvector', '~&gt; 0.2'              # Vector operations\ngem 'connection_pool', '~&gt; 2.4'      # Connection pooling\ngem 'faraday', '~&gt; 2.7'              # HTTP client (for embedding APIs)\n\n# Optional dependencies\ngem 'tiktoken_ruby', '~&gt; 0.0.6'      # Token counting (OpenAI-compatible)\n</code></pre>"},{"location":"architecture/overview/#embedding-providers","title":"Embedding Providers","text":"<p>Client-Side Generation</p> <p>Embeddings are generated client-side in Ruby before database insertion. This provides reliable, cross-platform operation.</p> Provider Models Dimensions Speed Cost Ollama (default) nomic-embed-text, mxbai-embed-large, all-minilm 384-1024 Fast (local HTTP) Free OpenAI text-embedding-3-small, text-embedding-ada-002 1536 Fast (API) $0.0001/1K tokens"},{"location":"architecture/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/overview/#latency-benchmarks","title":"Latency Benchmarks","text":"<p>Based on typical production workloads with 10,000 nodes in long-term memory (client-side embeddings):</p> <p>Performance Characteristics</p> <p>Client-side embedding generation provides reliable, debuggable operation. Latency includes HTTP call to Ollama/OpenAI for embedding generation.</p> Operation Median P95 P99 Notes <code>add_message()</code> 50ms 110ms 190ms Client-side embedding generation + insert <code>recall()</code> (vector) 80ms 140ms 230ms Client-side query embedding + vector search <code>recall()</code> (fulltext) 30ms 60ms 100ms GIN index search (no embedding needed) <code>recall()</code> (hybrid) 110ms 190ms 330ms Client-side embedding + hybrid search <code>retrieve()</code> 5ms 10ms 20ms Simple primary key lookup <code>create_context()</code> 8ms 15ms 25ms In-memory sort + join <code>forget()</code> 10ms 20ms 40ms DELETE with cascades <p>Performance Optimization</p> <ul> <li>Use connection pooling (included by default)</li> <li>Add database indexes for common query patterns</li> <li>Consider read replicas for query-heavy workloads</li> <li>Monitor HNSW build time for large embedding tables</li> </ul>"},{"location":"architecture/overview/#throughput","title":"Throughput","text":"Workload Throughput Resource Usage Add nodes 500-1000/sec CPU-bound (embeddings) Vector search 2000-5000/sec I/O-bound (database) Full-text search 5000-10000/sec I/O-bound (database) Context assembly 10000+/sec Memory-bound (working memory)"},{"location":"architecture/overview/#storage","title":"Storage","text":"Component Size Estimate Compression Node (text only) ~1KB average None Node (with embedding) ~7KB (1536 dims \u00d7 4 bytes) TimescaleDB compression (70-90%) Indexes ~2x data size Minimal Operations log ~200 bytes/op TimescaleDB compression <p>Example: 100,000 nodes with embeddings:</p> <ul> <li>Raw data: ~700 MB</li> <li>With indexes: ~2.1 GB</li> <li>With compression (after 30 days): ~300 MB</li> </ul>"},{"location":"architecture/overview/#scalability-considerations","title":"Scalability Considerations","text":""},{"location":"architecture/overview/#vertical-scaling-limits","title":"Vertical Scaling Limits","text":"Resource Limit Mitigation Working Memory (RAM) ~2GB per robot process Use smaller <code>working_memory_size</code>, evict more aggressively PostgreSQL Connections ~100-200 (default) Connection pooling, adjust <code>max_connections</code> Embedding API Rate Limits Provider-dependent Implement rate limiting, use local models HNSW Build Time O(n log n) on large tables Partition tables by timeframe"},{"location":"architecture/overview/#horizontal-scaling-strategies","title":"Horizontal Scaling Strategies","text":""},{"location":"architecture/overview/#multi-process-single-host","title":"Multi-Process (Single Host)","text":"<ul> <li>Each robot process has independent working memory</li> <li>All processes share single PostgreSQL instance</li> <li>Connection pooling prevents connection exhaustion</li> </ul>"},{"location":"architecture/overview/#multi-host-distributed","title":"Multi-Host (Distributed)","text":"<ul> <li>Option 1: Shared Database</li> <li>All hosts connect to central PostgreSQL</li> <li>Read replicas for query scaling</li> <li> <p>Write operations to primary only</p> </li> <li> <p>Option 2: Sharded Database</p> </li> <li>Partition by <code>robot_id</code> or timeframe</li> <li>Requires coordination for cross-shard queries</li> <li>More complex but scales writes</li> </ul>"},{"location":"architecture/overview/#read-scaling","title":"Read Scaling","text":"<ul> <li>Add PostgreSQL read replicas</li> <li>Route <code>recall()</code> and <code>retrieve()</code> to replicas</li> <li>Primary handles writes only</li> <li>TimescaleDB native replication support</li> </ul> <p>Consistency Considerations</p> <p>Read replicas may lag primary by seconds. For strong consistency requirements, query primary database.</p>"},{"location":"architecture/overview/#future-scaling-enhancements","title":"Future Scaling Enhancements","text":"<ol> <li>Redis-backed Working Memory: Share working memory across processes</li> <li>Horizontal Partitioning: Shard <code>nodes</code> table by <code>robot_id</code> or time ranges</li> <li>Caching Layer: Add Redis cache for hot nodes</li> <li>Async Embedding Generation: Queue embedding jobs for batch processing</li> <li>Vector Database Migration: Consider specialized vector DB (Pinecone, Weaviate) at massive scale</li> </ol>"},{"location":"architecture/overview/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Index - Architecture overview and component summary</li> <li>Two-Tier Memory System - Working memory and long-term memory deep dive</li> <li>Hive Mind Architecture - Multi-robot shared memory design</li> <li>API Reference - Complete API documentation</li> <li>Architecture Decision Records - Decision history</li> </ul>"},{"location":"architecture/two-tier-memory/","title":"Two-Tier Memory System","text":"<p>HTM implements a sophisticated two-tier memory architecture that balances the competing needs of fast access (working memory) and unlimited retention (long-term memory). This document provides a comprehensive deep dive into both tiers, their interactions, and optimization strategies.</p>"},{"location":"architecture/two-tier-memory/#overview","title":"Overview","text":"<p>The two-tier architecture addresses a fundamental challenge in LLM-based applications: LLMs have limited context windows but need to maintain awareness across long conversations spanning days, weeks, or months.</p> <p> <p>Two-Tier Memory Architecture</p> <p> Working Memory (Hot) Capacity: Token-limited (128K) Storage: In-memory Ruby Hash Speed: O(1) lookups Lifetime: Process lifetime Eviction: Importance + Recency Fast, Token-Aware, Volatile</p> <p> Long-Term Memory (Cold) Capacity: Unlimited Storage: PostgreSQL + TimescaleDB Speed: O(log n) with indexes Lifetime: Permanent Retrieval: RAG (semantic + temporal) Durable, Searchable, Persistent</p> <p> Add Memory (Stored in both tiers)</p> <p> Eviction (Token limit \u2192 move to LTM only)</p> <p> Recall (RAG search \u2192 load back to WM)</p> <p> Never Forget: Evicted memories stay in LTM forever (explicit deletion only)</p> <p> </p> <p>Related ADR</p> <p>See ADR-002: Two-Tier Memory Architecture for the complete architectural decision record.</p>"},{"location":"architecture/two-tier-memory/#working-memory-hot-tier","title":"Working Memory (Hot Tier)","text":"<p>Working memory is a token-limited, in-memory cache for recently accessed or highly important memories. It provides O(1) access times for the LLM's immediate context needs.</p>"},{"location":"architecture/two-tier-memory/#design-characteristics","title":"Design Characteristics","text":"Aspect Details Purpose Immediate context for LLM consumption Capacity Token-limited (default: 128,000 tokens) Storage Ruby Hash: <code>{ key =&gt; node }</code> Access Pattern Frequent reads, moderate writes Eviction Policy Hybrid importance + recency (LRU-based) Lifetime Process lifetime (cleared on restart) Performance O(1) hash lookups, O(n log n) eviction"},{"location":"architecture/two-tier-memory/#data-structure","title":"Data Structure","text":"<pre><code>class WorkingMemory\n  def initialize(max_tokens:)\n    @max_tokens = max_tokens\n    @nodes = {}           # key =&gt; node_data\n    @access_order = []    # LRU tracking\n  end\n\n  # Node structure\n  # {\n  #   value: \"Memory content\",\n  #   token_count: 150,\n  #   importance: 5.0,\n  #   added_at: Time.now,\n  #   from_recall: false\n  # }\nend\n</code></pre>"},{"location":"architecture/two-tier-memory/#why-token-limited","title":"Why Token-Limited?","text":"<p>LLMs have finite context windows. Even with 200K token models, managing token budgets is critical:</p> <ul> <li>Prevent context overflow: Ensure working memory fits in LLM context</li> <li>Cost control: API-based LLMs charge per token</li> <li>Focus attention: Too much context dilutes LLM focus</li> <li>Performance: Smaller context = faster LLM inference</li> </ul> <p>Token Budget Strategy</p> <p>Working memory token limit should be 60-70% of LLM context window to leave room for system prompts, user queries, and LLM responses.</p>"},{"location":"architecture/two-tier-memory/#working-memory-operations","title":"Working Memory Operations","text":""},{"location":"architecture/two-tier-memory/#add-node","title":"Add Node","text":"<pre><code>def add(key, value, token_count:, importance: 1.0, from_recall: false)\n  # Check if eviction needed\n  if token_count + current_tokens &gt; @max_tokens\n    evict_to_make_space(token_count)\n  end\n\n  # Add to working memory\n  @nodes[key] = {\n    value: value,\n    token_count: token_count,\n    importance: importance,\n    added_at: Time.now,\n    from_recall: from_recall\n  }\n\n  # Track access order (LRU)\n  update_access(key)\nend\n</code></pre> <p>Time Complexity: O(1) amortized (eviction is O(n log n) when triggered)</p>"},{"location":"architecture/two-tier-memory/#retrieve-node","title":"Retrieve Node","text":"<pre><code>def retrieve(key)\n  return nil unless @nodes.key?(key)\n\n  # Update access order\n  update_access(key)\n\n  @nodes[key]\nend\n</code></pre> <p>Time Complexity: O(1)</p>"},{"location":"architecture/two-tier-memory/#remove-node","title":"Remove Node","text":"<pre><code>def remove(key)\n  @nodes.delete(key)\n  @access_order.delete(key)\nend\n</code></pre> <p>Time Complexity: O(1) for hash, O(n) for access_order</p>"},{"location":"architecture/two-tier-memory/#eviction-strategy","title":"Eviction Strategy","text":"<p>When working memory exceeds its token limit, HTM evicts nodes based on a hybrid importance + recency score.</p>"},{"location":"architecture/two-tier-memory/#eviction-algorithm","title":"Eviction Algorithm","text":"<pre><code>def evict_to_make_space(needed_tokens)\n  evicted = []\n  tokens_freed = 0\n\n  # Sort by [importance ASC, age DESC]\n  # Lower importance evicted first\n  # Within same importance, older evicted first\n  candidates = @nodes.sort_by do |key, node|\n    recency = Time.now - node[:added_at]\n    [node[:importance], -recency]\n  end\n\n  # Greedy eviction: stop when enough space\n  candidates.each do |key, node|\n    break if tokens_freed &gt;= needed_tokens\n\n    evicted &lt;&lt; { key: key, value: node[:value] }\n    tokens_freed += node[:token_count]\n    @nodes.delete(key)\n    @access_order.delete(key)\n  end\n\n  evicted\nend\n</code></pre>"},{"location":"architecture/two-tier-memory/#eviction-priority","title":"Eviction Priority","text":"<p>Nodes are evicted in this order:</p> <ol> <li>Low importance, old (e.g., importance: 1.0, age: 5 days)</li> <li>Low importance, recent (e.g., importance: 1.0, age: 1 hour)</li> <li>High importance, old (e.g., importance: 9.0, age: 5 days)</li> <li>High importance, recent (e.g., importance: 9.0, age: 1 hour) \u2190 Kept longest</li> </ol> <p> <p>Eviction Priority (Lower \u2192 Higher retention)</p> <p> Tier 1: Evict First</p> <p> Tier 2</p> <p> Tier 3</p> <p> Tier 4: Keep Longest</p> <p>Importance: 1.0 Age: 5 days Low value, stale</p> <p>Importance: 1.0 Age: 1 hour Low value, recent</p> <p>Importance: 9.0 Age: 5 days High value, older</p> <p>Importance: 9.0 Age: 1 hour High value, fresh</p> <p>Example Eviction Scenario: Working Memory: 127,500 / 128,000 tokens (99% full) New memory to add: 5,000 tokens Need to free: 4,500 tokens</p> <p>Eviction: Remove Tier 1 and Tier 2 nodes until 4,500+ tokens freed Result: Tier 3 and Tier 4 nodes preserved (high importance) </p> <p>Importance Matters</p> <p>Assign meaningful importance scores! Low-importance memories (1.0-3.0) will be evicted first. Use higher scores (7.0-10.0) for critical information like architectural decisions, user preferences, and long-term facts.</p> <p>Related ADR</p> <p>See ADR-007: Working Memory Eviction Strategy for detailed rationale and alternatives considered.</p>"},{"location":"architecture/two-tier-memory/#context-assembly-strategies","title":"Context Assembly Strategies","text":"<p>Working memory provides three strategies for assembling context strings for LLM consumption:</p>"},{"location":"architecture/two-tier-memory/#1-recent-recent","title":"1. Recent (<code>:recent</code>)","text":"<p>Sort by access order, most recently accessed first.</p> <pre><code>def assemble_context(strategy: :recent, max_tokens: nil)\n  nodes = @access_order.reverse.map { |key| @nodes[key] }\n  build_context(nodes, max_tokens || @max_tokens)\nend\n</code></pre> <p>Best For:</p> <ul> <li>Conversational continuity</li> <li>Chat interfaces</li> <li>Following current discussion thread</li> <li>Debugging sessions</li> </ul> <p>Example Use Case:</p> <pre><code># User having back-and-forth coding conversation\ncontext = htm.create_context(strategy: :recent, max_tokens: 8000)\n# Recent messages prioritized for coherent conversation flow\n</code></pre>"},{"location":"architecture/two-tier-memory/#2-important-important","title":"2. Important (<code>:important</code>)","text":"<p>Sort by importance score, highest first.</p> <pre><code>def assemble_context(strategy: :important, max_tokens: nil)\n  nodes = @nodes.sort_by { |k, v| -v[:importance] }.map(&amp;:last)\n  build_context(nodes, max_tokens || @max_tokens)\nend\n</code></pre> <p>Best For:</p> <ul> <li>Strategic planning</li> <li>Architectural decisions</li> <li>Summarization tasks</li> <li>Key facts retrieval</li> </ul> <p>Example Use Case:</p> <pre><code># LLM helping with architectural review\ncontext = htm.create_context(strategy: :important)\n# Critical decisions and facts prioritized over recent chat\n</code></pre>"},{"location":"architecture/two-tier-memory/#3-balanced-balanced-recommended-default","title":"3. Balanced (<code>:balanced</code>) - Recommended Default","text":"<p>Hybrid scoring with time decay: <code>importance * (1.0 / (1 + recency_hours))</code></p> <pre><code>def assemble_context(strategy: :balanced, max_tokens: nil)\n  nodes = @nodes.sort_by { |k, v|\n    recency_hours = (Time.now - v[:added_at]) / 3600.0\n    score = v[:importance] * (1.0 / (1 + recency_hours))\n    -score  # Descending\n  }.map(&amp;:last)\n\n  build_context(nodes, max_tokens || @max_tokens)\nend\n</code></pre> <p>Decay Function:</p> <ul> <li>Just added (0 hours): <code>importance * 1.0</code> (full weight)</li> <li>1 hour old: <code>importance * 0.5</code> (half weight)</li> <li>3 hours old: <code>importance * 0.25</code> (quarter weight)</li> <li>24 hours old: <code>importance * 0.04</code> (4% weight)</li> </ul> <p>Best For:</p> <ul> <li>General-purpose LLM interactions</li> <li>Mixed conversational + strategic tasks</li> <li>Default strategy when unsure</li> </ul> <p>Example Use Case:</p> <pre><code># Code helper assisting with debugging and design\ncontext = htm.create_context(strategy: :balanced)\n# Recent debugging context + important architectural decisions\n</code></pre> <p> <p>Balanced Strategy: Importance Decay Over Time</p> <p> </p> <p>0h 1h 3h 6h 24h Time Since Added (hours)</p> <p>0 3 6 9 10 Effective Score</p> <p> Imp: 10.0</p> <p> Imp: 5.0</p> <p> Imp: 1.0</p> <p> High-importance memories retain value longer, but recency still matters </p> <p>Related ADR</p> <p>See ADR-006: Context Assembly Strategies for detailed strategy analysis.</p>"},{"location":"architecture/two-tier-memory/#performance-characteristics","title":"Performance Characteristics","text":"Operation Time Complexity Typical Latency Add node O(1) amortized &lt; 1ms Retrieve node O(1) &lt; 1ms Eviction (when needed) O(n log n) &lt; 10ms (for 200 nodes) Context assembly O(n log n) &lt; 10ms (for 200 nodes) Check space O(n) &lt; 1ms <p>Memory Usage:</p> <ul> <li>Empty working memory: ~1KB</li> <li>100 nodes (avg 500 tokens each): ~50KB metadata + node content</li> <li>200 nodes (128K tokens): ~2-5MB total (including Ruby overhead)</li> </ul>"},{"location":"architecture/two-tier-memory/#long-term-memory-cold-tier","title":"Long-Term Memory (Cold Tier)","text":"<p>Long-term memory provides unlimited, durable storage for all memories with advanced retrieval capabilities using RAG (Retrieval-Augmented Generation) patterns.</p>"},{"location":"architecture/two-tier-memory/#design-characteristics_1","title":"Design Characteristics","text":"Aspect Details Purpose Permanent knowledge base Capacity Effectively unlimited Storage PostgreSQL 16+ with TimescaleDB Access Pattern RAG-based retrieval (semantic + temporal) Retention Permanent (explicit deletion only) Lifetime Forever (survives process restarts) Performance O(log n) with indexes and HNSW"},{"location":"architecture/two-tier-memory/#database-schema-simplified","title":"Database Schema (Simplified)","text":"<pre><code>CREATE TABLE nodes (\n  id BIGSERIAL PRIMARY KEY,\n  key TEXT UNIQUE NOT NULL,\n  value TEXT NOT NULL,\n  type TEXT,\n  importance REAL DEFAULT 1.0,\n  token_count INTEGER,\n  in_working_memory BOOLEAN DEFAULT FALSE,\n  robot_id TEXT NOT NULL REFERENCES robots(id),\n  embedding vector(1536),\n  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n  ...\n);\n\n-- HNSW index for vector similarity\nCREATE INDEX idx_nodes_embedding ON nodes\n  USING hnsw (embedding vector_cosine_ops)\n  WITH (m = 16, ef_construction = 64);\n\n-- Full-text search\nCREATE INDEX idx_nodes_value_gin ON nodes\n  USING gin(to_tsvector('english', value));\n</code></pre>"},{"location":"architecture/two-tier-memory/#why-postgresql-timescaledb","title":"Why PostgreSQL + TimescaleDB?","text":"<p>PostgreSQL provides:</p> <ul> <li>ACID guarantees for data integrity</li> <li>Rich ecosystem and tooling</li> <li>pgvector for vector similarity search</li> <li>Full-text search with GIN indexes</li> <li>Mature, production-proven</li> </ul> <p>TimescaleDB adds:</p> <ul> <li>Hypertable partitioning by time</li> <li>Automatic compression (70-90% reduction)</li> <li>Time-range query optimization</li> <li>Retention policies for data lifecycle</li> </ul> <p>Related ADR</p> <p>See ADR-001: Use PostgreSQL with TimescaleDB for Storage for complete rationale.</p>"},{"location":"architecture/two-tier-memory/#long-term-memory-operations","title":"Long-Term Memory Operations","text":""},{"location":"architecture/two-tier-memory/#add-node_1","title":"Add Node","text":"<pre><code>def add(key:, value:, embedding:, importance:, token_count:, robot_id:, type: nil)\n  result = @db.exec_params(&lt;&lt;~SQL, [key, value, type, importance, token_count, robot_id, embedding])\n    INSERT INTO nodes (key, value, type, importance, token_count, robot_id, embedding, in_working_memory)\n    VALUES ($1, $2, $3, $4, $5, $6, $7, TRUE)\n    RETURNING id\n  SQL\n\n  result[0]['id'].to_i\nend\n</code></pre> <p>Time Complexity: O(log n) with B-tree and HNSW indexes</p>"},{"location":"architecture/two-tier-memory/#retrieve-by-key","title":"Retrieve by Key","text":"<pre><code>def retrieve(key)\n  result = @db.exec_params(&lt;&lt;~SQL, [key])\n    SELECT * FROM nodes WHERE key = $1\n  SQL\n\n  result.first\nend\n</code></pre> <p>Time Complexity: O(1) with unique index on key</p>"},{"location":"architecture/two-tier-memory/#vector-search","title":"Vector Search","text":"<pre><code>def search(timeframe:, query:, limit:, embedding_service:)\n  query_embedding = embedding_service.embed(query)\n\n  result = @db.exec_params(&lt;&lt;~SQL, [timeframe.begin, timeframe.end, query_embedding, limit])\n    SELECT *, embedding &lt;=&gt; $3 AS distance\n    FROM nodes\n    WHERE created_at BETWEEN $1 AND $2\n    ORDER BY distance ASC\n    LIMIT $4\n  SQL\n\n  result.to_a\nend\n</code></pre> <p>Time Complexity: O(log n) with HNSW approximate nearest neighbor</p>"},{"location":"architecture/two-tier-memory/#hybrid-search-vector-full-text","title":"Hybrid Search (Vector + Full-Text)","text":"<p>Combines vector similarity and full-text search using Reciprocal Rank Fusion (RRF):</p> <pre><code>def search_hybrid(timeframe:, query:, limit:, embedding_service:)\n  query_embedding = embedding_service.embed(query)\n\n  # Get both vector and full-text results\n  vector_results = search(timeframe, query, limit * 2, embedding_service)\n  fulltext_results = search_fulltext(timeframe, query, limit * 2)\n\n  # RRF scoring\n  scores = {}\n\n  vector_results.each_with_index do |node, rank|\n    scores[node['id']] ||= 0\n    scores[node['id']] += 1.0 / (rank + 60)  # RRF constant: 60\n  end\n\n  fulltext_results.each_with_index do |node, rank|\n    scores[node['id']] ||= 0\n    scores[node['id']] += 1.0 / (rank + 60)\n  end\n\n  # Sort by combined score\n  all_nodes = (vector_results + fulltext_results).uniq { |n| n['id'] }\n  all_nodes.sort_by { |n| -scores[n['id']] }.take(limit)\nend\n</code></pre> <p>Related ADR</p> <p>See ADR-005: RAG-Based Retrieval with Hybrid Search for search strategy details.</p>"},{"location":"architecture/two-tier-memory/#rag-based-retrieval","title":"RAG-Based Retrieval","text":"<p>HTM uses Retrieval-Augmented Generation patterns to find relevant memories:</p>"},{"location":"architecture/two-tier-memory/#search-strategies","title":"Search Strategies","text":""},{"location":"architecture/two-tier-memory/#1-vector-search-vector","title":"1. Vector Search (<code>:vector</code>)","text":"<p>Pure semantic similarity using cosine distance between embeddings.</p> <p>Best For:</p> <ul> <li>Conceptual similarity</li> <li>Finding related ideas</li> <li>Semantic matching across paraphrases</li> </ul> <p>Example:</p> <pre><code># Query: \"database optimization\"\n# Finds: \"PostgreSQL index tuning\", \"query performance\", \"EXPLAIN ANALYZE\"\nmemories = htm.recall(timeframe: \"last month\", topic: \"database optimization\", strategy: :vector)\n</code></pre>"},{"location":"architecture/two-tier-memory/#2-full-text-search-fulltext","title":"2. Full-Text Search (<code>:fulltext</code>)","text":"<p>Keyword-based matching using PostgreSQL GIN indexes.</p> <p>Best For:</p> <ul> <li>Exact phrase matching</li> <li>Keyword search</li> <li>Code snippets</li> <li>Proper nouns</li> </ul> <p>Example:</p> <pre><code># Query: \"TimescaleDB compression\"\n# Finds exact matches for \"TimescaleDB\" and \"compression\"\nmemories = htm.recall(timeframe: \"last week\", topic: \"TimescaleDB compression\", strategy: :fulltext)\n</code></pre>"},{"location":"architecture/two-tier-memory/#3-hybrid-search-hybrid-recommended","title":"3. Hybrid Search (<code>:hybrid</code>) - Recommended","text":"<p>Combines vector and full-text with RRF scoring.</p> <p>Best For:</p> <ul> <li>General-purpose retrieval</li> <li>Balanced precision and recall</li> <li>Most use cases</li> </ul> <p>Example:</p> <pre><code># Combines semantic similarity AND keyword matching\nmemories = htm.recall(timeframe: \"last month\", topic: \"PostgreSQL performance\", strategy: :hybrid)\n</code></pre>"},{"location":"architecture/two-tier-memory/#performance-characteristics_1","title":"Performance Characteristics","text":"Operation Time Complexity Typical Latency Notes Add node O(log n) 20-50ms Includes index updates Retrieve by key O(1) 5-10ms Unique index lookup Vector search O(log n) 50-100ms HNSW approximate Full-text search O(log n) 20-40ms GIN index Hybrid search O(log n) 80-150ms Both + RRF merge Delete node O(log n) 10-30ms Cascading deletes <p>Storage Efficiency:</p> <p>With TimescaleDB compression (after 30 days):</p> <ul> <li>Text node: ~1KB \u2192 ~200 bytes (80% reduction)</li> <li>Node with embedding: ~7KB \u2192 ~1-2KB (70-85% reduction)</li> <li>100,000 nodes: ~700MB \u2192 ~100-200MB</li> </ul>"},{"location":"architecture/two-tier-memory/#memory-flow-add-evict-recall","title":"Memory Flow: Add \u2192 Evict \u2192 Recall","text":""},{"location":"architecture/two-tier-memory/#complete-flow-diagram","title":"Complete Flow Diagram","text":""},{"location":"architecture/two-tier-memory/#example-adding-5000-token-memory-to-full-working-memory","title":"Example: Adding 5000-Token Memory to Full Working Memory","text":"<pre><code># Initial state\nhtm.memory_stats[:working_memory]\n# =&gt; {\n#   current_tokens: 127_500,\n#   max_tokens: 128_000,\n#   utilization: 99.6%,\n#   node_count: 85\n# }\n\n# Add large memory\nhtm.add_node(\"large_doc\", large_documentation, importance: 7.0)\n\n# HTM automatically:\n# 1. Generates embedding (Ollama: ~50ms)\n# 2. Stores in long-term memory (PostgreSQL: ~30ms)\n# 3. Checks working memory space: 5000 + 127500 &gt; 128000 (no space!)\n# 4. Evicts low-importance old nodes to free 4500+ tokens\n#    - Evicts 3 nodes: (importance: 1.0, age: 3 days), (importance: 2.0, age: 1 day), etc.\n# 5. Adds new memory to working memory\n# 6. Total time: ~100ms\n\n# New state\nhtm.memory_stats[:working_memory]\n# =&gt; {\n#   current_tokens: 128_000,\n#   max_tokens: 128_000,\n#   utilization: 100.0%,\n#   node_count: 83  # Lost 3 nodes, gained 1\n# }\n\n# Evicted nodes still in long-term memory!\nevicted_node = htm.retrieve(\"old_debug_log\")  # Still works\n# =&gt; { \"key\" =&gt; \"old_debug_log\", \"in_working_memory\" =&gt; false, ... }\n</code></pre>"},{"location":"architecture/two-tier-memory/#context-assembly-strategies-in-detail","title":"Context Assembly Strategies in Detail","text":""},{"location":"architecture/two-tier-memory/#strategy-comparison","title":"Strategy Comparison","text":"Strategy Sort Key Best Use Case Typical Output Recent Access order (newest first) Conversations, debugging Recent 5-10 messages Important Importance score (highest first) Planning, decisions Top 10 most important facts Balanced <code>importance / (1 + hours)</code> General assistant Mix of recent + important"},{"location":"architecture/two-tier-memory/#code-examples","title":"Code Examples","text":""},{"location":"architecture/two-tier-memory/#recent-strategy","title":"Recent Strategy","text":"<pre><code># Assemble context from most recent memories\ncontext = htm.create_context(strategy: :recent, max_tokens: 8000)\n\n# Typical output (recent conversation):\n# \"\"\"\n# User: What's the capital of France?\n# Assistant: The capital of France is Paris.\n# User: Tell me about its history.\n# Assistant: Paris has been inhabited since...\n# ...\n# \"\"\"\n</code></pre>"},{"location":"architecture/two-tier-memory/#important-strategy","title":"Important Strategy","text":"<pre><code># Assemble context from most important memories\ncontext = htm.create_context(strategy: :important, max_tokens: 4000)\n\n# Typical output (critical facts):\n# \"\"\"\n# Decision: Use PostgreSQL with TimescaleDB for storage (importance: 10.0)\n# User preference: Always use debug_me over puts (importance: 9.0)\n# Architecture: Two-tier memory system (importance: 9.0)\n# ...\n# \"\"\"\n</code></pre>"},{"location":"architecture/two-tier-memory/#balanced-strategy","title":"Balanced Strategy","text":"<pre><code># Assemble context with time decay\ncontext = htm.create_context(strategy: :balanced)\n\n# Typical output (hybrid):\n# \"\"\"\n# Recent debugging: ValueError in embedding service (importance: 7.0, 10 min ago) [score: 42.0]\n# Critical decision: PostgreSQL chosen (importance: 10.0, 3 days ago) [score: 0.14]\n# Current task: Implementing RAG search (importance: 6.0, 1 hour ago) [score: 3.0]\n# ...\n# \"\"\"\n</code></pre>"},{"location":"architecture/two-tier-memory/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/two-tier-memory/#working-memory-optimization","title":"Working Memory Optimization","text":""},{"location":"architecture/two-tier-memory/#1-tune-token-limit","title":"1. Tune Token Limit","text":"<pre><code># For shorter context windows (e.g., GPT-3.5 with 16K tokens)\nhtm = HTM.new(working_memory_size: 8_000)  # Leave room for prompt + response\n\n# For longer context windows (e.g., Claude 3 with 200K tokens)\nhtm = HTM.new(working_memory_size: 128_000)  # Default\n</code></pre>"},{"location":"architecture/two-tier-memory/#2-adjust-importance-scores","title":"2. Adjust Importance Scores","text":"<pre><code># High importance for critical information\nhtm.add_node(\"user_preference\", \"User prefers Vim keybindings\", importance: 9.0)\n\n# Low importance for transient information\nhtm.add_node(\"debug_log\", \"Temporary debug output\", importance: 1.0)\n\n# Medium importance for general context\nhtm.add_node(\"discussion\", \"Discussed API design patterns\", importance: 5.0)\n</code></pre>"},{"location":"architecture/two-tier-memory/#3-use-appropriate-context-strategy","title":"3. Use Appropriate Context Strategy","text":"<pre><code># For chat: recent strategy\nchat_context = htm.create_context(strategy: :recent, max_tokens: 8000)\n\n# For planning: important strategy\nplanning_context = htm.create_context(strategy: :important, max_tokens: 4000)\n\n# For general: balanced strategy (default)\ngeneral_context = htm.create_context(strategy: :balanced)\n</code></pre>"},{"location":"architecture/two-tier-memory/#long-term-memory-optimization","title":"Long-Term Memory Optimization","text":""},{"location":"architecture/two-tier-memory/#1-leverage-timescaledb-compression","title":"1. Leverage TimescaleDB Compression","text":"<pre><code>-- Enable compression after 30 days\nSELECT add_compression_policy('nodes', INTERVAL '30 days');\n\n-- Compress by robot_id and type for better ratio\nALTER TABLE nodes SET (\n  timescaledb.compress,\n  timescaledb.compress_segmentby = 'robot_id,type'\n);\n</code></pre>"},{"location":"architecture/two-tier-memory/#2-use-appropriate-search-strategy","title":"2. Use Appropriate Search Strategy","text":"<pre><code># For exact matches: full-text\nexact_matches = htm.recall(timeframe: \"last week\", topic: \"PostgreSQL\", strategy: :fulltext)\n\n# For semantic similarity: vector\nsimilar_concepts = htm.recall(timeframe: \"last month\", topic: \"database performance\", strategy: :vector)\n\n# For best results: hybrid (default)\nbest_results = htm.recall(timeframe: \"last month\", topic: \"PostgreSQL performance\", strategy: :hybrid)\n</code></pre>"},{"location":"architecture/two-tier-memory/#3-index-tuning","title":"3. Index Tuning","text":"<pre><code>-- Monitor HNSW build time\nSELECT pg_size_pretty(pg_relation_size('idx_nodes_embedding')) AS index_size;\n\n-- Rebuild HNSW index if needed\nREINDEX INDEX CONCURRENTLY idx_nodes_embedding;\n\n-- Analyze query plans\nEXPLAIN ANALYZE\nSELECT * FROM nodes\nWHERE created_at &gt; NOW() - INTERVAL '7 days'\n  AND embedding &lt;=&gt; '[...]' &lt; 0.5\nORDER BY embedding &lt;=&gt; '[...]'\nLIMIT 20;\n</code></pre>"},{"location":"architecture/two-tier-memory/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Index - System overview and component summary</li> <li>Architecture Overview - Detailed architecture and data flows</li> <li>Hive Mind Architecture - Multi-robot shared memory</li> <li>ADR-002: Two-Tier Memory Architecture</li> <li>ADR-006: Context Assembly Strategies</li> <li>ADR-007: Working Memory Eviction Strategy</li> </ul>"},{"location":"architecture/adrs/","title":"Architecture Decision Records (ADRs)","text":""},{"location":"architecture/adrs/#introduction","title":"Introduction","text":"<p>Architecture Decision Records (ADRs) document significant architectural decisions made during the development of HTM (Hierarchical Temporal Memory). Each ADR captures the context, decision, rationale, and consequences of important design choices.</p>"},{"location":"architecture/adrs/#what-are-adrs","title":"What are ADRs?","text":"<p>Architecture Decision Records are lightweight documents that capture important architectural decisions along with their context and consequences. They serve as a historical record of why decisions were made, helping current and future developers understand the system's design.</p>"},{"location":"architecture/adrs/#key-benefits","title":"Key Benefits","text":"<ul> <li>Historical Context: Understand why decisions were made</li> <li>Knowledge Transfer: Onboard new team members faster</li> <li>Decision Tracking: See how the architecture evolved over time</li> <li>Avoid Revisiting: Prevent rehashing settled decisions</li> </ul>"},{"location":"architecture/adrs/#adr-structure","title":"ADR Structure","text":"<p>Each ADR follows a consistent structure:</p> <ul> <li>Status: Current state (Accepted, Proposed, Deprecated, Superseded)</li> <li>Date: When the decision was made</li> <li>Decision Makers: Who participated in the decision</li> <li>Quick Summary: TL;DR of the decision</li> <li>Context: Background and problem statement</li> <li>Decision: What was decided</li> <li>Rationale: Why this decision was made</li> <li>Consequences: Positive, negative, and neutral outcomes</li> <li>Alternatives Considered: What other options were evaluated</li> <li>References: Related documentation and resources</li> </ul>"},{"location":"architecture/adrs/#adr-status-legend","title":"ADR Status Legend","text":"Status Meaning Accepted Decision is approved and implemented Proposed Decision is under consideration Rejected Decision was considered but not adopted Deprecated Decision is no longer recommended Superseded Decision has been replaced by another ADR"},{"location":"architecture/adrs/#how-to-read-adrs","title":"How to Read ADRs","text":"<ol> <li>Start with Quick Summary: Get the high-level decision quickly</li> <li>Read Context: Understand the problem being solved</li> <li>Review Decision and Rationale: See what was chosen and why</li> <li>Consider Consequences: Understand trade-offs and implications</li> <li>Check Alternatives: See what else was considered</li> </ol>"},{"location":"architecture/adrs/#complete-adr-list","title":"Complete ADR List","text":""},{"location":"architecture/adrs/#adr-001-postgresql-with-timescaledb-for-storage","title":"ADR-001: PostgreSQL with TimescaleDB for Storage","text":"<p>Status: Accepted | Date: 2025-10-25</p> <p>PostgreSQL with TimescaleDB extension chosen as the primary storage backend, providing time-series optimization, vector embeddings, full-text search, and ACID compliance in a single database system.</p> <p>Key Decision: Use PostgreSQL + TimescaleDB instead of specialized vector databases or multiple storage systems.</p> <p>Read more: ADR-001: PostgreSQL with TimescaleDB</p>"},{"location":"architecture/adrs/#adr-002-two-tier-memory-architecture","title":"ADR-002: Two-Tier Memory Architecture","text":"<p>Status: Accepted | Date: 2025-10-25</p> <p>Implementation of a two-tier memory system with token-limited working memory (hot tier) and unlimited long-term memory (cold tier) to manage LLM context windows while preserving all historical data.</p> <p>Key Decision: Separate fast working memory from durable long-term storage with RAG-based retrieval.</p> <p>Read more: ADR-002: Two-Tier Memory Architecture</p>"},{"location":"architecture/adrs/#adr-003-ollama-as-default-embedding-provider","title":"ADR-003: Ollama as Default Embedding Provider","text":"<p>Status: Accepted | Date: 2025-10-25</p> <p>Ollama with the gpt-oss model selected as the default embedding provider, prioritizing local-first, privacy-preserving operation with zero API costs while supporting pluggable alternatives.</p> <p>Key Decision: Local embeddings by default, with support for cloud providers (OpenAI, Cohere) as options.</p> <p>Read more: ADR-003: Ollama Default Embedding Provider</p>"},{"location":"architecture/adrs/#adr-004-multi-robot-shared-memory-hive-mind","title":"ADR-004: Multi-Robot Shared Memory (Hive Mind)","text":"<p>Status: Accepted | Date: 2025-10-25</p> <p>All robots share a single global memory database with attribution tracking, enabling seamless context sharing and cross-robot learning while maintaining individual robot identity.</p> <p>Key Decision: Shared global memory instead of per-robot isolation, with attribution via robot_id.</p> <p>Read more: ADR-004: Multi-Robot Shared Memory</p>"},{"location":"architecture/adrs/#adr-005-rag-based-retrieval-with-hybrid-search","title":"ADR-005: RAG-Based Retrieval with Hybrid Search","text":"<p>Status: Accepted | Date: 2025-10-25</p> <p>Three search strategies implemented (vector, full-text, hybrid) with temporal filtering, allowing users to choose the best approach for their query type while combining semantic understanding with keyword precision.</p> <p>Key Decision: Hybrid search as default, combining full-text pre-filtering with vector reranking.</p> <p>Read more: ADR-005: RAG-Based Retrieval</p>"},{"location":"architecture/adrs/#adr-006-context-assembly-strategies","title":"ADR-006: Context Assembly Strategies","text":"<p>Status: Accepted | Date: 2025-10-25</p> <p>Three context assembly strategies (recent, important, balanced) for selecting which memories to include when token limits prevent loading all working memory, with balanced as the recommended default.</p> <p>Key Decision: Multiple strategies for different use cases, with importance-weighted recency decay as default.</p> <p>Read more: ADR-006: Context Assembly Strategies</p>"},{"location":"architecture/adrs/#adr-007-working-memory-eviction-strategy","title":"ADR-007: Working Memory Eviction Strategy","text":"<p>Status: Accepted | Date: 2025-10-25</p> <p>Hybrid eviction policy combining importance and recency scoring, evicting low-importance older memories first while preserving all data in long-term storage (never-forget principle).</p> <p>Key Decision: Eviction moves to long-term storage, never deletes. Primary sort by importance, secondary by age.</p> <p>Read more: ADR-007: Working Memory Eviction</p>"},{"location":"architecture/adrs/#adr-008-robot-identification-system","title":"ADR-008: Robot Identification System","text":"<p>Status: Accepted | Date: 2025-10-25</p> <p>Dual-identifier system using UUID v4 for unique robot_id plus optional human-readable robot_name, with automatic generation if not provided and comprehensive robot registry tracking.</p> <p>Key Decision: UUID for uniqueness, name for readability, auto-generation for convenience.</p> <p>Read more: ADR-008: Robot Identification</p>"},{"location":"architecture/adrs/#adr-009-never-forget-philosophy-with-explicit-deletion","title":"ADR-009: Never-Forget Philosophy with Explicit Deletion","text":"<p>Status: Accepted | Date: 2025-10-25</p> <p>Never-forget philosophy where memories are never automatically deleted, eviction only moves data between tiers, and deletion requires explicit confirmation to prevent accidental data loss.</p> <p>Key Decision: Permanent storage by default, deletion only via <code>forget(confirm: :confirmed)</code>.</p> <p>Read more: ADR-009: Never-Forget Philosophy</p>"},{"location":"architecture/adrs/#adr-010-redis-based-working-memory-rejected","title":"ADR-010: Redis-Based Working Memory (Rejected)","text":"<p>Status: Rejected | Date: 2025-10-25</p> <p>Proposal to add Redis as a persistent storage layer for working memory was thoroughly analyzed and rejected. PostgreSQL already provides durability, working memory's ephemeral nature is by design, and Redis would add complexity without solving a proven problem.</p> <p>Key Decision: Keep two-tier architecture with in-memory working memory. Trust PostgreSQL for durability. Apply YAGNI principle.</p> <p>Why Rejected: Unnecessary complexity, performance penalty, operational burden, and no proven requirement. PostgreSQL already handles multi-process sharing and crash recovery.</p> <p>Read more: ADR-010: Redis Working Memory (Rejected)</p>"},{"location":"architecture/adrs/#adr-dependencies","title":"ADR Dependencies","text":""},{"location":"architecture/adrs/#related-documentation","title":"Related Documentation","text":"<ul> <li>HTM API Guide</li> <li>Database Schema</li> <li>Configuration Guide</li> <li>Development Workflow</li> </ul>"},{"location":"architecture/adrs/#contributing-to-adrs","title":"Contributing to ADRs","text":"<p>When making significant architectural decisions:</p> <ol> <li>Create a new ADR using the next sequential number</li> <li>Follow the established structure and format</li> <li>Include thorough context, rationale, and consequences</li> <li>Document alternatives considered and why they were rejected</li> <li>Update this index with a summary</li> <li>Link related documentation</li> </ol>"},{"location":"architecture/adrs/#questions","title":"Questions?","text":"<p>For questions about architectural decisions, please:</p> <ul> <li>Review the specific ADR documentation</li> <li>Check the related guides and API documentation</li> <li>Open a GitHub issue for clarification</li> <li>Consult the development team</li> </ul>"},{"location":"architecture/adrs/001-postgresql-timescaledb/","title":"ADR-001: PostgreSQL with TimescaleDB for Storage","text":"<p>Status: Accepted</p> <p>Date: 2025-10-25</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#quick-summary","title":"Quick Summary","text":"<p>HTM uses PostgreSQL with TimescaleDB as its primary storage backend, providing time-series optimization, vector embeddings (pgvector), full-text search, and ACID compliance in a single, production-proven database system.</p> <p>Why: Consolidates time-series data, vector search, and full-text capabilities in one system rather than maintaining multiple specialized databases.</p> <p>Impact: Production-ready storage with excellent tooling, at the cost of some operational complexity compared to simpler alternatives.</p>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#context","title":"Context","text":"<p>HTM requires a persistent storage solution that can handle:</p> <ul> <li>Time-series data with efficient time-range queries</li> <li>Vector embeddings for semantic search</li> <li>Full-text search capabilities</li> <li>ACID compliance for data integrity</li> <li>Scalability for growing memory databases</li> <li>Production-grade reliability</li> </ul>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#alternative-options-considered","title":"Alternative Options Considered","text":"<ol> <li>Pure PostgreSQL: Solid relational database, pgvector support</li> <li>TimescaleDB: PostgreSQL extension optimized for time-series</li> <li>Elasticsearch: Strong full-text search, vector support added</li> <li>Pinecone/Weaviate: Specialized vector databases</li> <li>SQLite + extensions: Simple, embedded option</li> </ol>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#decision","title":"Decision","text":"<p>We will use PostgreSQL with TimescaleDB as the primary storage backend for HTM.</p>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/001-postgresql-timescaledb/#why-postgresql","title":"Why PostgreSQL?","text":"<p>Production-proven: - Decades of reliability in demanding environments - ACID compliance guarantees data integrity for memory operations - Rich ecosystem with extensive tooling, monitoring, and support</p> <p>Search capabilities: - pgvector extension: Native vector similarity search with HNSW indexing - Full-text search: Built-in tsvector with GIN indexing - pg_trgm extension: Trigram-based fuzzy matching</p> <p>Developer experience: - Strong typing with schema enforcement prevents data corruption - Wide adoption means well-understood by developers - Standard SQL with PostgreSQL-specific enhancements</p>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#why-timescaledb","title":"Why TimescaleDB?","text":"<p>Time-series optimization: - Hypertable partitioning: Automatic chunk-based time partitioning - Compression policies: Automatic compression of old data (70-90% reduction) - Time-range optimization: Fast queries on temporal data via chunk exclusion</p> <p>PostgreSQL compatibility: - Drop-in extension, not a fork - All PostgreSQL features remain available - Standard PostgreSQL tools work seamlessly</p> <p>Operational features: - Continuous aggregates: Pre-computed summaries for analytics - Retention policies: Automatic data lifecycle management - Cloud offering: Managed service available (TimescaleDB Cloud)</p>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#why-not-alternatives","title":"Why Not Alternatives?","text":"<p>Elasticsearch</p> <ul> <li>High operational complexity (JVM tuning, cluster management)</li> <li>Higher resource usage</li> <li>Vector support more recent, less mature</li> <li>Superior full-text search not critical for our use case</li> </ul> <p>Specialized Vector DBs (Pinecone, Weaviate, Qdrant)</p> <ul> <li>Additional service dependency increases complexity</li> <li>Limited relational capabilities</li> <li>Vendor lock-in concerns</li> <li>Cost considerations for managed services</li> <li>Excellent vector search performance</li> <li>Purpose-built for embeddings</li> </ul> <p>SQLite</p> <ul> <li>Limited concurrency (write locks)</li> <li>No native vector search (extensions experimental)</li> <li>Not suitable for multi-robot scenarios</li> <li>Simple deployment</li> <li>Zero configuration</li> </ul>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adrs/001-postgresql-timescaledb/#schema-design","title":"Schema Design","text":"<pre><code>-- Nodes table as TimescaleDB hypertable\nCREATE TABLE nodes (\n  id SERIAL PRIMARY KEY,\n  key TEXT UNIQUE NOT NULL,\n  value TEXT NOT NULL,\n  embedding vector(1536),\n  robot_id TEXT NOT NULL,\n  created_at TIMESTAMP NOT NULL,\n  importance FLOAT DEFAULT 1.0,\n  type TEXT,\n  metadata JSONB\n);\n\n-- Convert to hypertable (TimescaleDB)\nSELECT create_hypertable('nodes', 'created_at');\n\n-- Vector indexing (HNSW for approximate nearest neighbor)\nCREATE INDEX nodes_embedding_idx ON nodes\nUSING hnsw (embedding vector_cosine_ops);\n\n-- Full-text indexing\nCREATE INDEX nodes_fts_idx ON nodes\nUSING GIN (to_tsvector('english', value));\n\n-- Additional indexes\nCREATE INDEX nodes_robot_id_idx ON nodes (robot_id);\nCREATE INDEX nodes_created_at_idx ON nodes (created_at DESC);\nCREATE INDEX nodes_type_idx ON nodes (type);\n</code></pre>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#connection-configuration","title":"Connection Configuration","text":"<pre><code># Via environment variable (preferred)\nENV['HTM_DBURL'] = \"postgresql://user:pass@host:port/dbname?sslmode=require\"\n\n# Parsed into connection hash\n{\n  host: 'host',\n  port: 5432,\n  dbname: 'tsdb',\n  user: 'tsdbadmin',\n  password: 'secret',\n  sslmode: 'require'\n}\n</code></pre>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#key-features-enabled","title":"Key Features Enabled","text":"<p>Vector similarity search: <pre><code>-- Find semantically similar nodes\nSELECT *, 1 - (embedding &lt;=&gt; $1::vector) as similarity\nFROM nodes\nWHERE created_at &gt; NOW() - INTERVAL '30 days'\nORDER BY embedding &lt;=&gt; $1::vector\nLIMIT 10;\n</code></pre></p> <p>Full-text search: <pre><code>-- Find nodes containing keywords\nSELECT *, ts_rank(to_tsvector('english', value),\n                  plainto_tsquery('english', $1)) as rank\nFROM nodes\nWHERE to_tsvector('english', value) @@ plainto_tsquery('english', $1)\nORDER BY rank DESC\nLIMIT 10;\n</code></pre></p> <p>Time-range queries (optimized by chunk exclusion): <pre><code>-- Fast time-range query (TimescaleDB prunes chunks)\nSELECT * FROM nodes\nWHERE created_at BETWEEN '2025-10-01' AND '2025-10-25'\nAND robot_id = 'robot-123'\nORDER BY created_at DESC;\n</code></pre></p> <p>Automatic compression: <pre><code>-- Compress chunks older than 30 days\nSELECT add_compression_policy('nodes', INTERVAL '30 days');\n\n-- Segment by robot_id and type for better compression\nALTER TABLE nodes SET (\n  timescaledb.compress,\n  timescaledb.compress_segmentby = 'robot_id, type'\n);\n</code></pre></p>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/001-postgresql-timescaledb/#positive","title":"Positive","text":"<ul> <li>Production-ready with battle-tested reliability</li> <li>Multi-modal search: vector, full-text, and hybrid strategies</li> <li>Time-series optimization for efficient temporal queries</li> <li>Cost-effective storage with compression reducing cloud costs</li> <li>Familiar tooling: standard PostgreSQL tools and practices</li> <li>Flexible querying: full SQL power for complex operations</li> <li>ACID guarantees for critical memory operations</li> </ul>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#negative","title":"Negative","text":"<ul> <li>Operational complexity requires database management (mitigated by managed service)</li> <li>Vertical scaling limits (mitigated by partitioning)</li> <li>Connection overhead: PostgreSQL connections relatively heavy</li> <li>Vector search performance slower than specialized vector DBs at massive scale</li> </ul>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#neutral","title":"Neutral","text":"<ul> <li>Learning curve: developers need PostgreSQL + TimescaleDB knowledge</li> <li>Cloud dependency: currently using TimescaleDB Cloud (could self-host)</li> <li>Extension management requires extensions (timescaledb, pgvector, pg_trgm)</li> </ul>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#risks-and-mitigations","title":"Risks and Mitigations","text":""},{"location":"architecture/adrs/001-postgresql-timescaledb/#risk-extension-availability","title":"Risk: Extension Availability","text":"<p>Risk</p> <p>Extensions not available in all PostgreSQL environments</p> <p>Likelihood: Low (extensions widely available) Impact: High (breaks core functionality) Mitigation: Document requirements clearly, verify in setup process</p>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#risk-connection-exhaustion","title":"Risk: Connection Exhaustion","text":"<p>Risk</p> <p>PostgreSQL connections limited (default ~100)</p> <p>Likelihood: Medium (with many robots) Impact: Medium (service degradation) Mitigation: Implement connection pooling (ConnectionPool gem)</p>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#risk-storage-costs","title":"Risk: Storage Costs","text":"<p>Risk</p> <p>Vector data storage can be expensive at scale</p> <p>Likelihood: Medium (depends on usage) Impact: Medium (operational cost) Mitigation: Compression policies, retention policies, archival strategies</p>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#risk-query-performance-at-scale","title":"Risk: Query Performance at Scale","text":"<p>Risk</p> <p>Complex hybrid searches may slow with millions of nodes</p> <p>Likelihood: Low (with proper indexing) Impact: Medium (user experience) Mitigation: Query optimization, read replicas, caching layer</p>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#alternatives-comparison","title":"Alternatives Comparison","text":"Solution Pros Cons Decision Pure PostgreSQL Simple, reliable, pgvector No time-series optimization Rejected PostgreSQL + TimescaleDB Best of both worlds Slight complexity ACCEPTED Elasticsearch Excellent full-text search High resource usage, complexity Rejected Pinecone Purpose-built vectors Vendor lock-in, cost, limited relational Rejected SQLite Simple, embedded Limited concurrency, no vectors Rejected"},{"location":"architecture/adrs/001-postgresql-timescaledb/#future-considerations","title":"Future Considerations","text":"<ul> <li>Read replicas: For query scaling when needed</li> <li>Partitioning strategies: By robot_id for tenant isolation</li> <li>Caching layer: Redis for hot nodes</li> <li>Archive tier: S3/Glacier for very old memories</li> <li>Multi-region: For global deployment</li> </ul>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#references","title":"References","text":"<ul> <li>TimescaleDB Documentation</li> <li>pgvector Documentation</li> <li>PostgreSQL Full-Text Search</li> <li>HTM Database Schema Guide</li> <li>HTM Configuration Guide</li> </ul>"},{"location":"architecture/adrs/001-postgresql-timescaledb/#review-notes","title":"Review Notes","text":"<p>Systems Architect: Solid choice for time-series + vector workload. Consider read replicas for scaling.</p> <p>Database Architect: Excellent indexing strategy. Monitor query performance as data grows.</p> <p>Performance Specialist: TimescaleDB compression will help with costs. Add connection pooling soon.</p> <p>Maintainability Expert: PostgreSQL tooling is mature and well-documented. Good choice for long-term maintenance.</p>"},{"location":"architecture/adrs/002-two-tier-memory/","title":"ADR-002: Two-Tier Memory Architecture","text":"<p>Status: Accepted</p> <p>Date: 2025-10-25</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p>"},{"location":"architecture/adrs/002-two-tier-memory/#quick-summary","title":"Quick Summary","text":"<p>HTM implements a two-tier memory architecture with token-limited working memory (hot tier) and unlimited long-term memory (cold tier), managing LLM context windows while preserving all historical data through RAG-based retrieval.</p> <p>Why: LLMs have limited context windows but need awareness across long conversations. Two tiers provide fast access to recent context while maintaining complete history.</p> <p>Impact: Efficient token budget management with never-forget guarantees, at the cost of coordination between two storage layers.</p>"},{"location":"architecture/adrs/002-two-tier-memory/#context","title":"Context","text":"<p>LLM-based applications face a fundamental challenge: LLMs have limited context windows (typically 128K-200K tokens) but need to maintain awareness across long conversations and sessions spanning days, weeks, or months.</p>"},{"location":"architecture/adrs/002-two-tier-memory/#requirements","title":"Requirements","text":"<ul> <li>Persist memories across sessions (durable storage)</li> <li>Provide fast access to recent/relevant context</li> <li>Manage token budgets efficiently</li> <li>Never lose data accidentally</li> <li>Support contextual recall from the past</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#alternative-approaches","title":"Alternative Approaches","text":"<ol> <li>Database-only: Store everything in PostgreSQL, load on demand</li> <li>Memory-only: Keep everything in RAM, serialize on shutdown</li> <li>Two-tier: Combine fast working memory with durable long-term storage</li> <li>External service: Use a managed memory service</li> </ol>"},{"location":"architecture/adrs/002-two-tier-memory/#decision","title":"Decision","text":"<p>We will implement a two-tier memory architecture with:</p> <ul> <li>Working Memory: Token-limited, in-memory active context</li> <li>Long-term Memory: Durable PostgreSQL storage</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/002-two-tier-memory/#working-memory-hot-tier","title":"Working Memory (Hot Tier)","text":"<p>Characteristics: - Purpose: Immediate context for LLM - Storage: In-memory Ruby data structures - Capacity: Token-limited (default 128K tokens) - Eviction: LRU-based eviction when full - Access pattern: Frequent reads, moderate writes - Lifetime: Process lifetime</p> <p>Benefits: - O(1) hash lookups for fast context access - Token budget control prevents context overflow - Explicit eviction policy with transparent behavior</p>"},{"location":"architecture/adrs/002-two-tier-memory/#long-term-memory-cold-tier","title":"Long-term Memory (Cold Tier)","text":"<p>Characteristics: - Purpose: Permanent knowledge base - Storage: PostgreSQL with TimescaleDB - Capacity: Effectively unlimited - Retention: Permanent (explicit deletion only) - Access pattern: RAG-based retrieval - Lifetime: Forever</p> <p>Benefits: - Never lose data, survives restarts - Search historical context semantically - Time-series queries for temporal context</p>"},{"location":"architecture/adrs/002-two-tier-memory/#data-flow","title":"Data Flow","text":"<pre><code>Add Memory:\n  User Input \u2192 Working Memory \u2192 Long-term Memory\n               (immediate)      (persisted)\n\nRecall Memory:\n  Query \u2192 Long-term Memory (RAG search) \u2192 Working Memory\n          (semantic + temporal)            (evict if needed)\n\nEviction:\n  Working Memory (full) \u2192 Evict LRU \u2192 Long-term Memory (already there)\n                                       (mark as evicted, not deleted)\n</code></pre>"},{"location":"architecture/adrs/002-two-tier-memory/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adrs/002-two-tier-memory/#working-memory","title":"Working Memory","text":"<pre><code>class WorkingMemory\n  attr_reader :max_tokens, :token_count\n\n  def initialize(max_tokens: 128_000)\n    @nodes = {}           # key =&gt; {value, token_count, importance, timestamp}\n    @max_tokens = max_tokens\n    @token_count = 0\n    @access_order = []    # Track access for LRU\n  end\n\n  def add(key, value, token_count:, importance: 1.0)\n    evict_to_make_space(token_count) if needs_eviction?(token_count)\n    @nodes[key] = {\n      value: value,\n      token_count: token_count,\n      importance: importance,\n      added_at: Time.now,\n      last_accessed: Time.now\n    }\n    @token_count += token_count\n    @access_order &lt;&lt; key\n  end\n\n  def evict_to_make_space(needed_tokens)\n    # LRU eviction based on last access + importance\n    # See ADR-007 for detailed eviction strategy\n  end\n\n  def assemble_context(strategy: :balanced, max_tokens: nil)\n    # Sort by strategy and assemble within budget\n    # See ADR-006 for context assembly strategies\n  end\nend\n</code></pre>"},{"location":"architecture/adrs/002-two-tier-memory/#long-term-memory","title":"Long-term Memory","text":"<pre><code>class LongTermMemory\n  def add(key:, value:, embedding:, robot_id:, importance: 1.0, type: nil)\n    # Insert into PostgreSQL with vector embedding\n    @db.exec_params(&lt;&lt;~SQL, [key, value, embedding, robot_id, importance, type])\n      INSERT INTO nodes (key, value, embedding, robot_id, importance, type, created_at)\n      VALUES ($1, $2, $3, $4, $5, $6, CURRENT_TIMESTAMP)\n      RETURNING id\n    SQL\n  end\n\n  def search(timeframe:, query:, embedding_service:, limit:, strategy: :hybrid)\n    # RAG-based retrieval: temporal + semantic\n    # See ADR-005 for retrieval strategies\n  end\n\n  def mark_evicted(keys)\n    # Update in_working_memory flag (not deleted)\n    @db.exec_params(&lt;&lt;~SQL, [keys])\n      UPDATE nodes\n      SET in_working_memory = FALSE\n      WHERE key = ANY($1)\n    SQL\n  end\nend\n</code></pre>"},{"location":"architecture/adrs/002-two-tier-memory/#coordination-htm-class","title":"Coordination (HTM Class)","text":"<pre><code>class HTM\n  def initialize(robot_name:, robot_id: nil, max_tokens: 128_000, ...)\n    @working_memory = WorkingMemory.new(max_tokens: max_tokens)\n    @long_term_memory = LongTermMemory.new(db_config)\n    @embedding_service = EmbeddingService.new(...)\n    @robot_id = robot_id || SecureRandom.uuid\n    @robot_name = robot_name\n  end\n\n  def add_node(key, value, importance: 1.0, type: nil)\n    # 1. Generate embedding\n    embedding = @embedding_service.embed(value)\n\n    # 2. Store in long-term memory\n    @long_term_memory.add(\n      key: key,\n      value: value,\n      embedding: embedding,\n      robot_id: @robot_id,\n      importance: importance,\n      type: type\n    )\n\n    # 3. Add to working memory (evict if needed)\n    token_count = estimate_tokens(value)\n    @working_memory.add(key, value,\n                        token_count: token_count,\n                        importance: importance)\n  end\n\n  def recall(timeframe:, topic:, limit: 10, strategy: :hybrid)\n    # 1. Search long-term memory (RAG)\n    results = @long_term_memory.search(\n      timeframe: timeframe,\n      query: topic,\n      embedding_service: @embedding_service,\n      limit: limit,\n      strategy: strategy\n    )\n\n    # 2. Add results to working memory (evict if needed)\n    results.each do |node|\n      @working_memory.add(node[:key], node[:value],\n                          token_count: node[:token_count],\n                          importance: node[:importance])\n    end\n\n    # 3. Return nodes\n    results\n  end\nend\n</code></pre>"},{"location":"architecture/adrs/002-two-tier-memory/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/002-two-tier-memory/#positive","title":"Positive","text":"<ul> <li>Fast context access through O(1) working memory lookups</li> <li>Durable storage ensures never lose data, survives restarts</li> <li>Token budget control with automatic management</li> <li>Explicit eviction policy provides transparent behavior</li> <li>RAG-enabled search of historical context semantically</li> <li>Never-delete philosophy: eviction moves data, never removes</li> <li>Process-isolated: each robot instance has independent working memory</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#negative","title":"Negative","text":"<ul> <li>Complexity of coordinating two storage layers</li> <li>Memory overhead from working memory consuming RAM</li> <li>Synchronization challenges keeping both tiers consistent</li> <li>Eviction overhead when moving data between tiers</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#neutral","title":"Neutral","text":"<ul> <li>Token counting requires accurate estimation</li> <li>Strategy tuning for eviction and assembly needs calibration</li> <li>Per-process state means working memory not shared across processes</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#eviction-strategies","title":"Eviction Strategies","text":""},{"location":"architecture/adrs/002-two-tier-memory/#lru-based-implemented","title":"LRU-based (Implemented)","text":"<pre><code>def eviction_score(node)\n  recency = Time.now - node[:last_accessed]\n  importance = node[:importance]\n\n  # Lower score = evict first\n  importance / (recency + 1.0)\nend\n</code></pre> <p>See ADR-007: Working Memory Eviction Strategy for detailed eviction algorithm.</p>"},{"location":"architecture/adrs/002-two-tier-memory/#future-strategies","title":"Future Strategies","text":"<ul> <li>Importance-only: Keep most important nodes</li> <li>Recency-only: Pure LRU cache</li> <li>Frequency-based: Track access counts</li> <li>Category-based: Pin certain types (facts, preferences)</li> <li>Smart eviction: ML-based prediction of future access</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#context-assembly-strategies","title":"Context Assembly Strategies","text":""},{"location":"architecture/adrs/002-two-tier-memory/#recent-recent","title":"Recent (<code>:recent</code>)","text":"<p>Sort by <code>created_at DESC</code>, newest first</p>"},{"location":"architecture/adrs/002-two-tier-memory/#important-important","title":"Important (<code>:important</code>)","text":"<p>Sort by <code>importance DESC</code>, most important first</p>"},{"location":"architecture/adrs/002-two-tier-memory/#balanced-balanced","title":"Balanced (<code>:balanced</code>)","text":"<pre><code>score = importance * (1.0 / (1 + age_in_hours))\n</code></pre> <p>See ADR-006: Context Assembly Strategies for detailed assembly algorithms.</p>"},{"location":"architecture/adrs/002-two-tier-memory/#design-principles","title":"Design Principles","text":""},{"location":"architecture/adrs/002-two-tier-memory/#never-forget-unless-told","title":"Never Forget (Unless Told)","text":"<ul> <li>Eviction moves data, never deletes</li> <li>Only <code>forget(confirm: :confirmed)</code> deletes</li> <li>Long-term memory is append-only (updates rare)</li> </ul> <p>See ADR-009: Never-Forget Philosophy for deletion policies.</p>"},{"location":"architecture/adrs/002-two-tier-memory/#token-budget-management","title":"Token Budget Management","text":"<ul> <li>Token counting happens at add time</li> <li>Working memory enforces hard token limit</li> <li>Context assembly respects token budget</li> <li>Safety margin (10%) for token estimation errors</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#transparent-behavior","title":"Transparent Behavior","text":"<ul> <li>Log all evictions</li> <li>Track <code>in_working_memory</code> flag</li> <li>Operations log for audit trail</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#risks-and-mitigations","title":"Risks and Mitigations","text":""},{"location":"architecture/adrs/002-two-tier-memory/#risk-token-count-inaccuracy","title":"Risk: Token Count Inaccuracy","text":"<p>Risk</p> <p>Tiktoken approximation differs from LLM's actual count</p> <p>Likelihood: Medium (different tokenizers) Impact: Medium (context overflow) Mitigation: Add safety margin (10%), use LLM-specific counters</p>"},{"location":"architecture/adrs/002-two-tier-memory/#risk-eviction-thrashing","title":"Risk: Eviction Thrashing","text":"<p>Risk</p> <p>Constant eviction/recall cycles</p> <p>Likelihood: Low (with proper sizing) Impact: Medium (performance degradation) Mitigation: Larger working memory, smarter eviction, caching</p>"},{"location":"architecture/adrs/002-two-tier-memory/#risk-working-memory-growth","title":"Risk: Working Memory Growth","text":"<p>Risk</p> <p>Memory leaks or unbounded growth</p> <p>Likelihood: Low (token budget enforced) Impact: High (OOM crashes) Mitigation: Hard limits, monitoring, alerts</p>"},{"location":"architecture/adrs/002-two-tier-memory/#risk-stale-working-memory","title":"Risk: Stale Working Memory","text":"<p>Risk</p> <p>Working memory doesn't reflect long-term updates</p> <p>Likelihood: Low (single-writer pattern) Impact: Low (eventual consistency OK) Mitigation: Refresh on recall, invalidation on update</p>"},{"location":"architecture/adrs/002-two-tier-memory/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/adrs/002-two-tier-memory/#working-memory_1","title":"Working Memory","text":"<ul> <li>Add: O(1) amortized (eviction is O(n) when needed)</li> <li>Retrieve: O(1) hash lookup</li> <li>Eviction: O(n log n) for sorting, O(k) for removing k nodes</li> <li>Context assembly: O(n log n) for sorting, O(k) for selecting</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#long-term-memory_1","title":"Long-term Memory","text":"<ul> <li>Add: O(log n) PostgreSQL insert with indexes</li> <li>Vector search: O(log n) with HNSW index (approximate)</li> <li>Full-text search: O(log n) with GIN index</li> <li>Hybrid search: O(log n) for both, then merge</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Shared working memory: Redis-backed for multi-process</li> <li>Lazy loading: Load nodes on first access</li> <li>Pre-fetching: Anticipate needed context</li> <li>Compression: Compress old working memory nodes</li> <li>Tiered eviction: Multiple working memory levels</li> <li>Smart assembly: ML-driven context selection</li> </ol>"},{"location":"architecture/adrs/002-two-tier-memory/#references","title":"References","text":"<ul> <li>Working Memory (Psychology)</li> <li>Cache Eviction Policies</li> <li>LLM Context Window Management</li> <li>ADR-001: PostgreSQL Storage</li> <li>ADR-006: Context Assembly</li> <li>ADR-007: Eviction Strategy</li> </ul>"},{"location":"architecture/adrs/002-two-tier-memory/#review-notes","title":"Review Notes","text":"<p>Systems Architect: Clean separation of concerns. Consider shared cache for horizontal scaling.</p> <p>Performance Specialist: Good balance of speed and durability. Monitor eviction frequency.</p> <p>AI Engineer: Token budget management is critical. Add safety margins for token count variance.</p> <p>Ruby Expert: Consider using Concurrent::Map for thread-safe working memory in future.</p>"},{"location":"architecture/adrs/003-ollama-embeddings/","title":"ADR-003: Ollama as Default Embedding Provider","text":"<p>Status: Accepted (Reinstated After ADR-011 Reversal)</p> <p>Date: 2025-10-25 (Updated: 2025-10-27)</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p> <p>Architecture Status (October 2025)</p> <p>October 27, 2025: This ADR is once again the current architecture. Following the reversal of ADR-011, HTM has returned to client-side embedding generation using Ollama as the default provider. Embeddings are generated in Ruby before database insertion.</p>"},{"location":"architecture/adrs/003-ollama-embeddings/#quick-summary","title":"Quick Summary","text":"<p>HTM uses Ollama with the nomic-embed-text model as the default embedding provider, prioritizing local-first, privacy-preserving operation with zero API costs while supporting pluggable alternatives (OpenAI).</p> <p>Why: Local embeddings eliminate API costs, preserve privacy, and enable offline operation while maintaining good semantic search quality.</p> <p>Impact: Users must install Ollama locally, trading convenience for privacy and cost savings. Client-side embedding generation provides reliable operation without complex database extension dependencies.</p>"},{"location":"architecture/adrs/003-ollama-embeddings/#context","title":"Context","text":"<p>HTM requires vector embeddings for semantic search functionality. Embeddings convert text into high-dimensional vectors that capture semantic meaning, enabling similarity search beyond keyword matching.</p>"},{"location":"architecture/adrs/003-ollama-embeddings/#requirements","title":"Requirements","text":"<ul> <li>Generate embeddings for memory nodes</li> <li>Support semantic similarity search</li> <li>Consistent embedding dimensions (1536 recommended)</li> <li>Reasonable latency (&lt; 1 second per embedding)</li> <li>Cost-effective for development and production</li> <li>Privacy-preserving (sensitive data handling)</li> </ul>"},{"location":"architecture/adrs/003-ollama-embeddings/#options-considered","title":"Options Considered","text":"<ol> <li>OpenAI: text-embedding-3-small, excellent quality</li> <li>Ollama: Local models (gpt-oss, nomic-embed-text), privacy-first</li> <li>Cohere: embed-english-v3.0, good performance</li> <li>Anthropic: No native embedding API (yet)</li> <li>Sentence Transformers: Local Python models via API</li> <li>Voyage AI: Specialized embeddings, high quality</li> </ol>"},{"location":"architecture/adrs/003-ollama-embeddings/#decision","title":"Decision","text":"<p>We will use Ollama with the gpt-oss model as the default embedding provider for HTM, while supporting pluggable alternatives (OpenAI, Cohere, etc.).</p>"},{"location":"architecture/adrs/003-ollama-embeddings/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/003-ollama-embeddings/#why-ollama","title":"Why Ollama?","text":"<p>Local-first approach:</p> <ul> <li>Runs on user's machine (M2 Mac handles it well)</li> <li>No API costs during development</li> <li>No internet dependency once models downloaded</li> <li>Fast iteration without rate limits</li> </ul> <p>Privacy-preserving:</p> <ul> <li>Data never leaves the user's machine</li> <li>Critical for sensitive conversations</li> <li>No terms of service restrictions</li> <li>Full control over data</li> </ul> <p>Developer-friendly:</p> <ul> <li>Simple installation (<code>ollama pull gpt-oss</code>)</li> <li>HTTP API at localhost:11434</li> <li>Multiple model support</li> <li>Growing ecosystem</li> </ul> <p>Cost-effective:</p> <ul> <li>Zero ongoing costs</li> <li>Pay once for compute (user's hardware)</li> <li>No per-token pricing</li> <li>Predictable operational costs</li> </ul>"},{"location":"architecture/adrs/003-ollama-embeddings/#why-gpt-oss-model","title":"Why gpt-oss Model?","text":"<p>Technical characteristics:</p> <ul> <li>Vector dimension: 1536 (matches OpenAI text-embedding-3-small)</li> <li>Speed: ~100-300ms per embedding on M2 Mac</li> <li>Quality: Good semantic understanding for general text</li> <li>Size: Reasonable model size (~274MB)</li> </ul> <p>Compatibility:</p> <ul> <li>Same dimension as OpenAI (easier migration)</li> <li>Works with pgvector (supports any dimension)</li> <li>Compatible with other tools expecting 1536d vectors</li> </ul>"},{"location":"architecture/adrs/003-ollama-embeddings/#implementation-details","title":"Implementation Details","text":"<p>Architecture Change (October 2025)</p> <p>Embedding generation has moved from Ruby application code to database triggers via pgai. The implementation below is deprecated. See ADR-011 for current architecture.</p>"},{"location":"architecture/adrs/003-ollama-embeddings/#current-architecture-pgai-based","title":"Current Architecture (pgai-based)","text":"<p>Database Trigger (automatic embedding generation):</p> <pre><code>CREATE OR REPLACE FUNCTION generate_node_embedding()\nRETURNS TRIGGER AS $$\nDECLARE\n  embedding_provider TEXT;\n  embedding_model TEXT;\n  ollama_host TEXT;\n  generated_embedding vector;\nBEGIN\n  embedding_provider := COALESCE(current_setting('htm.embedding_provider', true), 'ollama');\n  embedding_model := COALESCE(current_setting('htm.embedding_model', true), 'nomic-embed-text');\n  ollama_host := COALESCE(current_setting('htm.ollama_url', true), 'http://localhost:11434');\n\n  IF embedding_provider = 'ollama' THEN\n    generated_embedding := ai.ollama_embed(embedding_model, NEW.value, host =&gt; ollama_host);\n  ELSIF embedding_provider = 'openai' THEN\n    generated_embedding := ai.openai_embed(embedding_model, NEW.value, api_key =&gt; current_setting('htm.openai_api_key', true));\n  END IF;\n\n  NEW.embedding := generated_embedding;\n  NEW.embedding_dimension := array_length(generated_embedding::real[], 1);\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER nodes_generate_embedding\n  BEFORE INSERT OR UPDATE OF value ON nodes\n  FOR EACH ROW\n  WHEN (NEW.embedding IS NULL OR NEW.value IS DISTINCT FROM OLD.value)\n  EXECUTE FUNCTION generate_node_embedding();\n</code></pre> <p>EmbeddingService (configuration only):</p> <pre><code>class EmbeddingService\n  def initialize(provider = :ollama, model: 'nomic-embed-text', ollama_url: nil, db_config: nil)\n    @provider = provider\n    @model = model\n    @ollama_url = ollama_url || ENV['OLLAMA_URL'] || 'http://localhost:11434'\n    @db_config = db_config\n    @dimensions = KNOWN_DIMENSIONS[@model]\n\n    configure_pgai if @db_config\n  end\n\n  def configure_pgai\n    conn = PG.connect(@db_config)\n    case @provider\n    when :ollama\n      conn.exec_params(\n        \"SELECT htm_set_embedding_config($1, $2, $3, NULL, $4)\",\n        ['ollama', @model, @ollama_url, @dimensions]\n      )\n    when :openai\n      api_key = ENV['OPENAI_API_KEY']\n      conn.exec_params(\n        \"SELECT htm_set_embedding_config($1, $2, NULL, $3, $4)\",\n        ['openai', @model, api_key, @dimensions]\n      )\n    end\n    conn.close\n  end\n\n  def embed(_text)\n    raise HTM::EmbeddingError, \"Direct embedding generation is deprecated. Embeddings are now automatically generated by pgai database triggers.\"\n  end\nend\n</code></pre>"},{"location":"architecture/adrs/003-ollama-embeddings/#legacy-architecture-deprecated","title":"Legacy Architecture (deprecated)","text":"Click to view deprecated Ruby-side embedding generation <pre><code># DEPRECATED: This architecture is no longer used\nclass EmbeddingService\n  def embed_ollama(text)\n    response = Net::HTTP.post(\n      URI(\"#{@ollama_url}/api/embeddings\"),\n      {model: @model, prompt: text}.to_json,\n      {'Content-Type' =&gt; 'application/json'}\n    )\n    JSON.parse(response.body)['embedding']\n  rescue =&gt; e\n    warn \"Error generating embedding with Ollama: #{e.message}\"\n    Array.new(768) { rand(-1.0..1.0) }\n  end\nend\n</code></pre>"},{"location":"architecture/adrs/003-ollama-embeddings/#user-configuration","title":"User Configuration","text":"<p>pgai Configuration</p> <p>With pgai, configuration sets database session variables. Embedding generation happens automatically via triggers.</p> <pre><code># Default: Ollama with nomic-embed-text (768 dimensions)\nhtm = HTM.new(robot_name: \"My Robot\")\n\n# Explicit Ollama configuration\nhtm = HTM.new(\n  robot_name: \"My Robot\",\n  embedding_provider: :ollama,\n  embedding_model: 'nomic-embed-text'\n)\n\n# Use different Ollama model\nhtm = HTM.new(\n  robot_name: \"My Robot\",\n  embedding_provider: :ollama,\n  embedding_model: 'mxbai-embed-large',  # 1024 dimensions\n  embedding_dimensions: 1024\n)\n\n# Use OpenAI\nhtm = HTM.new(\n  robot_name: \"My Robot\",\n  embedding_provider: :openai,\n  embedding_model: 'text-embedding-3-small'  # 1536 dimensions\n)\n\n# Add node - embedding generated automatically by database trigger!\nhtm.add_node(\"fact_001\", \"PostgreSQL is awesome\", type: :fact)\n# No embedding parameter needed - pgai handles it in the database\n</code></pre>"},{"location":"architecture/adrs/003-ollama-embeddings/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/003-ollama-embeddings/#positive","title":"Positive","text":"<ul> <li>Zero cost: no API fees for embedding generation</li> <li>Privacy-first: data stays local (Ollama runs locally)</li> <li>Fast iteration: no rate limits during development</li> <li>Offline capable: works without internet</li> <li>Simple setup: one command to install model</li> <li>Flexible: easy to swap providers later</li> <li>pgai Benefits (added October 2025):</li> <li>10-20% faster: Database-side generation eliminates Ruby HTTP overhead</li> <li>Automatic: Triggers handle embeddings on INSERT/UPDATE</li> <li>Simpler code: No application-side embedding calls</li> <li>Consistent: Same embedding model for all operations</li> <li>Parallel execution: PostgreSQL connection pooling enables concurrent embedding generation</li> </ul>"},{"location":"architecture/adrs/003-ollama-embeddings/#negative","title":"Negative","text":"<ul> <li>Setup required: users must install Ollama and pull model</li> <li>Hardware dependency: requires decent CPU/GPU (M2 Mac sufficient)</li> <li>Quality trade-off: not quite OpenAI quality (acceptable for most use cases)</li> <li>Compatibility: users on older hardware may struggle</li> <li>Debugging: local issues harder to diagnose than API errors</li> <li>pgai Requirements (added October 2025):</li> <li>PostgreSQL extension: Requires TimescaleDB Cloud or self-hosted with pgai installed</li> <li>Database coupling: Embedding logic now in database, not application</li> <li>Migration complexity: Existing applications need schema updates</li> </ul>"},{"location":"architecture/adrs/003-ollama-embeddings/#neutral","title":"Neutral","text":"<ul> <li>Model choice: gpt-oss is reasonable default, but users can experiment</li> <li>Version drift: Ollama model updates may change embeddings</li> <li>Dimension flexibility: could support other dimensions with schema changes</li> </ul>"},{"location":"architecture/adrs/003-ollama-embeddings/#setup-instructions","title":"Setup Instructions","text":"<p>Installation</p> <pre><code># Install Ollama\ncurl https://ollama.ai/install.sh | sh\n\n# Or download from: https://ollama.ai/download\n\n# Pull gpt-oss model\nollama pull gpt-oss\n\n# Verify Ollama is running\ncurl http://localhost:11434/api/version\n</code></pre>"},{"location":"architecture/adrs/003-ollama-embeddings/#risks-and-mitigations","title":"Risks and Mitigations","text":""},{"location":"architecture/adrs/003-ollama-embeddings/#risk-ollama-not-installed","title":"Risk: Ollama Not Installed","text":"<p>Risk</p> <p>Users try to use HTM without Ollama</p> <p>Likelihood: High (on first run) Impact: High (no embeddings, broken search) Mitigation: - Clear error messages with installation instructions - Fallback to stub embeddings (with warning) - Check Ollama availability in setup script</p>"},{"location":"architecture/adrs/003-ollama-embeddings/#risk-model-not-downloaded","title":"Risk: Model Not Downloaded","text":"<p>Risk</p> <p>Ollama installed but gpt-oss model not pulled</p> <p>Likelihood: Medium Impact: High (embedding generation fails) Mitigation: - Setup script checks for model - Error message includes <code>ollama pull gpt-oss</code> - Document in README and SETUP.md</p>"},{"location":"architecture/adrs/003-ollama-embeddings/#risk-performance-on-low-end-hardware","title":"Risk: Performance on Low-end Hardware","text":"<p>Risk</p> <p>Slow embedding generation on older machines</p> <p>Likelihood: Medium Impact: Medium (poor user experience) Mitigation: - Document minimum requirements - Provide alternative providers - Batch embedding generation where possible</p>"},{"location":"architecture/adrs/003-ollama-embeddings/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/adrs/003-ollama-embeddings/#ollama-gpt-oss-on-m2-mac","title":"Ollama (gpt-oss on M2 Mac)","text":"<ul> <li>Latency: 100-300ms per embedding</li> <li>Throughput: ~5-10 embeddings/second</li> <li>Memory: ~500MB for model</li> <li>CPU: Moderate (benefits from Apple Silicon)</li> </ul>"},{"location":"architecture/adrs/003-ollama-embeddings/#openai-for-comparison","title":"OpenAI (for comparison)","text":"<ul> <li>Latency: 50-150ms (network + API)</li> <li>Throughput: Limited by rate limits (3000 RPM = 50/sec)</li> <li>Cost: $0.02 per 1M tokens</li> <li>Quality: Slightly better semantic understanding</li> </ul>"},{"location":"architecture/adrs/003-ollama-embeddings/#migration-path","title":"Migration Path","text":""},{"location":"architecture/adrs/003-ollama-embeddings/#to-openai","title":"To OpenAI","text":"<pre><code># 1. Set up OpenAI API key\nENV['OPENAI_API_KEY'] = 'sk-...'\n\n# 2. Change initialization\nhtm = HTM.new(\n  robot_name: \"My Robot\",\n  embedding_service: :openai\n)\n\n# 3. Re-embed existing nodes (embeddings not compatible)\n# Migration tool needed\n</code></pre>"},{"location":"architecture/adrs/003-ollama-embeddings/#to-custom-ollama-url","title":"To Custom Ollama URL","text":"<pre><code>htm = HTM.new(\n  robot_name: \"My Robot\",\n  embedding_service: :ollama,\n  ollama_url: 'http://custom-host:11434'\n)\n</code></pre>"},{"location":"architecture/adrs/003-ollama-embeddings/#alternatives-comparison","title":"Alternatives Comparison","text":"Provider Quality Cost Privacy Decision Ollama (gpt-oss) Good Free Local DEFAULT OpenAI Excellent $0.02/1M Cloud Optional Cohere Excellent $0.10/1M Cloud Optional Sentence Transformers Good Free Local Future Voyage AI Excellent $0.12/1M Cloud Rejected"},{"location":"architecture/adrs/003-ollama-embeddings/#references","title":"References","text":"<ul> <li>Ollama Documentation</li> <li>nomic-embed-text Model</li> <li>OpenAI Embeddings</li> <li>pgvector Documentation</li> <li>pgai Documentation</li> <li>ADR-011: Database-Side Embedding Generation with pgai - Supersedes this ADR</li> <li>HTM Setup Guide</li> </ul>"},{"location":"architecture/adrs/003-ollama-embeddings/#review-notes","title":"Review Notes","text":"<p>AI Engineer: Local-first approach is excellent for privacy. Consider batch embedding for performance.</p> <p>Performance Specialist: 100-300ms is acceptable. Monitor for bottlenecks with large recall operations.</p> <p>Security Specialist: Privacy-preserving by default. Ensure users are aware of trade-offs when switching to cloud providers.</p> <p>Ruby Expert: Clean abstraction. Consider using Faraday for HTTP calls for better connection management.</p> <p>Systems Architect: Pluggable design allows easy provider switching. Good balance of pragmatism and flexibility.</p>"},{"location":"architecture/adrs/004-hive-mind/","title":"ADR-004: Multi-Robot Shared Memory (Hive Mind)","text":"<p>Status: Accepted</p> <p>Date: 2025-10-25</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p>"},{"location":"architecture/adrs/004-hive-mind/#quick-summary","title":"Quick Summary","text":"<p>HTM implements a hive mind architecture where all robots share a single global memory database with attribution tracking. This enables seamless context continuity across robots, cross-robot learning, and unified knowledge management without requiring users to repeat information.</p> <p>Why: Users often interact with multiple AI agents over time. Shared memory eliminates context fragmentation and enables robots to benefit from each other's experiences.</p> <p>Impact: Simplified architecture with unified search and seamless robot switching, at the cost of potential context pollution and privacy complexity.</p>"},{"location":"architecture/adrs/004-hive-mind/#context","title":"Context","text":"<p>In LLM-based applications, users often interact with multiple \"robots\" (AI agents) over time. These robots may serve different purposes (coding assistant, research assistant, chat companion) or represent different instances of the same application across sessions.</p>"},{"location":"architecture/adrs/004-hive-mind/#challenges-with-isolated-memory","title":"Challenges with Isolated Memory","text":"<ul> <li>Each robot has independent context</li> <li>User repeats information across robots</li> <li>No cross-robot learning</li> <li>Conversations fragmented across agents</li> <li>Lost context when switching robots</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#alternative-approaches","title":"Alternative Approaches","text":"<ol> <li>Isolated memory: Each robot has completely separate memory</li> <li>Shared memory (hive mind): All robots access global memory pool</li> <li>Hierarchical memory: Per-robot memory + shared global memory</li> <li>Explicit sharing: User chooses what to share across robots</li> </ol>"},{"location":"architecture/adrs/004-hive-mind/#decision","title":"Decision","text":"<p>We will implement a shared memory (hive mind) architecture where all robots access a single global memory database, with attribution tracking to identify which robot contributed each memory.</p>"},{"location":"architecture/adrs/004-hive-mind/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/004-hive-mind/#why-shared-memory","title":"Why Shared Memory?","text":"<p>Context continuity:</p> <ul> <li>User doesn't repeat themselves across robots</li> <li>\"You\" refers to the user consistently</li> <li>Preferences persist across sessions</li> <li>Conversation history accessible to all</li> </ul> <p>Cross-robot learning:</p> <ul> <li>Knowledge gained by one robot benefits all</li> <li>Architectural decisions visible to coding assistants</li> <li>Research findings available to writers</li> <li>Bug fixes remembered globally</li> </ul> <p>Simplified data model:</p> <ul> <li>Single source of truth</li> <li>No synchronization complexity</li> <li>Unified search across all conversations</li> <li>Consistent robot registry</li> </ul> <p>User experience:</p> <ul> <li>Seamless switching between robots</li> <li>Coherent memory across interactions</li> <li>No need to \"catch up\" new robots</li> <li>Transparent collaboration</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#attribution-tracking","title":"Attribution Tracking","text":"<p>Every node stores <code>robot_id</code>:</p> <pre><code>CREATE TABLE nodes (\n  ...\n  robot_id TEXT NOT NULL,\n  ...\n);\n</code></pre> <p>Benefits:</p> <ul> <li>Track which robot said what</li> <li>Debug conversation attribution</li> <li>Analyze robot behavior patterns</li> <li>Support privacy controls (future)</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#hive-mind-queries","title":"Hive Mind Queries","text":"<pre><code># Which robot discussed this topic?\nbreakdown = htm.which_robot_said(\"PostgreSQL\")\n# =&gt; { \"robot-123\" =&gt; 15, \"robot-456\" =&gt; 8 }\n\n# Get chronological conversation\ntimeline = htm.conversation_timeline(\"HTM design\", limit: 50)\n# =&gt; [{ timestamp: ..., robot: \"...\", content: \"...\" }, ...]\n</code></pre>"},{"location":"architecture/adrs/004-hive-mind/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adrs/004-hive-mind/#robot-registry","title":"Robot Registry","text":"<pre><code>CREATE TABLE robots (\n  id TEXT PRIMARY KEY,\n  name TEXT,\n  created_at TIMESTAMP,\n  last_active TIMESTAMP,\n  metadata JSONB\n);\n</code></pre> <p>Tracks all robots using the system:</p> <ul> <li>Registration on first use</li> <li>Activity timestamps</li> <li>Custom metadata (configuration, purpose, etc.)</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#robot-initialization","title":"Robot Initialization","text":"<pre><code>htm = HTM.new(\n  robot_name: \"Code Helper\",\n  robot_id: \"robot-123\"  # optional, auto-generated if not provided\n)\n\n# Registers robot in database\n@long_term_memory.register_robot(@robot_id, @robot_name)\n</code></pre>"},{"location":"architecture/adrs/004-hive-mind/#adding-memories-with-attribution","title":"Adding Memories with Attribution","text":"<pre><code>def add_node(key, value, ...)\n  node_id = @long_term_memory.add(\n    key: key,\n    value: value,\n    robot_id: @robot_id,  # Attribution\n    ...\n  )\nend\n</code></pre>"},{"location":"architecture/adrs/004-hive-mind/#querying-by-robot","title":"Querying by Robot","text":"<pre><code>-- All nodes by specific robot\nSELECT * FROM nodes WHERE robot_id = 'robot-123';\n\n-- Breakdown by robot\nSELECT robot_id, COUNT(*)\nFROM nodes\nWHERE value ILIKE '%PostgreSQL%'\nGROUP BY robot_id;\n</code></pre>"},{"location":"architecture/adrs/004-hive-mind/#working-memory-per-robot","title":"Working Memory: Per-Robot","text":"<p>Important Distinction</p> <p>While long-term memory is shared globally, working memory is per-robot instance (per-process):</p> <pre><code>class HTM\n  def initialize(...)\n    @working_memory = WorkingMemory.new(max_tokens: 128_000)  # Per-instance\n    @long_term_memory = LongTermMemory.new(db_config)         # Shared database\n  end\nend\n</code></pre> <p>Each robot has:</p> <ul> <li>Own working memory: Token-limited, process-local</li> <li>Shared long-term memory: Durable, global PostgreSQL</li> </ul> <p>This design provides:</p> <ul> <li>Fast local access (working memory)</li> <li>Global knowledge sharing (long-term memory)</li> <li>Process isolation (no cross-process RAM access needed)</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/004-hive-mind/#positive","title":"Positive","text":"<ul> <li>Seamless context: User never repeats information</li> <li>Cross-robot learning: Knowledge compounds across agents</li> <li>Conversation attribution: Clear ownership of memories</li> <li>Unified search: Find information regardless of which robot stored it</li> <li>Simplified architecture: Single database, no synchronization</li> <li>Activity tracking: Monitor robot usage patterns</li> <li>Debugging: Trace memories back to source robot</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#negative","title":"Negative","text":"<ul> <li>Privacy complexity: All robots see all data (no isolation)</li> <li>Namespace conflicts: Key collisions across robots (mitigated by UUID keys)</li> <li>Context pollution: Irrelevant memories from other robots</li> <li>Testing complexity: Shared state harder to isolate in tests</li> <li>Multi-tenancy: No built-in tenant isolation (future requirement)</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#neutral","title":"Neutral","text":"<ul> <li>Global namespace: Requires coordination for key naming</li> <li>Robot identity: User must provide meaningful robot names</li> <li>Memory attribution: \"Who said this?\" vs. \"What was said?\"</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#use-cases","title":"Use Cases","text":""},{"location":"architecture/adrs/004-hive-mind/#use-case-1-cross-session-context","title":"Use Case 1: Cross-Session Context","text":"<pre><code># Session 1 - Robot A\nhtm_a = HTM.new(robot_name: \"Code Helper A\")\nhtm_a.add_node(\"user_pref_001\", \"User prefers debug_me over puts\",\n               type: :preference)\n\n# Session 2 - Robot B (different process, later time)\nhtm_b = HTM.new(robot_name: \"Code Helper B\")\nmemories = htm_b.recall(timeframe: \"last week\", topic: \"debugging\")\n# =&gt; Finds preference from Robot A\n</code></pre>"},{"location":"architecture/adrs/004-hive-mind/#use-case-2-collaborative-development","title":"Use Case 2: Collaborative Development","text":"<pre><code># Robot A (architecture discussion)\nhtm_a.add_node(\"decision_001\",\n               \"We decided to use PostgreSQL for storage\",\n               type: :decision)\n\n# Robot B (implementation)\nhtm_b.recall(timeframe: \"today\", topic: \"database\")\n# =&gt; Finds architectural decision from Robot A\n</code></pre>"},{"location":"architecture/adrs/004-hive-mind/#use-case-3-activity-analysis","title":"Use Case 3: Activity Analysis","text":"<pre><code>-- Which robot has been most active?\nSELECT robot_id, COUNT(*) as contributions\nFROM nodes\nGROUP BY robot_id\nORDER BY contributions DESC;\n\n-- What did each robot contribute this week?\nSELECT r.name, COUNT(n.id) as memories_added\nFROM robots r\nJOIN nodes n ON n.robot_id = r.id\nWHERE n.created_at &gt; NOW() - INTERVAL '7 days'\nGROUP BY r.name;\n</code></pre>"},{"location":"architecture/adrs/004-hive-mind/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/adrs/004-hive-mind/#decision-global-by-default","title":"Decision: Global by Default","text":"<p>Rationale: Simplicity and user experience trump isolation. Users can implement privacy layers on top if needed.</p> <p>Alternative: Per-robot namespaces with opt-in sharing</p> <p>Rejected: Adds complexity, defeats purpose of hive mind</p>"},{"location":"architecture/adrs/004-hive-mind/#decision-robot-id-required","title":"Decision: Robot ID Required","text":"<p>Rationale: Essential for attribution and debugging</p> <p>Alternative: Optional robot_id</p> <p>Rejected: Lose critical context and debugging capability</p>"},{"location":"architecture/adrs/004-hive-mind/#decision-working-memory-per-process","title":"Decision: Working Memory Per-Process","text":"<p>Rationale: Avoid distributed state synchronization complexity</p> <p>Alternative: Shared working memory (Redis)</p> <p>Deferred: Consider for multi-process/multi-host scenarios</p>"},{"location":"architecture/adrs/004-hive-mind/#risks-and-mitigations","title":"Risks and Mitigations","text":""},{"location":"architecture/adrs/004-hive-mind/#risk-context-pollution","title":"Risk: Context Pollution","text":"<p>Risk</p> <p>Robot sees irrelevant memories from other robots</p> <p>Likelihood: Medium (depends on use patterns)</p> <p>Impact: Medium (degraded relevance)</p> <p>Mitigation:</p> <ul> <li>Importance scoring helps filter</li> <li>Robot-specific recall filters (future)</li> <li>Category/tag-based filtering</li> <li>Smart context assembly</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#risk-privacy-violations","title":"Risk: Privacy Violations","text":"<p>Risk</p> <p>Sensitive data accessible to all robots</p> <p>Likelihood: Low (single-user scenario)</p> <p>Impact: High (if multi-user)</p> <p>Mitigation:</p> <ul> <li>Document single-user assumption</li> <li>Add row-level security for multi-tenant (future)</li> <li>Encryption for sensitive data (future)</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#risk-key-collisions","title":"Risk: Key Collisions","text":"<p>Risk</p> <p>Different robots use same key for different data</p> <p>Likelihood: Low (UUID recommendations)</p> <p>Impact: Medium (data corruption)</p> <p>Mitigation:</p> <ul> <li>Recommend UUIDs or prefixed keys</li> <li>Unique constraint on key column</li> <li>Error handling for collisions</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#risk-unbounded-growth","title":"Risk: Unbounded Growth","text":"<p>Risk</p> <p>Memory grows indefinitely with multiple robots</p> <p>Likelihood: High (no automatic cleanup)</p> <p>Impact: Medium (storage costs, query slowdown)</p> <p>Mitigation:</p> <ul> <li>Retention policies (future)</li> <li>Archival strategies</li> <li>Importance-based pruning (future)</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#future-enhancements","title":"Future Enhancements","text":""},{"location":"architecture/adrs/004-hive-mind/#privacy-controls","title":"Privacy Controls","text":"<pre><code># Mark memories as private to specific robot\nhtm.add_node(\"private_key\", \"sensitive data\",\n             visibility: :private)  # Only accessible to this robot\n\n# Or shared with specific robots\nhtm.add_node(\"shared_key\", \"team data\",\n             visibility: [:shared, robot_ids: ['robot-a', 'robot-b']])\n</code></pre>"},{"location":"architecture/adrs/004-hive-mind/#robot-groupsteams","title":"Robot Groups/Teams","text":"<pre><code># Group robots by purpose\nhtm.add_robot_to_group(\"robot-123\", \"coding-team\")\nhtm.add_robot_to_group(\"robot-456\", \"research-team\")\n\n# Query by group\nmemories = htm.recall(robot_group: \"coding-team\", topic: \"APIs\")\n</code></pre>"},{"location":"architecture/adrs/004-hive-mind/#multi-tenancy","title":"Multi-Tenancy","text":"<pre><code># Tenant isolation\nhtm = HTM.new(\n  robot_name: \"Helper\",\n  tenant_id: \"user-abc123\"  # Row-level security\n)\n</code></pre>"},{"location":"architecture/adrs/004-hive-mind/#alternatives-comparison","title":"Alternatives Comparison","text":"Approach Pros Cons Decision Shared Memory (Hive Mind) Seamless UX, cross-learning Privacy, pollution ACCEPTED Isolated Memory Complete isolation, simple privacy User repeats info, no learning Rejected Hierarchical Memory Best of both worlds Complex sync, unclear semantics Rejected Explicit Sharing User control High friction, complexity Rejected Federated Memory (P2P) Distributed Sync complexity, consistency Rejected"},{"location":"architecture/adrs/004-hive-mind/#references","title":"References","text":"<ul> <li>Collective Intelligence</li> <li>Hive Mind Concept</li> <li>Multi-Agent Systems</li> <li>ADR-002: Two-Tier Memory</li> <li>ADR-008: Robot Identification</li> <li>Multi-Robot Guide</li> </ul>"},{"location":"architecture/adrs/004-hive-mind/#review-notes","title":"Review Notes","text":"<p>Systems Architect: Solid choice for single-user scenario. Plan for multi-tenancy early.</p> <p>Domain Expert: Hive mind metaphor maps well to shared knowledge base. Consider robot personality/role in memory interpretation.</p> <p>Security Specialist: Single-user assumption is critical. Document clearly and add tenant isolation before production multi-user deployment.</p> <p>AI Engineer: Cross-robot context sharing improves LLM effectiveness. Monitor for context pollution in practice.</p> <p>Database Architect: Robot_id indexing will scale well. Consider partitioning by robot_id if one robot dominates.</p>"},{"location":"architecture/adrs/005-rag-retrieval/","title":"ADR-005: RAG-Based Retrieval with Hybrid Search","text":"<p>Status: Accepted (Updated for Client-Side Embeddings)</p> <p>Date: 2025-10-25 (Updated: 2025-10-27)</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p> <p>Architecture Update (October 2025)</p> <p>Following the reversal of ADR-011, query embeddings are now generated client-side in Ruby using <code>EmbeddingService</code> before being passed to SQL for vector similarity search. This provides a reliable, cross-platform solution.</p>"},{"location":"architecture/adrs/005-rag-retrieval/#quick-summary","title":"Quick Summary","text":"<p>HTM implements RAG-based retrieval with three search strategies: vector search (semantic), full-text search (keywords), and hybrid search (combined). All strategies include temporal filtering to leverage TimescaleDB's time-series optimization.</p> <p>Why: Different queries benefit from different approaches. Semantic search handles concepts, full-text handles precise terms, and hybrid provides the best balance for most use cases.</p> <p>Impact: Flexible retrieval with excellent recall and precision. Client-side embedding generation provides reliable, debuggable operation across all platforms.</p>"},{"location":"architecture/adrs/005-rag-retrieval/#context","title":"Context","text":"<p>Traditional memory systems for LLMs face challenges in retrieving relevant information:</p> <ul> <li>Keyword-only search: Misses semantic relationships (\"car\" vs \"automobile\")</li> <li>Vector-only search: May miss exact keyword matches (\"PostgreSQL 17.2\" vs \"database\")</li> <li>No temporal context: Doesn't leverage time-based relevance</li> <li>Scalability: Simple linear scans don't scale to thousands of memories</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#requirements","title":"Requirements","text":"<p>HTM needs intelligent retrieval that balances:</p> <ul> <li>Semantic understanding (what does the query mean?)</li> <li>Keyword precision (exact term matching)</li> <li>Temporal relevance (recent vs historical context)</li> <li>Performance (fast retrieval from large datasets)</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#alternative-approaches","title":"Alternative Approaches","text":"<ol> <li>Pure vector search: Semantic only, no keyword precision</li> <li>Pure full-text search: Keywords only, no semantic understanding</li> <li>Hybrid search: Combine vector + full-text + temporal filtering</li> <li>LLM-as-retriever: Use LLM to generate retrieval queries (slow, expensive)</li> </ol>"},{"location":"architecture/adrs/005-rag-retrieval/#decision","title":"Decision","text":"<p>We will implement RAG-based retrieval with three search strategies: vector, full-text, and hybrid, all with temporal filtering.</p>"},{"location":"architecture/adrs/005-rag-retrieval/#search-strategies","title":"Search Strategies","text":"<p>1. Vector Search (<code>:vector</code>)</p> <ul> <li>Generate embedding for query</li> <li>Compute cosine similarity with stored embeddings</li> <li>Temporal filtering on timeframe</li> <li>Best for: Semantic queries, conceptual relationships</li> </ul> <p>2. Full-Text Search (<code>:fulltext</code>)</p> <ul> <li>PostgreSQL <code>to_tsvector</code> and <code>plainto_tsquery</code></li> <li><code>ts_rank</code> scoring for relevance</li> <li>Temporal filtering on timeframe</li> <li>Best for: Exact keywords, technical terms, proper nouns</li> </ul> <p>3. Hybrid Search (<code>:hybrid</code>) - Recommended Default</p> <ul> <li>Full-text pre-filter to get candidates (top 100)</li> <li>Vector reranking of candidates for semantic relevance</li> <li>Temporal filtering on timeframe</li> <li>Best for: Balanced retrieval with precision + recall</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/005-rag-retrieval/#why-rag-based-retrieval","title":"Why RAG-Based Retrieval?","text":"<p>Temporal filtering is foundational:</p> <ul> <li>\"What did we discuss last week?\" - time is the primary filter</li> <li>Recent context often more relevant than old context</li> <li>TimescaleDB optimized for time-range queries</li> </ul> <p>Semantic search handles synonyms:</p> <ul> <li>User says \"database\", finds memories about \"PostgreSQL\"</li> <li>\"Bug fix\" matches \"resolved issue\"</li> <li>Captures conceptual relationships</li> </ul> <p>Full-text handles precision:</p> <ul> <li>\"PostgreSQL 17.2\" needs exact version match</li> <li>Technical terminology like \"pgvector\", \"HNSW\"</li> <li>Proper nouns like robot names, project names</li> </ul> <p>Hybrid combines strengths:</p> <ul> <li>Pre-filter with keywords reduces vector search space</li> <li>Vector reranking improves relevance of keyword matches</li> <li>Avoids false positives from pure vector search</li> <li>Avoids missing results from pure keyword search</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#implementation-details","title":"Implementation Details","text":"<p>Client-Side Embedding Generation</p> <p>Query embeddings are generated client-side in Ruby via <code>EmbeddingService</code> before being passed to SQL for vector similarity search.</p>"},{"location":"architecture/adrs/005-rag-retrieval/#vector-search","title":"Vector Search","text":"<pre><code>def search(timeframe:, query:, limit:, embedding_service:)\n  # Generate query embedding client-side\n  query_embedding = embedding_service.embed(query)\n\n  # Pad to 2000 dimensions if needed\n  query_embedding += Array.new(2000 - query_embedding.length, 0.0) if query_embedding.length &lt; 2000\n\n  # Convert to PostgreSQL vector format\n  embedding_str = \"[#{query_embedding.join(',')}]\"\n\n  # Vector search in database\n  conn.exec_params(&lt;&lt;~SQL, [embedding_str, timeframe.begin, timeframe.end, limit])\n    SELECT id, content, speaker, type, category, importance, created_at, robot_id, token_count,\n           1 - (embedding &lt;=&gt; $1::vector) as similarity\n    FROM nodes\n    WHERE created_at BETWEEN $2 AND $3\n    AND embedding IS NOT NULL\n    ORDER BY embedding &lt;=&gt; $1::vector\n    LIMIT $4\n  SQL\nend\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#full-text-search","title":"Full-Text Search","text":"<pre><code>def search_fulltext(timeframe:, query:, limit:)\n  # No embedding needed for full-text search\n  conn.exec_params(&lt;&lt;~SQL, [query, timeframe.begin, timeframe.end, limit])\n    SELECT *, ts_rank(to_tsvector('english', content), plainto_tsquery('english', $1)) as rank\n    FROM nodes\n    WHERE created_at BETWEEN $2 AND $3\n    AND to_tsvector('english', content) @@ plainto_tsquery('english', $1)\n    ORDER BY rank DESC\n    LIMIT $4\n  SQL\nend\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#hybrid-search","title":"Hybrid Search","text":"<pre><code>def search_hybrid(timeframe:, query:, limit:, embedding_service:, prefilter_limit: 100)\n  # Generate query embedding client-side\n  query_embedding = embedding_service.embed(query)\n  query_embedding += Array.new(2000 - query_embedding.length, 0.0) if query_embedding.length &lt; 2000\n  embedding_str = \"[#{query_embedding.join(',')}]\"\n\n  # Combine full-text pre-filter with vector reranking\n  conn.exec_params(&lt;&lt;~SQL, [embedding_str, timeframe.begin, timeframe.end, query, prefilter_limit, limit])\n    WITH candidates AS (\n      SELECT id, content, speaker, type, category, importance, created_at, robot_id, token_count, embedding\n      FROM nodes\n      WHERE created_at BETWEEN $2 AND $3\n      AND to_tsvector('english', content) @@ plainto_tsquery('english', $4)\n      AND embedding IS NOT NULL\n      LIMIT $5  -- Pre-filter to top candidates\n    )\n    SELECT id, content, speaker, type, category, importance, created_at, robot_id, token_count,\n           1 - (embedding &lt;=&gt; $1::vector) as similarity\n    FROM candidates\n    ORDER BY embedding &lt;=&gt; $1::vector\n    LIMIT $6  -- Final top results\n  SQL\nend\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#user-api","title":"User API","text":"<pre><code># Use hybrid search (recommended)\nmemories = htm.recall(\n  timeframe: \"last week\",\n  topic: \"PostgreSQL performance\",\n  limit: 20,\n  strategy: :hybrid  # default recommended\n)\n\n# Use pure vector search\nmemories = htm.recall(\n  timeframe: \"last month\",\n  topic: \"database design philosophy\",\n  strategy: :vector  # best for conceptual queries\n)\n\n# Use pure full-text search\nmemories = htm.recall(\n  timeframe: \"yesterday\",\n  topic: \"PostgreSQL 17.2 upgrade\",\n  strategy: :fulltext  # best for exact keywords\n)\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/005-rag-retrieval/#positive","title":"Positive","text":"<ul> <li>Flexible retrieval: Choose strategy based on query type</li> <li>Temporal context: Time-range filtering built into all strategies</li> <li>Semantic understanding: Vector search captures relationships</li> <li>Keyword precision: Full-text search handles exact matches</li> <li>Balanced hybrid: Best of both worlds with pre-filter optimization</li> <li>Scalable: HNSW indexing on vectors, GIN indexing on tsvectors</li> <li>Transparent scoring: Return similarity/rank scores for debugging</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#negative","title":"Negative","text":"<ul> <li>Complexity: Three strategies to understand and choose from</li> <li>Embedding latency: Vector/hybrid require embedding generation</li> <li>Storage overhead: Both embeddings and full-text indexes</li> <li>English-only: Full-text optimized for English language</li> <li>Tuning required: Hybrid prefilter_limit may need adjustment</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#neutral","title":"Neutral","text":"<ul> <li>Strategy selection: User must choose appropriate strategy</li> <li>Timeframe parsing: Natural language time parsing adds complexity</li> <li>Embedding consistency: Different embedding models produce different results</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#use-cases","title":"Use Cases","text":""},{"location":"architecture/adrs/005-rag-retrieval/#use-case-1-semantic-concept-retrieval","title":"Use Case 1: Semantic Concept Retrieval","text":"<pre><code># Query: \"What architectural decisions have we made?\"\n# Best strategy: :vector (semantic concept matching)\n\nmemories = htm.recall(\n  timeframe: \"last month\",\n  topic: \"architectural decisions design choices\",\n  strategy: :vector\n)\n\n# Finds: \"We decided to use PostgreSQL\", \"Chose two-tier memory model\", etc.\n# Matches conceptually even without exact keywords\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#use-case-2-exact-technical-term","title":"Use Case 2: Exact Technical Term","text":"<pre><code># Query: \"Find all mentions of PostgreSQL 17.2\"\n# Best strategy: :fulltext (exact version number)\n\nmemories = htm.recall(\n  timeframe: \"this week\",\n  topic: \"PostgreSQL 17.2\",\n  strategy: :fulltext\n)\n\n# Finds: Exact \"PostgreSQL 17.2\" mentions\n# Avoids false matches to \"PostgreSQL 16\" or generic \"database\"\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#use-case-3-balanced-query","title":"Use Case 3: Balanced Query","text":"<pre><code># Query: \"What did we discuss about database performance?\"\n# Best strategy: :hybrid (keyword + semantic)\n\nmemories = htm.recall(\n  timeframe: \"last week\",\n  topic: \"database performance optimization\",\n  strategy: :hybrid\n)\n\n# Pre-filters: Documents containing \"database\", \"performance\", \"optimization\"\n# Reranks: By semantic similarity to full query\n# Result: Best balance of precision + recall\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#use-case-4-conversation-timeline","title":"Use Case 4: Conversation Timeline","text":"<pre><code># Get chronological conversation about a topic\ntimeline = htm.conversation_timeline(\"HTM design\", limit: 50)\n\n# Returns memories sorted by created_at\n# Useful for replaying decision evolution over time\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#performance-characteristics","title":"Performance Characteristics","text":"<p>Client-Side Embedding Generation</p> <p>Embeddings are generated client-side before SQL queries. Latency includes HTTP call to Ollama/OpenAI for embedding generation.</p>"},{"location":"architecture/adrs/005-rag-retrieval/#vector-search_1","title":"Vector Search","text":"<ul> <li>Latency: ~30-50ms for client-side embedding + index lookup</li> <li>Index: HNSW (Hierarchical Navigable Small World)</li> <li>Scalability: O(log n) with HNSW, sublinear</li> <li>Best case: Conceptual queries, semantic relationships</li> <li>Breakdown: ~20-30ms embedding generation, ~10-20ms vector search</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#full-text-search_1","title":"Full-Text Search","text":"<ul> <li>Latency: ~5-20ms (no embedding generation)</li> <li>Index: GIN (Generalized Inverted Index) on tsvector</li> <li>Scalability: O(log n) with GIN index</li> <li>Best case: Exact keywords, technical terms</li> <li>Benefit: Fastest option when embeddings not needed</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#hybrid-search_1","title":"Hybrid Search","text":"<ul> <li>Latency: Full-text pre-filter + client-side embedding + vector reranking</li> <li>Total: ~35-70ms</li> <li>Optimization: Pre-filter reduces vector search space</li> <li>Best case: Large datasets where full-text can narrow candidates</li> <li>Breakdown: ~20-30ms embedding, ~5-10ms full-text, ~10-30ms vector reranking</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#temporal-filtering","title":"Temporal Filtering","text":"<ul> <li>Optimization: TimescaleDB hypertable partitioning by time</li> <li>Index: B-tree on <code>created_at</code> column</li> <li>Benefit: Prunes partitions outside timeframe, faster scans</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/adrs/005-rag-retrieval/#decision-three-strategies-instead-of-one","title":"Decision: Three Strategies Instead of One","text":"<p>Rationale: Different queries benefit from different approaches. Give users flexibility.</p> <p>Alternative: Single hybrid strategy for all queries</p> <p>Rejected: Forces hybrid approach even when pure vector or full-text is better</p>"},{"location":"architecture/adrs/005-rag-retrieval/#decision-temporal-filtering-is-mandatory","title":"Decision: Temporal Filtering is Mandatory","text":"<p>Rationale: HTM is time-oriented. All retrieval should consider temporal context.</p> <p>Alternative: Optional timeframe parameter</p> <p>Rejected: Easy to forget, defeats TimescaleDB optimization benefits</p>"},{"location":"architecture/adrs/005-rag-retrieval/#decision-hybrid-pre-filter-limit-100","title":"Decision: Hybrid Pre-filter Limit = 100","text":"<p>Rationale: Balances recall (enough candidates) with performance (vector search cost)</p> <p>Alternative: Dynamic limit based on result count</p> <p>Deferred: Can optimize later based on real-world usage patterns</p>"},{"location":"architecture/adrs/005-rag-retrieval/#decision-return-similarityrank-scores","title":"Decision: Return Similarity/Rank Scores","text":"<p>Rationale: Enables debugging, threshold filtering, and understanding retrieval quality</p> <p>Alternative: Just return nodes without scores</p> <p>Rejected: Lose valuable signal for debugging and optimization</p>"},{"location":"architecture/adrs/005-rag-retrieval/#risks-and-mitigations","title":"Risks and Mitigations","text":""},{"location":"architecture/adrs/005-rag-retrieval/#risk-wrong-strategy-selection","title":"Risk: Wrong Strategy Selection","text":"<p>Risk</p> <p>User chooses vector for exact keyword query (poor results)</p> <p>Likelihood: Medium (requires understanding differences)</p> <p>Impact: Medium (degraded retrieval quality)</p> <p>Mitigation:</p> <ul> <li>Default to hybrid for balanced results</li> <li>Document use cases clearly</li> <li>Provide examples in API docs</li> <li>Consider auto-detection in future</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#risk-embedding-latency","title":"Risk: Embedding Latency","text":"<p>Risk</p> <p>Vector/hybrid slow due to embedding generation</p> <p>Likelihood: High (embedding is I/O bound)</p> <p>Impact: Medium (100-500ms for Ollama)</p> <p>Mitigation:</p> <ul> <li>Cache embeddings for common queries (future)</li> <li>Use fast local embedding models (gpt-oss)</li> <li>Provide fallback to full-text if embedding fails</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#risk-language-limitation","title":"Risk: Language Limitation","text":"<p>Risk</p> <p>Full-text search optimized for English only</p> <p>Likelihood: Low (single-user, likely English)</p> <p>Impact: High (non-English users)</p> <p>Mitigation:</p> <ul> <li>Document English assumption</li> <li>Support language parameter in future</li> <li>Vector search language-agnostic (works for all languages)</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#risk-pre-filter-misses-results","title":"Risk: Pre-filter Misses Results","text":"<p>Risk</p> <p>Hybrid pre-filter (100) misses relevant candidates</p> <p>Likelihood: Low (100 is generous)</p> <p>Impact: Medium (reduced recall)</p> <p>Mitigation:</p> <ul> <li>Make prefilter_limit configurable</li> <li>Monitor recall metrics in practice</li> <li>Adjust default if needed</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#future-enhancements","title":"Future Enhancements","text":""},{"location":"architecture/adrs/005-rag-retrieval/#query-auto-detection","title":"Query Auto-Detection","text":"<pre><code># Automatically choose strategy based on query\nhtm.recall_smart(timeframe: \"last week\", topic: \"PostgreSQL 17.2\")\n# Detects version number \u2192 uses :fulltext\n\nhtm.recall_smart(timeframe: \"last month\", topic: \"architectural philosophy\")\n# Detects conceptual query \u2192 uses :vector\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#re-ranking-strategies","title":"Re-ranking Strategies","text":"<pre><code># Custom re-ranking based on multiple signals\nmemories = htm.recall(\n  timeframe: \"last week\",\n  topic: \"PostgreSQL\",\n  strategy: :hybrid,\n  rerank: [:similarity, :importance, :recency]  # Multi-factor scoring\n)\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#query-expansion","title":"Query Expansion","text":"<pre><code># LLM-powered query expansion\noriginal = \"database\"\nexpanded = [\"database\", \"PostgreSQL\", \"TimescaleDB\", \"SQL\", \"storage\"]\n\nmemories = htm.recall(\n  timeframe: \"last month\",\n  topic: expanded,\n  strategy: :fulltext\n)\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#caching-layer","title":"Caching Layer","text":"<pre><code># Cache embedding generation for common queries\n@embedding_cache = {}\n\ndef search_cached(query)\n  @embedding_cache[query] ||= embedding_service.embed(query)\nend\n</code></pre>"},{"location":"architecture/adrs/005-rag-retrieval/#alternatives-comparison","title":"Alternatives Comparison","text":"Approach Pros Cons Decision Hybrid Search Balanced precision + recall Strategy selection ACCEPTED Pure Vector Only Simplest API, semantic Misses exact matches, slower Rejected Pure Full-Text Only Fast, no embeddings No semantic understanding Rejected LLM-as-Retriever Most flexible queries Too slow, expensive Rejected Elasticsearch Dedicated search engine Additional infrastructure Rejected"},{"location":"architecture/adrs/005-rag-retrieval/#references","title":"References","text":"<ul> <li>RAG (Retrieval-Augmented Generation)</li> <li>pgvector Documentation</li> <li>PostgreSQL Full-Text Search</li> <li>HNSW Algorithm</li> <li>Hybrid Search Best Practices</li> <li>ADR-001: PostgreSQL Storage</li> <li>ADR-003: Ollama Embeddings - Superseded by ADR-011</li> <li>ADR-011: Database-Side Embedding Generation with pgai - Superseded (returned to client-side)</li> <li>Search Strategies Guide</li> </ul>"},{"location":"architecture/adrs/005-rag-retrieval/#review-notes","title":"Review Notes","text":"<p>AI Engineer: Hybrid search is the right approach for RAG systems. Pre-filter optimization is smart.</p> <p>Database Architect: TimescaleDB + pgvector + full-text is well-architected. Consider query plan analysis for optimization.</p> <p>Performance Specialist: HNSW and GIN indexes will scale. Monitor embedding latency in production.</p> <p>Systems Architect: Three strategies provide good flexibility. Document decision matrix clearly for users.</p> <p>Ruby Expert: Clean API design. Consider strategy as default parameter: <code>recall(..., strategy: :hybrid)</code></p>"},{"location":"architecture/adrs/006-context-assembly/","title":"ADR-006: Context Assembly Strategies","text":"<p>Status: Accepted</p> <p>Date: 2025-10-25</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p>"},{"location":"architecture/adrs/006-context-assembly/#quick-summary","title":"Quick Summary","text":"<p>HTM implements three context assembly strategies: recent (newest first), important (highest priority first), and balanced (importance weighted by time decay). Users choose the strategy based on their use case when preparing context for LLMs.</p> <p>Why: Different tasks need different context. Chat needs recency, strategic planning needs importance, and general assistance benefits from a balanced approach.</p> <p>Impact: Flexible context assembly with predictable behavior and user control, at the cost of strategy selection responsibility.</p>"},{"location":"architecture/adrs/006-context-assembly/#context","title":"Context","text":"<p>When preparing context for an LLM, working memory may contain more information than can fit within token limits. HTM needs to intelligently select which memories to include in the assembled context string.</p>"},{"location":"architecture/adrs/006-context-assembly/#challenges-in-context-assembly","title":"Challenges in Context Assembly","text":"<ul> <li>Token limits: LLMs have finite context windows (even with 128K working memory)</li> <li>Relevance: Not all memories are equally important for current task</li> <li>Recency bias: Recent context often more relevant, but not always</li> <li>Priority conflicts: Important old memories vs. less important recent ones</li> <li>Performance: Context assembly should be fast (&lt; 10ms)</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#alternative-approaches","title":"Alternative Approaches","text":"<ol> <li>FIFO (First-In-First-Out): Always include oldest memories</li> <li>LIFO (Last-In-First-Out): Always include newest memories</li> <li>Importance-only: Sort by importance score</li> <li>Recency-only: Sort by access time</li> <li>Balanced (hybrid): Combine importance and recency with decay function</li> </ol>"},{"location":"architecture/adrs/006-context-assembly/#decision","title":"Decision","text":"<p>We will implement three context assembly strategies: recent, important, and balanced, allowing users to choose based on their use case.</p>"},{"location":"architecture/adrs/006-context-assembly/#strategy-definitions","title":"Strategy Definitions","text":"<p>1. Recent (<code>:recent</code>)</p> <ul> <li>Sort by access order, most recently accessed first</li> <li>Best for: Conversational continuity, following current discussion thread</li> <li>Use case: Chat interfaces, debugging sessions, iterative development</li> </ul> <p>2. Important (<code>:important</code>)</p> <ul> <li>Sort by importance score, highest first</li> <li>Best for: Critical information, architectural decisions, key facts</li> <li>Use case: Decision-making, strategic planning, summarization</li> </ul> <p>3. Balanced (<code>:balanced</code>) - Recommended Default</p> <ul> <li>Hybrid scoring: <code>importance * (1.0 / (1 + recency_hours))</code></li> <li>Importance weighted by time decay (1-hour half-life)</li> <li>Best for: General-purpose context assembly</li> <li>Use case: Most LLM interactions, mixed conversational + strategic context</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/006-context-assembly/#why-multiple-strategies","title":"Why Multiple Strategies?","text":"<p>Different tasks need different context:</p> <ul> <li>Chat conversation: Recent context critical for coherence</li> <li>Strategic planning: Important decisions matter more than recency</li> <li>General assistance: Balance of both</li> </ul> <p>User control over LLM context:</p> <ul> <li>Explicit strategy selection gives predictable behavior</li> <li>No hidden heuristics or magic</li> <li>Easy to debug context issues</li> </ul> <p>Performance and simplicity:</p> <ul> <li>All strategies are simple sorts (O(n log n))</li> <li>No ML models or complex algorithms</li> <li>Fast context assembly (&lt; 10ms for typical working memory)</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#decay-function-analysis","title":"Decay Function Analysis","text":"<p>The balanced strategy uses a 1-hour half-life decay:</p> <pre><code>score = importance * (1.0 / (1 + hours))\n\nExamples:\n- Just added (0 hours): importance * 1.0 (full weight)\n- 1 hour old: importance * 0.5 (half weight)\n- 3 hours old: importance * 0.25 (quarter weight)\n- 24 hours old: importance * 0.04 (4% weight)\n</code></pre> <p>This means:</p> <ul> <li>Recent memories get full importance weight</li> <li>Importance decays quickly in first few hours</li> <li>Very old memories need high importance to compete</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adrs/006-context-assembly/#context-assembly-function","title":"Context Assembly Function","text":"<pre><code>def assemble_context(strategy:, max_tokens: nil)\n  max = max_tokens || @max_tokens\n\n  nodes = case strategy\n  when :recent\n    # Most recently accessed first\n    @access_order.reverse.map { |k| @nodes[k] }\n\n  when :important\n    # Highest importance first\n    @nodes.sort_by { |k, v| -v[:importance] }.map(&amp;:last)\n\n  when :balanced\n    # Importance * recency decay (1-hour half-life)\n    @nodes.sort_by { |k, v|\n      recency_hours = (Time.now - v[:added_at]) / 3600.0\n      score = v[:importance] * (1.0 / (1 + recency_hours))\n      -score  # Negate for descending sort\n    }.map(&amp;:last)\n\n  else\n    raise ArgumentError, \"Unknown strategy: #{strategy}\"\n  end\n\n  # Build context up to token limit\n  context_parts = []\n  current_tokens = 0\n\n  nodes.each do |node|\n    break if current_tokens + node[:token_count] &gt; max\n    context_parts &lt;&lt; node[:value]\n    current_tokens += node[:token_count]\n  end\n\n  context_parts.join(\"\\n\\n\")\nend\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#user-api","title":"User API","text":"<pre><code># Use balanced strategy (recommended default)\ncontext = htm.create_context(strategy: :balanced)\n\n# Use recent for conversational continuity\ncontext = htm.create_context(strategy: :recent, max_tokens: 4000)\n\n# Use important for strategic decisions\ncontext = htm.create_context(strategy: :important)\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/006-context-assembly/#positive","title":"Positive","text":"<ul> <li>Flexible: Choose strategy based on use case</li> <li>Predictable: Clear sorting behavior, no hidden heuristics</li> <li>Fast: Simple sorting algorithms, &lt; 10ms</li> <li>Debuggable: Easy to understand why context contains certain memories</li> <li>User control: Explicit strategy selection</li> <li>Sensible default: Balanced strategy works well for most cases</li> <li>Token-aware: Respects max_tokens limit in all strategies</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#negative","title":"Negative","text":"<ul> <li>Strategy selection burden: User must understand differences</li> <li>No automatic optimization: Doesn't learn optimal strategy</li> <li>Decay tuning: 1-hour half-life may not suit all use cases</li> <li>No semantic clustering: Doesn't group related memories together</li> <li>Position bias: Earlier context may have more LLM influence</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#neutral","title":"Neutral","text":"<ul> <li>Importance scoring: Requires user to assign meaningful importance</li> <li>Access tracking: Recent strategy depends on access order</li> <li>Token estimation: Accuracy depends on token counting precision</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#use-cases","title":"Use Cases","text":""},{"location":"architecture/adrs/006-context-assembly/#use-case-1-conversational-chat","title":"Use Case 1: Conversational Chat","text":"<pre><code># User having back-and-forth conversation with LLM\n# Recent context is critical for coherence\n\ncontext = htm.create_context(strategy: :recent, max_tokens: 8000)\n\n# Example memories in working memory:\n# - \"User prefers debug_me over puts\" (importance: 9, 5 days old)\n# - \"What is the capital of France?\" (importance: 1, 2 minutes ago)\n# - \"Paris is the capital\" (importance: 1, 1 minute ago)\n\n# Result: Recent conversation about Paris included first,\n# even though user preference is more important\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#use-case-2-strategic-planning","title":"Use Case 2: Strategic Planning","text":"<pre><code># LLM helping with architectural decisions\n# Important decisions matter more than recent chat\n\ncontext = htm.create_context(strategy: :important)\n\n# Example memories:\n# - \"We decided to use PostgreSQL\" (importance: 10, 3 days ago)\n# - \"What time is it?\" (importance: 1, 1 minute ago)\n# - \"TimescaleDB chosen for time-series\" (importance: 9, 2 days ago)\n\n# Result: Architectural decisions included first,\n# time check ignored if space limited\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#use-case-3-general-assistance-balanced","title":"Use Case 3: General Assistance (Balanced)","text":"<pre><code># LLM helping with mixed tasks\n# Balance recent context + important knowledge\n\ncontext = htm.create_context(strategy: :balanced)\n\n# Example memories:\n# - \"User prefers debug_me\" (importance: 9, 5 days ago) \u2192 score: 0.007\n# - \"PostgreSQL decision\" (importance: 10, 3 days ago) \u2192 score: 0.014\n# - \"Current debugging issue\" (importance: 5, 10 minutes ago) \u2192 score: 3.0\n# - \"Error: foreign key violation\" (importance: 7, 2 minutes ago) \u2192 score: 21.0\n\n# Result: Recent debugging context ranked highest,\n# but important decisions still included if space permits\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#use-case-4-custom-token-limit","title":"Use Case 4: Custom Token Limit","text":"<pre><code># Generate short summary for preview\nshort_context = htm.create_context(strategy: :important, max_tokens: 500)\n\n# Generate full context for LLM\nfull_context = htm.create_context(strategy: :balanced, max_tokens: 128_000)\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/adrs/006-context-assembly/#complexity","title":"Complexity","text":"<ul> <li>Recent: O(n) - reverse access order array</li> <li>Important: O(n log n) - sort by importance</li> <li>Balanced: O(n log n) - sort by computed score</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#typical-performance","title":"Typical Performance","text":"<ul> <li>Working memory size: 50-200 nodes</li> <li>Sorting time: &lt; 5ms (all strategies)</li> <li>String assembly: &lt; 5ms</li> <li>Total: &lt; 10ms for context assembly</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#memory-usage","title":"Memory Usage","text":"<ul> <li>No duplication: Nodes stored once, sorted references</li> <li>Temporary arrays: O(n) for sorted node references</li> <li>Output string: O(total_tokens) for assembled context</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/adrs/006-context-assembly/#decision-three-strategies-instead-of-one","title":"Decision: Three Strategies Instead of One","text":"<p>Rationale: Different use cases benefit from different strategies. Flexibility &gt; simplicity.</p> <p>Alternative: Single balanced strategy for all use cases</p> <p>Rejected: Forces one-size-fits-all approach, limits user control</p>"},{"location":"architecture/adrs/006-context-assembly/#decision-balanced-as-default","title":"Decision: Balanced as Default","text":"<p>Rationale: Best general-purpose behavior, balances competing priorities</p> <p>Alternative: Recent as default</p> <p>Rejected: Important long-term knowledge gets lost in conversations</p>"},{"location":"architecture/adrs/006-context-assembly/#decision-1-hour-decay-half-life","title":"Decision: 1-Hour Decay Half-Life","text":"<p>Rationale:</p> <ul> <li>1 hour matches typical development session length</li> <li>Quick decay prevents stale context from dominating</li> <li>Long enough to preserve within-session continuity</li> </ul> <p>Alternative: Configurable decay parameter</p> <p>Deferred: Can add if real-world usage shows need for tuning</p>"},{"location":"architecture/adrs/006-context-assembly/#decision-linear-decay-1-1-hours","title":"Decision: Linear Decay (1 / (1 + hours))","text":"<p>Rationale: Simple, predictable, computationally cheap</p> <p>Alternative: Exponential decay <code>exp(-\u03bb * hours)</code></p> <p>Rejected: More complex, harder to reason about, minimal practical difference</p>"},{"location":"architecture/adrs/006-context-assembly/#decision-token-limit-enforced-strictly","title":"Decision: Token Limit Enforced Strictly","text":"<p>Rationale: Never exceed LLM context window, prevent truncation errors</p> <p>Alternative: Soft limit with warnings</p> <p>Rejected: Hard limits prevent surprising behavior</p>"},{"location":"architecture/adrs/006-context-assembly/#risks-and-mitigations","title":"Risks and Mitigations","text":""},{"location":"architecture/adrs/006-context-assembly/#risk-suboptimal-decay-parameter","title":"Risk: Suboptimal Decay Parameter","text":"<p>Risk</p> <p>1-hour half-life doesn't match real usage patterns</p> <p>Likelihood: Medium (usage patterns vary)</p> <p>Impact: Low (balanced strategy still works reasonably)</p> <p>Mitigation:</p> <ul> <li>Monitor real-world usage patterns</li> <li>Make decay configurable if needed</li> <li>Document decay behavior clearly</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#risk-strategy-confusion","title":"Risk: Strategy Confusion","text":"<p>Risk</p> <p>Users don't understand which strategy to use</p> <p>Likelihood: Medium (three choices require understanding)</p> <p>Impact: Low (balanced default works well)</p> <p>Mitigation:</p> <ul> <li>Clear documentation with use cases</li> <li>Examples in API docs</li> <li>Sensible default (balanced)</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#risk-position-bias-in-llm","title":"Risk: Position Bias in LLM","text":"<p>Risk</p> <p>LLM pays more attention to early context</p> <p>Likelihood: High (known LLM behavior)</p> <p>Impact: Medium (affects which memories have most influence)</p> <p>Mitigation:</p> <ul> <li>Document bias in user guide</li> <li>Consider reverse ordering for some LLMs (future)</li> <li>Let users experiment with strategies</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#risk-importance-scoring-inconsistency","title":"Risk: Importance Scoring Inconsistency","text":"<p>Risk</p> <p>Users assign arbitrary importance scores</p> <p>Likelihood: High (subjective scoring)</p> <p>Impact: Medium (affects balanced and important strategies)</p> <p>Mitigation:</p> <ul> <li>Document importance scoring guidelines</li> <li>Provide default importance (1.0) for most memories</li> <li>Consider learned importance in future</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#future-enhancements","title":"Future Enhancements","text":""},{"location":"architecture/adrs/006-context-assembly/#automatic-strategy-selection","title":"Automatic Strategy Selection","text":"<pre><code># Detect conversation vs planning context\ncontext = htm.create_context_smart()\n\n# Uses recent for conversational turns\n# Uses important for strategic questions\n# Uses balanced for mixed contexts\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#configurable-decay","title":"Configurable Decay","text":"<pre><code># Adjust decay half-life\ncontext = htm.create_context(\n  strategy: :balanced,\n  decay_hours: 0.5  # Faster decay\n)\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#semantic-clustering","title":"Semantic Clustering","text":"<pre><code># Group related memories together\ncontext = htm.create_context(\n  strategy: :clustered,\n  cluster_by: :embedding  # Group semantically related nodes\n)\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#llm-optimized-ordering","title":"LLM-Optimized Ordering","text":"<pre><code># Reverse ordering for LLMs with recency bias\ncontext = htm.create_context(\n  strategy: :balanced,\n  order: :reverse  # Most important last (closer to query)\n)\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#multi-factor-scoring","title":"Multi-Factor Scoring","text":"<pre><code># Custom scoring function\ncontext = htm.create_context(\n  strategy: :custom,\n  score_fn: -&gt;(node) {\n    recency = Time.now - node[:added_at]\n    importance = node[:importance]\n    access_count = node[:access_count]\n\n    importance * (1.0 / (1 + recency / 3600)) * Math.log(1 + access_count)\n  }\n)\n</code></pre>"},{"location":"architecture/adrs/006-context-assembly/#alternatives-comparison","title":"Alternatives Comparison","text":"Approach Pros Cons Decision Three Strategies Flexible, user control Selection burden ACCEPTED Always Include Everything No selection logic Exceeds token limits Rejected LLM-Powered Selection Most intelligent Too slow, expensive Rejected Learned Importance Automatic optimization Complex, non-deterministic Deferred Semantic Similarity Most relevant Slower, breaks generality Deferred"},{"location":"architecture/adrs/006-context-assembly/#references","title":"References","text":"<ul> <li>LLM Context Window Management</li> <li>Attention Mechanisms in LLMs</li> <li>Position Bias in Language Models</li> <li>Working Memory in Cognitive Science</li> <li>ADR-002: Two-Tier Memory</li> <li>ADR-007: Eviction Strategy</li> <li>Context Assembly Guide</li> </ul>"},{"location":"architecture/adrs/006-context-assembly/#review-notes","title":"Review Notes","text":"<p>AI Engineer: Three strategies cover common use cases well. Balanced default is smart. Consider position bias documentation.</p> <p>Performance Specialist: O(n log n) sorting is fast enough for typical working memory sizes. No concerns.</p> <p>Ruby Expert: Clean API design. Consider default parameter: <code>create_context(strategy: :balanced)</code> \u2192 <code>create_context(strategy = :balanced)</code>.</p> <p>Domain Expert: Decay function is intuitive. 1-hour half-life matches development session rhythm.</p> <p>Systems Architect: Strategy pattern is appropriate. Document decision matrix for users.</p>"},{"location":"architecture/adrs/007-eviction-strategy/","title":"ADR-007: Working Memory Eviction Strategy","text":"<p>Status: Accepted</p> <p>Date: 2025-10-25</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p>"},{"location":"architecture/adrs/007-eviction-strategy/#quick-summary","title":"Quick Summary","text":"<p>HTM implements a hybrid eviction strategy that prioritizes memories by both importance and recency, evicting low-importance older memories first when working memory reaches token capacity. Evicted memories are preserved in long-term storage, never deleted.</p> <p>Why: Token limits are hard constraints, but eviction policy determines which context stays active. Hybrid approach balances preserving critical knowledge with maintaining temporal relevance.</p> <p>Impact: Predictable eviction behavior with never-forget guarantees, at the cost of requiring meaningful importance scoring from users.</p>"},{"location":"architecture/adrs/007-eviction-strategy/#context","title":"Context","text":"<p>Working memory is token-limited (default 128,000 tokens). When adding new memories that would exceed the token limit, HTM must decide which existing memories to evict.</p>"},{"location":"architecture/adrs/007-eviction-strategy/#eviction-challenges","title":"Eviction Challenges","text":"<ul> <li>Token limits: Hard constraint, cannot exceed working memory capacity</li> <li>Importance preservation: Critical memories should remain in working memory</li> <li>Recency: Recent context often more relevant than old context</li> <li>Fairness: Don't always evict the same types of memories</li> <li>Performance: Eviction should be fast (&lt; 10ms)</li> <li>Never forget: Evicted memories must be preserved in long-term memory</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#alternative-eviction-policies","title":"Alternative Eviction Policies","text":"<ol> <li>FIFO (First-In-First-Out): Evict oldest memories</li> <li>LRU (Least Recently Used): Evict least recently accessed</li> <li>LFU (Least Frequently Used): Evict least frequently accessed</li> <li>Random: Evict random memories</li> <li>Importance-only: Evict lowest importance first</li> <li>Hybrid: Combine importance and recency with scoring function</li> </ol>"},{"location":"architecture/adrs/007-eviction-strategy/#decision","title":"Decision","text":"<p>We will implement a hybrid eviction strategy that prioritizes memories by both importance and recency, evicting low-importance older memories first.</p> <p> <p>Hybrid Eviction Algorithm</p> <p>Eviction Process</p> <p> 1. Calculate Eviction Score score = [importance, -recency]</p> <p> 2. Sort Memories by Score Primary: importance (asc), Secondary: age (desc)</p> <p> 3. Greedy Eviction Evict from front until needed_tokens freed</p> <p> 4. Mark as Evicted Set in_working_memory = false in database</p> <p> </p> <p>Eviction Priority Tiers (Lower tier = evicted first)</p> <p> Tier 1: Low Importance + Old Evicted First importance: 1.0, age: 5 days \u2192 evicted 1<sup>st</sup> importance: 2.0, age: 3 days \u2192 evicted 2<sup>nd</sup> Examples: Temporary notes, scratch data</p> <p> Tier 2: Low Importance + Recent Evicted Second importance: 1.0, age: 1 hour \u2192 evicted 3<sup>rd</sup> importance: 2.0, age: 30 min \u2192 evicted 4<sup>th</sup> Examples: Recent but low-value context</p> <p> Tier 3: High Importance + Old Evicted Third importance: 9.0, age: 30 days \u2192 evicted 5<sup>th</sup> importance: 10.0, age: 90 days \u2192 evicted 6<sup>th</sup> Examples: Old but critical decisions</p> <p> Tier 4: High Importance + Recent Kept (Evicted Last) importance: 9.0, age: 1 hour \u2192 kept longest importance: 10.0, age: 5 min \u2192 never evicted (if possible) Examples: Critical + actively used data</p> <p>Guarantees</p> <p> \u2713 Never-forget principle \u2713 Preserved in long-term \u2713 Can be recalled \u2713 No data loss \u2713 Deterministic order \u2713 O(n log n) complexity</p> <p> Performance: \u2022 Sort: O(n log n) \u2022 Eviction: O(k) greedy \u2022 Typical: &lt;10ms \u2022 Memory: O(n) temp \u2022 Stops when enough   space freed</p> <p>Example: Need 5,000 tokens</p> <p></p> <p>Working memory contains: \u2022 \"random_note\" (imp: 1.0, 2000 tok, 1 hr ago) \u2192 EVICTED (Tier 2) \u2022 \"debug_log\" (imp: 2.0, 1500 tok, 2 days ago) \u2192 EVICTED (Tier 1) \u2022 \"temp_calc\" (imp: 1.5, 1600 tok, 5 days ago) \u2192 EVICTED (Tier 1) \u2022 \"user_pref\" (imp: 8.0, 100 tok, 5 days ago) \u2192 KEPT (Tier 3) \u2022 \"architecture_decision\" (imp: 10.0, 3000 tok, 3 days ago) \u2192 KEPT (Tier 3) </p>"},{"location":"architecture/adrs/007-eviction-strategy/#eviction-algorithm","title":"Eviction Algorithm","text":"<pre><code>def evict_to_make_space(needed_tokens)\n  # Sort by [importance, -recency]\n  # Lower importance evicted first\n  # Within same importance, older memories evicted first\n  candidates = @nodes.sort_by do |key, node|\n    recency = Time.now - node[:added_at]\n    [node[:importance], -recency]\n  end\n\n  # Evict from front until enough space\n  evicted = []\n  tokens_freed = 0\n\n  candidates.each do |key, node|\n    break if tokens_freed &gt;= needed_tokens\n\n    evicted &lt;&lt; { key: key, value: node[:value] }\n    tokens_freed += node[:token_count]\n    @nodes.delete(key)\n    @access_order.delete(key)\n  end\n\n  evicted\nend\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#eviction-scoring","title":"Eviction Scoring","text":"<p>Primary sort: Importance (ascending)</p> <ul> <li>Importance 1.0 evicted before importance 5.0</li> <li>Importance 5.0 evicted before importance 10.0</li> </ul> <p>Secondary sort: Recency (descending age)</p> <ul> <li>Within same importance, older memories evicted first</li> <li>Added 5 days ago evicted before added 1 hour ago</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#never-forget-guarantee","title":"Never-Forget Guarantee","text":"<p>Critical Design Principle</p> <p>Evicted memories are NOT deleted. They:</p> <ol> <li>Remain in long-term memory (PostgreSQL) permanently</li> <li>Can be recalled via <code>recall()</code> when needed</li> <li>Marked as <code>in_working_memory = FALSE</code> in database</li> <li>Automatically reloaded if recalled again</li> </ol>"},{"location":"architecture/adrs/007-eviction-strategy/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/007-eviction-strategy/#why-hybrid-importance-recency","title":"Why Hybrid (Importance + Recency)?","text":"<p>Importance alone is insufficient:</p> <ul> <li>All memories might have same importance</li> <li>Old important memories eventually become stale</li> <li>Doesn't account for temporal relevance</li> </ul> <p>Recency alone is insufficient:</p> <ul> <li>Critical information gets evicted just because it's old</li> <li>Architectural decisions disappear after time passes</li> <li>Loses important long-term context</li> </ul> <p>Hybrid balances both:</p> <ul> <li>Low-importance recent memories evicted before high-importance old ones</li> <li>Within same importance, temporal relevance matters</li> <li>Preserves critical knowledge while making space for new context</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#why-this-sort-order","title":"Why This Sort Order?","text":"<pre><code>[node[:importance], -recency]\n</code></pre> <p>This creates tiers:</p> <ol> <li>Tier 1 (evict first): Low importance, old</li> <li>Tier 2: Low importance, recent</li> <li>Tier 3: High importance, old</li> <li>Tier 4 (evict last): High importance, recent</li> </ol> <p>Example eviction order:</p> <pre><code>importance: 1.0, age: 5 days   \u2192 evicted first\nimportance: 1.0, age: 1 hour   \u2192 evicted second\nimportance: 5.0, age: 5 days   \u2192 evicted third\nimportance: 5.0, age: 1 hour   \u2192 evicted fourth\nimportance: 10.0, age: 5 days  \u2192 evicted fifth\nimportance: 10.0, age: 1 hour  \u2192 kept (evict last)\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#eviction-guarantees","title":"Eviction Guarantees","text":"<p>Safety:</p> <ul> <li>Evicted memories preserved in long-term storage</li> <li>No data loss, only working memory removal</li> <li>Can be recalled when needed</li> </ul> <p>Performance:</p> <ul> <li>Greedy eviction: stops as soon as enough space freed</li> <li>O(n log n) sorting (one-time cost)</li> <li>O(k) eviction where k = nodes evicted</li> <li>Typical eviction: &lt; 10ms for 100-node working memory</li> </ul> <p>Correctness:</p> <ul> <li>Always frees enough tokens (or evicts all nodes trying)</li> <li>Respects importance ordering</li> <li>Deterministic (same state \u2192 same evictions)</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/007-eviction-strategy/#positive","title":"Positive","text":"<ul> <li>Preserves important context: High-importance memories stay longer</li> <li>Temporal relevance: Recent context preferred over old</li> <li>Never forgets: Evicted memories remain in long-term storage</li> <li>Predictable: Clear eviction order based on importance + recency</li> <li>Fast: O(n log n) sort, greedy eviction</li> <li>Greedy: Only evicts what's necessary, no over-eviction</li> <li>Safe: No data loss, recallable from long-term memory</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#negative","title":"Negative","text":"<ul> <li>No access frequency: Doesn't track how often memory is used</li> <li>No semantic clustering: May split related memories</li> <li>Importance subjectivity: Relies on user-assigned importance</li> <li>Batch eviction cost: O(n log n) sort on every eviction</li> <li>No look-ahead: Doesn't predict which memories will be needed soon</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#neutral","title":"Neutral","text":"<ul> <li>Importance matters: Users must assign meaningful importance scores</li> <li>Eviction visibility: No notification when memories evicted</li> <li>Recall overhead: Need to recall evicted memories if needed again</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#use-cases","title":"Use Cases","text":""},{"location":"architecture/adrs/007-eviction-strategy/#use-case-1-adding-large-memory-to-full-working-memory","title":"Use Case 1: Adding Large Memory to Full Working Memory","text":"<pre><code># Working memory: 127,500 / 128,000 tokens (99% full)\n# Attempt to add 5,000 token memory\n\nhtm.add_node(\"new_large_memory\", large_text, importance: 7.0)\n\n# Eviction needed: 5,000 - 500 = 4,500 tokens\n\n# Working memory contains:\n# - \"user_pref\" (importance: 8.0, 100 tokens, 5 days old)\n# - \"random_note\" (importance: 1.0, 2,000 tokens, 1 hour ago)\n# - \"architecture_decision\" (importance: 10.0, 3,000 tokens, 3 days ago)\n# - \"debug_log\" (importance: 2.0, 1,500 tokens, 2 days ago)\n\n# Eviction order:\n# 1. \"random_note\" (importance: 1.0) \u2192 2,000 tokens freed\n# 2. \"debug_log\" (importance: 2.0) \u2192 3,500 tokens freed\n# 3. Stop (4,500 &gt; needed)\n\n# Result: user_pref and architecture_decision preserved\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#use-case-2-importance-ties","title":"Use Case 2: Importance Ties","text":"<pre><code># All memories have importance: 5.0\n\n# Working memory:\n# - \"note_1\" (5 days old, 1,000 tokens)\n# - \"note_2\" (3 days old, 1,000 tokens)\n# - \"note_3\" (1 hour ago, 1,000 tokens)\n\n# Need to evict 2,000 tokens\n\n# Eviction order:\n# 1. \"note_1\" (oldest) \u2192 1,000 tokens\n# 2. \"note_2\" (second oldest) \u2192 2,000 tokens\n# 3. Stop\n\n# Result: Most recent note preserved\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#use-case-3-recall-evicted-memory","title":"Use Case 3: Recall Evicted Memory","text":"<pre><code># Memory evicted from working memory\nhtm.add_node(\"temp_note\", \"Some temporary information\", importance: 1.0)\n# ... later evicted to make space ...\n\n# Later: recall the evicted memory\nmemories = htm.recall(timeframe: \"last week\", topic: \"temporary information\")\n\n# Result: Memory retrieved from long-term storage\n# And automatically added back to working memory\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#use-case-4-high-importance-old-memory","title":"Use Case 4: High-Importance Old Memory","text":"<pre><code># Critical decision made months ago\nhtm.add_node(\"critical_decision\",\n             \"We must never use MongoDB for time-series data\",\n             importance: 10.0)\n\n# ... 90 days later, working memory full ...\n\n# Many recent low-importance memories added\n# Critical decision still in working memory due to importance: 10.0\n\n# Eviction: Low-importance recent memories evicted first\n# Result: Critical decision preserved despite being 90 days old\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/adrs/007-eviction-strategy/#time-complexity","title":"Time Complexity","text":"<ul> <li>Sorting: O(n log n) where n = nodes in working memory</li> <li>Eviction: O(k) where k = nodes evicted</li> <li>Total: O(n log n + k) \u2248 O(n log n)</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#space-complexity","title":"Space Complexity","text":"<ul> <li>Sorted candidates: O(n) temporary array</li> <li>Evicted list: O(k) returned result</li> <li>Total: O(n) additional memory</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#typical-performance","title":"Typical Performance","text":"<ul> <li>Working memory size: 50-200 nodes</li> <li>Eviction frequency: Low (most additions fit without eviction)</li> <li>Sort time: &lt; 5ms for 200 nodes</li> <li>Eviction time: &lt; 1ms (greedy, stops early)</li> <li>Total: &lt; 10ms per eviction event</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#optimization-opportunities","title":"Optimization Opportunities","text":"<ul> <li>Cache sorted order: Invalidate on add/remove</li> <li>Incremental sort: Use heap for O(k log n) eviction</li> <li>Lazy eviction: Evict only when space check fails</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/adrs/007-eviction-strategy/#decision-hybrid-importance-recency-vs-pure-policies","title":"Decision: Hybrid (Importance + Recency) vs Pure Policies","text":"<p>Rationale: Balances competing priorities better than any single factor</p> <p>Alternative: Pure LRU (least recently used)</p> <p>Rejected: Loses important long-term context</p> <p>Alternative: Pure importance-based</p> <p>Rejected: All old memories evicted regardless of importance</p>"},{"location":"architecture/adrs/007-eviction-strategy/#decision-primary-sort-by-importance","title":"Decision: Primary Sort by Importance","text":"<p>Rationale: Importance is the stronger signal for retention</p> <p>Alternative: Primary sort by recency</p> <p>Rejected: Would evict critical old memories before trivial recent ones</p>"},{"location":"architecture/adrs/007-eviction-strategy/#decision-greedy-eviction-stop-when-enough-space","title":"Decision: Greedy Eviction (Stop When Enough Space)","text":"<p>Rationale: Minimize evictions, preserve as much context as possible</p> <p>Alternative: Evict to some threshold (e.g., 80% full)</p> <p>Rejected: Unnecessary evictions, reduces available context</p>"},{"location":"architecture/adrs/007-eviction-strategy/#decision-no-access-frequency-tracking","title":"Decision: No Access Frequency Tracking","text":"<p>Rationale: Simplicity, access patterns not stable in LLM workflows</p> <p>Alternative: LFU (Least Frequently Used)</p> <p>Deferred: Can add access_count tracking if real-world usage shows value</p>"},{"location":"architecture/adrs/007-eviction-strategy/#decision-evicted-memories-preserved-in-long-term","title":"Decision: Evicted Memories Preserved in Long-Term","text":"<p>Rationale: Never-forget philosophy, safety, recallability</p> <p>Alternative: Truly delete evicted memories</p> <p>Rejected: Violates never-forget principle, data loss</p>"},{"location":"architecture/adrs/007-eviction-strategy/#risks-and-mitigations","title":"Risks and Mitigations","text":""},{"location":"architecture/adrs/007-eviction-strategy/#risk-batch-eviction-cost","title":"Risk: Batch Eviction Cost","text":"<p>Risk</p> <p>O(n log n) sort on every eviction is expensive</p> <p>Likelihood: Low (evictions infrequent, n is small)</p> <p>Impact: Low (&lt; 10ms for typical working memory)</p> <p>Mitigation:</p> <ul> <li>Working memory stays small (128K tokens \u2248 50-200 nodes)</li> <li>Sort only when eviction needed</li> <li>Consider heap-based eviction if profiling shows bottleneck</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#risk-importance-scoring-inconsistency","title":"Risk: Importance Scoring Inconsistency","text":"<p>Risk</p> <p>Users assign arbitrary importance, breaks eviction quality</p> <p>Likelihood: Medium (subjective scoring)</p> <p>Impact: Medium (suboptimal evictions)</p> <p>Mitigation:</p> <ul> <li>Document importance scoring guidelines</li> <li>Provide examples of importance ranges</li> <li>Default importance: 1.0 for most memories</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#risk-no-access-frequency-signal","title":"Risk: No Access Frequency Signal","text":"<p>Risk</p> <p>Frequently accessed memories evicted if old and low importance</p> <p>Likelihood: Low (frequently accessed \u2192 likely important or recent)</p> <p>Impact: Low (can recall from long-term)</p> <p>Mitigation:</p> <ul> <li>Monitor real-world eviction patterns</li> <li>Add access_count tracking if needed</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#risk-related-memories-split","title":"Risk: Related Memories Split","text":"<p>Risk</p> <p>Semantically related memories evicted separately</p> <p>Likelihood: Medium (no clustering)</p> <p>Impact: Low (can recall together with topic search)</p> <p>Mitigation:</p> <ul> <li>Use relationships to co-recall related memories</li> <li>Consider cluster-aware eviction in future</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#future-enhancements","title":"Future Enhancements","text":""},{"location":"architecture/adrs/007-eviction-strategy/#access-frequency-tracking","title":"Access Frequency Tracking","text":"<pre><code>def add(key, value, ...)\n  @nodes[key] = {\n    ...,\n    access_count: 0\n  }\nend\n\ndef evict_to_make_space(needed_tokens)\n  candidates = @nodes.sort_by do |key, node|\n    recency = Time.now - node[:added_at]\n    access_freq = node[:access_count]\n\n    # Lower score evicted first\n    score = node[:importance] * (1 + Math.log(1 + access_freq))\n    [score, -recency]\n  end\nend\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#cluster-aware-eviction","title":"Cluster-Aware Eviction","text":"<pre><code># Keep related memories together\ndef evict_to_make_space(needed_tokens)\n  # Identify memory clusters\n  clusters = identify_clusters(@nodes)\n\n  # Evict entire clusters to preserve coherence\n  clusters.sort_by { |c| cluster_score(c) }.each do |cluster|\n    # Evict full cluster\n  end\nend\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#lazy-eviction","title":"Lazy Eviction","text":"<pre><code># Don't evict until actually assembling context\ndef assemble_context(strategy:, max_tokens:)\n  # Only NOW evict if needed\n  evict_to_fit(max_tokens) if token_count &gt; max_tokens\nend\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#predicted-need-scoring","title":"Predicted Need Scoring","text":"<pre><code># Use LLM or heuristics to predict which memories will be needed\ndef evict_to_make_space(needed_tokens)\n  candidates = @nodes.sort_by do |key, node|\n    predicted_need = predict_future_access(node)  # ML model or heuristics\n    [node[:importance] * predicted_need, -recency]\n  end\nend\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#configurable-eviction-policy","title":"Configurable Eviction Policy","text":"<pre><code>htm = HTM.new(\n  eviction_policy: :lru  # or :importance, :hybrid, :custom\n)\n</code></pre>"},{"location":"architecture/adrs/007-eviction-strategy/#alternatives-comparison","title":"Alternatives Comparison","text":"Policy Pros Cons Decision Hybrid (Importance + Recency) Balanced retention Requires importance ACCEPTED Pure LRU Standard caching, temporal Loses important context Rejected Pure Importance Preserves critical info No temporal relevance Rejected LFU Captures access patterns New memories always evicted Rejected Random Simple, no sorting Unpredictable, risky Rejected Learned Policy Optimal for patterns Complex, non-deterministic Deferred"},{"location":"architecture/adrs/007-eviction-strategy/#references","title":"References","text":"<ul> <li>Cache Replacement Policies</li> <li>LRU Cache</li> <li>Working Memory Management</li> <li>Multi-Level Memory Hierarchies</li> <li>ADR-002: Two-Tier Memory</li> <li>ADR-006: Context Assembly</li> <li>ADR-009: Never-Forget Philosophy</li> <li>Working Memory Guide</li> </ul>"},{"location":"architecture/adrs/007-eviction-strategy/#review-notes","title":"Review Notes","text":"<p>Systems Architect: Hybrid eviction is the right choice. Consider heap-based eviction for better asymptotic complexity.</p> <p>Performance Specialist: O(n log n) is acceptable for n &lt; 200. Monitor real-world eviction frequency.</p> <p>Domain Expert: Never-forget guarantee is essential. Eviction is just working memory management.</p> <p>AI Engineer: Importance + recency works well for LLM context. Consider learned eviction in future.</p> <p>Ruby Expert: Clean implementation. Consider extracting eviction policy to strategy pattern for extensibility.</p>"},{"location":"architecture/adrs/008-robot-identification/","title":"ADR-008: Robot Identification System","text":"<p>Status: Accepted</p> <p>Date: 2025-10-25</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p>"},{"location":"architecture/adrs/008-robot-identification/#quick-summary","title":"Quick Summary","text":"<p>HTM uses a dual-identifier system with mandatory unique <code>robot_id</code> (UUID) and optional human-readable <code>robot_name</code>. Both are auto-generated if not provided, with automatic robot registration and activity tracking in the database.</p> <p>Why: Unique robot identification enables memory attribution, cross-robot analytics, debugging, and future features like permissions and groups while maintaining developer-friendly names.</p> <p>Impact: Globally unique identity with human-readable names and automatic management, at the cost of dual identifiers and manual persistence requirements for stable identities.</p>"},{"location":"architecture/adrs/008-robot-identification/#context","title":"Context","text":"<p>In the HTM hive mind architecture (ADR-004), multiple robots share a global memory database. Each robot needs a unique identity to:</p> <ul> <li>Attribute memories to their creator</li> <li>Track robot activity over time</li> <li>Enable queries like \"which robot said this?\"</li> <li>Debug conversation attribution</li> <li>Support future features (privacy, analytics, collaboration)</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#identity-requirements","title":"Identity Requirements","text":"<ul> <li>Uniqueness: Must be globally unique (no collisions)</li> <li>Persistence: Should remain stable across sessions</li> <li>Human-readable: Developers need to identify robots easily</li> <li>Automation: Should auto-generate if not provided</li> <li>Registration: Track all robots using the system</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#alternative-identification-schemes","title":"Alternative Identification Schemes","text":"<ol> <li>Auto-generated UUID only: Unique but not human-readable</li> <li>User-provided name only: Readable but collision-prone</li> <li>UUID + optional name: Unique ID with optional readable name</li> <li>Sequential ID: Simple but requires coordination</li> <li>Hostname + PID: Automatic but not persistent across restarts</li> </ol>"},{"location":"architecture/adrs/008-robot-identification/#decision","title":"Decision","text":"<p>We will use a dual-identifier system: mandatory unique <code>robot_id</code> (UUID) + optional human-readable <code>robot_name</code>, with automatic generation if not provided.</p>"},{"location":"architecture/adrs/008-robot-identification/#robot-identification-components","title":"Robot Identification Components","text":"<p>1. Robot ID (<code>robot_id</code>)</p> <ul> <li>Type: UUID v4 (RFC 4122)</li> <li>Format: <code>\"f47ac10b-58cc-4372-a567-0e02b2c3d479\"</code></li> <li>Generation: <code>SecureRandom.uuid</code> if not provided</li> <li>Persistence: User-provided for stability, or auto-generated per session</li> <li>Usage: Primary key, foreign key references, attribution</li> </ul> <p>2. Robot Name (<code>robot_name</code>)</p> <ul> <li>Type: String (optional, human-readable)</li> <li>Format: Any descriptive string (e.g., \"Code Helper\", \"Research Assistant\")</li> <li>Generation: <code>\"robot_#{robot_id[0..7]}\"</code> if not provided</li> <li>Persistence: Stored in database, updatable</li> <li>Usage: Display, debugging, user interfaces</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/008-robot-identification/#why-uuid-name","title":"Why UUID + Name?","text":"<p>UUID provides guarantees:</p> <ul> <li>Globally unique (collision probability: ~10^-36)</li> <li>No coordination required (decentralized generation)</li> <li>Cryptographically random (unpredictable)</li> <li>Standard format (RFC 4122, widely supported)</li> </ul> <p>Name provides usability:</p> <ul> <li>Human-readable in logs and debugging</li> <li>Meaningful for user-facing features</li> <li>Easy to remember (\"Code Helper\" vs UUID)</li> <li>Updatable without breaking references</li> </ul> <p>Combination is best:</p> <ul> <li>UUID for database integrity</li> <li>Name for developer experience</li> <li>Auto-generation for convenience</li> <li>User control for persistence</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#why-auto-generate","title":"Why Auto-Generate?","text":"<p>Convenience:</p> <ul> <li>Users don't need to manage UUIDs manually</li> <li>Works out-of-box with no configuration</li> <li>Still allows explicit robot_id for persistence</li> </ul> <p>Session vs Persistent Identity:</p> <ul> <li>Session identity: Auto-generated UUID, ephemeral robot</li> <li>Persistent identity: User-provided UUID, stable across restarts</li> </ul> <pre><code># Ephemeral robot (new UUID every session)\nhtm = HTM.new(robot_name: \"Temp Helper\")\n\n# Persistent robot (same UUID across sessions)\nROBOT_ID = \"f47ac10b-58cc-4372-a567-0e02b2c3d479\"\nhtm = HTM.new(robot_id: ROBOT_ID, robot_name: \"Persistent Helper\")\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#why-register-robots","title":"Why Register Robots?","text":"<p>Activity tracking:</p> <ul> <li>Know which robots are active</li> <li>Monitor robot usage patterns</li> <li>Identify inactive robots for cleanup</li> </ul> <p>Metadata extensibility:</p> <ul> <li>Future: Robot roles, permissions, preferences</li> <li>Future: Robot groups/teams</li> <li>Future: Configuration per robot</li> </ul> <p>Debugging:</p> <ul> <li>\"Which robot created this memory?\"</li> <li>\"What has this robot been doing?\"</li> <li>\"When was this robot last active?\"</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adrs/008-robot-identification/#robot-registry","title":"Robot Registry","text":"<pre><code>CREATE TABLE robots (\n  id TEXT PRIMARY KEY,              -- robot_id (UUID)\n  name TEXT,                         -- robot_name (human-readable)\n  created_at TIMESTAMP DEFAULT NOW(),\n  last_active TIMESTAMP DEFAULT NOW(),\n  metadata JSONB                     -- future extensibility\n);\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#robot-attribution","title":"Robot Attribution","text":"<p>Every memory node stores <code>robot_id</code>:</p> <pre><code>CREATE TABLE nodes (\n  id SERIAL PRIMARY KEY,\n  key TEXT UNIQUE NOT NULL,\n  value TEXT NOT NULL,\n  robot_id TEXT NOT NULL REFERENCES robots(id),  -- Attribution\n  ...\n);\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#initialization","title":"Initialization","text":"<pre><code>htm = HTM.new(\n  robot_id: \"f47ac10b-58cc-4372-a567-0e02b2c3d479\",  # optional, auto-generated\n  robot_name: \"Code Helper\"                          # optional, descriptive\n)\n\n# Auto-generated example:\n# robot_id: \"3a7b2c4d-8e9f-4a5b-9c8d-7e6f5a4b3c2d\"\n# robot_name: \"robot_3a7b2c4d\"\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#registration-logic","title":"Registration Logic","text":"<p>Upsert semantics:</p> <pre><code>INSERT INTO robots (id, name)\nVALUES ($1, $2)\nON CONFLICT (id) DO UPDATE\nSET name = $2, last_active = CURRENT_TIMESTAMP\n</code></pre> <ul> <li>Automatic on first HTM initialization</li> <li>Updates name and last_active if robot_id exists</li> <li>Activity tracking: <code>last_active</code> updated on every operation</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/008-robot-identification/#positive","title":"Positive","text":"<ul> <li>Uniqueness guaranteed: UUID collision-proof</li> <li>Human-readable: Names easy to identify in logs</li> <li>Auto-generation: Works without manual configuration</li> <li>Persistence option: User can provide stable robot_id</li> <li>Attribution tracking: Every memory linked to creator</li> <li>Activity monitoring: Track robot usage over time</li> <li>Future-proof: Metadata field for extensibility</li> <li>Standard format: UUID is widely recognized</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#negative","title":"Negative","text":"<ul> <li>Dual identifiers: Two fields instead of one (complexity)</li> <li>Name collisions: Names not unique (only IDs are)</li> <li>Manual persistence: User must manage robot_id for stability</li> <li>No automatic migration: Robot ID changes break historical attribution</li> <li>UUID verbosity: UUIDs are long (36 characters)</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#neutral","title":"Neutral","text":"<ul> <li>Session vs persistent: User chooses ephemeral or stable identity</li> <li>Name mutability: Names can be updated, IDs cannot</li> <li>Registry cleanup: Inactive robots accumulate (manual cleanup needed)</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#use-cases","title":"Use Cases","text":""},{"location":"architecture/adrs/008-robot-identification/#use-case-1-ephemeral-robot-session-identity","title":"Use Case 1: Ephemeral Robot (Session Identity)","text":"<pre><code># New UUID generated every time\nhtm = HTM.new(robot_name: \"Quick Helper\")\n\n# robot_id: auto-generated UUID\n# robot_name: \"Quick Helper\"\n\n# Next session: different robot_id, same name\n# Memories attributed to different robot each time\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#use-case-2-persistent-robot-stable-identity","title":"Use Case 2: Persistent Robot (Stable Identity)","text":"<pre><code># Store robot_id in config or environment\nROBOT_ID = ENV['ROBOT_ID'] || \"f47ac10b-58cc-4372-a567-0e02b2c3d479\"\n\nhtm = HTM.new(\n  robot_id: ROBOT_ID,\n  robot_name: \"Code Helper\"\n)\n\n# Same robot_id across sessions\n# Memories consistently attributed to this robot\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#use-case-3-multiple-robots-in-same-process","title":"Use Case 3: Multiple Robots in Same Process","text":"<pre><code># Research assistant\nresearch_bot = HTM.new(\n  robot_id: \"research-001\",\n  robot_name: \"Research Assistant\"\n)\n\n# Code helper\ncode_bot = HTM.new(\n  robot_id: \"code-001\",\n  robot_name: \"Code Helper\"\n)\n\n# Each robot has own working memory\n# Both share same long-term memory database\n# Memories attributed to respective robots\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#use-case-4-robot-activity-analysis","title":"Use Case 4: Robot Activity Analysis","text":"<pre><code>-- Which robots have been active?\nSELECT id, name, last_active\nFROM robots\nORDER BY last_active DESC;\n\n-- Which robot contributed most memories?\nSELECT robot_id, COUNT(*) as memory_count\nFROM nodes\nGROUP BY robot_id\nORDER BY memory_count DESC;\n\n-- What has \"Code Helper\" been doing?\nSELECT operation, created_at, details\nFROM operations_log\nWHERE robot_id = (SELECT id FROM robots WHERE name = 'Code Helper')\nORDER BY created_at DESC\nLIMIT 50;\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#use-case-5-conversation-attribution","title":"Use Case 5: Conversation Attribution","text":"<pre><code># Which robot discussed PostgreSQL?\nbreakdown = htm.which_robot_said(\"PostgreSQL\")\n# =&gt; { \"f47ac10b-...\" =&gt; 15, \"3a7b2c4d-...\" =&gt; 8 }\n\n# Get robot names\nrobot_names = breakdown.keys.map do |robot_id|\n  db.query(\"SELECT name FROM robots WHERE id = $1\", [robot_id]).first\nend\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#robot-identity-lifecycle","title":"Robot Identity Lifecycle","text":""},{"location":"architecture/adrs/008-robot-identification/#1-creation","title":"1. Creation","text":"<pre><code>htm = HTM.new(robot_id: uuid, robot_name: name)\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#2-registration","title":"2. Registration","text":"<pre><code>INSERT INTO robots (id, name, created_at, last_active)\nVALUES (uuid, name, NOW(), NOW())\nON CONFLICT (id) DO UPDATE SET name = name, last_active = NOW()\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#3-activity-tracking","title":"3. Activity Tracking","text":"<pre><code># On every HTM operation (add_node, recall, forget, retrieve)\n@long_term_memory.update_robot_activity(@robot_id)\n</code></pre> <pre><code>UPDATE robots\nSET last_active = CURRENT_TIMESTAMP\nWHERE id = robot_id\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#4-attribution","title":"4. Attribution","text":"<pre><code># Every node stores robot_id\nnode_id = @long_term_memory.add(\n  key: key,\n  value: value,\n  robot_id: @robot_id,  # Attribution\n  ...\n)\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#5-querying","title":"5. Querying","text":"<pre><code>-- Find memories by robot\nSELECT * FROM nodes WHERE robot_id = 'f47ac10b-...'\n\n-- Find robot by name\nSELECT * FROM robots WHERE name = 'Code Helper'\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#6-cleanup-manual","title":"6. Cleanup (Manual)","text":"<pre><code>-- Find inactive robots\nSELECT * FROM robots WHERE last_active &lt; NOW() - INTERVAL '30 days';\n\n-- Delete robot and all its memories (use with caution!)\nDELETE FROM nodes WHERE robot_id = 'f47ac10b-...';\nDELETE FROM robots WHERE id = 'f47ac10b-...';\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/adrs/008-robot-identification/#uuid-generation","title":"UUID Generation","text":"<ul> <li>Time: &lt; 1ms (SecureRandom.uuid)</li> <li>Collision probability: ~10^-36 for v4 UUID</li> <li>Entropy: 122 random bits</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#robot-registration","title":"Robot Registration","text":"<ul> <li>Upsert query: &lt; 5ms</li> <li>Index: Primary key on <code>robots.id</code></li> <li>Frequency: Once per HTM initialization</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#activity-tracking","title":"Activity Tracking","text":"<ul> <li>Update query: &lt; 2ms</li> <li>Frequency: Every HTM operation</li> <li>Index: Primary key on <code>robots.id</code></li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#attribution-queries","title":"Attribution Queries","text":"<ul> <li>Foreign key join: O(log n) with index</li> <li>Index: <code>nodes.robot_id</code> indexed as foreign key</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/adrs/008-robot-identification/#decision-uuid-v4-random-instead-of-uuid-v1-timestamp","title":"Decision: UUID v4 (Random) Instead of UUID v1 (Timestamp)","text":"<p>Rationale:</p> <ul> <li>No MAC address leakage (privacy)</li> <li>Cryptographically random (security)</li> <li>No clock synchronization needed</li> </ul> <p>Alternative: UUID v1 (timestamp-based)</p> <p>Rejected: MAC address exposure, clock sync issues</p>"},{"location":"architecture/adrs/008-robot-identification/#decision-optional-robot_id-mandatory-robot_name","title":"Decision: Optional robot_id, Mandatory robot_name","text":"<p>Rationale: Auto-generate both if not provided, allow user override</p> <p>Alternative: Require user to provide robot_id</p> <p>Rejected: Too much friction, poor DX</p>"},{"location":"architecture/adrs/008-robot-identification/#decision-auto-generated-name-format","title":"Decision: Auto-Generated Name Format","text":"<p>Rationale: <code>\"robot_#{uuid[0..7]}\"</code> provides:</p> <ul> <li>Uniqueness (first 8 chars usually unique)</li> <li>Traceability (prefix of robot_id)</li> <li>Consistency (predictable format)</li> </ul> <p>Alternative: Random adjective + noun (\"Happy Robot\")</p> <p>Rejected: Harder to correlate with robot_id</p>"},{"location":"architecture/adrs/008-robot-identification/#decision-upsert-semantics-for-registration","title":"Decision: Upsert Semantics for Registration","text":"<p>Rationale: Allows robot_name updates, prevents duplicate registration errors</p> <p>Alternative: Strict insert-only (error on duplicate)</p> <p>Rejected: Prevents name updates, complicates initialization</p>"},{"location":"architecture/adrs/008-robot-identification/#decision-jsonb-metadata-field","title":"Decision: JSONB Metadata Field","text":"<p>Rationale: Future extensibility without schema migrations</p> <p>Alternative: Add columns as needed</p> <p>Deferred: JSONB is flexible, add columns for indexed fields later</p>"},{"location":"architecture/adrs/008-robot-identification/#risks-and-mitigations","title":"Risks and Mitigations","text":""},{"location":"architecture/adrs/008-robot-identification/#risk-robot-id-changes-break-attribution","title":"Risk: Robot ID Changes Break Attribution","text":"<p>Risk</p> <p>User changes robot_id, breaks historical attribution</p> <p>Likelihood: Low (user must explicitly change)</p> <p>Impact: High (attribution lost)</p> <p>Mitigation:</p> <ul> <li>Document robot_id persistence clearly</li> <li>Recommend storing robot_id in config</li> <li>Consider robot_id migration tool (future)</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#risk-name-collisions","title":"Risk: Name Collisions","text":"<p>Risk</p> <p>Multiple robots with same name</p> <p>Likelihood: Medium (names not enforced unique)</p> <p>Impact: Low (IDs are unique, names just for display)</p> <p>Mitigation:</p> <ul> <li>Document that names are not unique</li> <li>Use robot_id for queries, name for display</li> <li>Consider unique constraint on name (future)</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#risk-robot-registry-growth","title":"Risk: Robot Registry Growth","text":"<p>Risk</p> <p>Inactive robots accumulate indefinitely</p> <p>Likelihood: High (no automatic cleanup)</p> <p>Impact: Low (storage, query slowdown)</p> <p>Mitigation:</p> <ul> <li>Document cleanup procedures</li> <li>Provide cleanup utilities (future)</li> <li>Monitor robot registry size</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#risk-uuid-verbosity","title":"Risk: UUID Verbosity","text":"<p>Risk</p> <p>UUIDs are long (36 chars) in logs</p> <p>Likelihood: High (by design)</p> <p>Impact: Low (readability in logs)</p> <p>Mitigation:</p> <ul> <li>Use robot_name for logging</li> <li>Truncate UUID for display: <code>robot_id[0..7]</code></li> <li>Full UUID available for debugging</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#future-enhancements","title":"Future Enhancements","text":""},{"location":"architecture/adrs/008-robot-identification/#robot-groupsteams","title":"Robot Groups/Teams","text":"<pre><code># Assign robots to teams\nrobot_team = db.exec(\"INSERT INTO robot_teams (robot_id, team_name) VALUES ($1, $2)\",\n                     [robot_id, \"coding-team\"])\n\n# Query by team\nmemories = htm.recall(robot_team: \"coding-team\", topic: \"APIs\")\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#robot-permissions","title":"Robot Permissions","text":"<pre><code># Role-based access control\nrobot_role = db.exec(\"INSERT INTO robot_roles (robot_id, role) VALUES ($1, $2)\",\n                     [robot_id, \"admin\"])\n\n# Restrict operations by role\nhtm.forget(key, confirm: :confirmed)  # Requires admin role\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#robot-configuration","title":"Robot Configuration","text":"<pre><code># Per-robot settings\nhtm = HTM.new(\n  robot_id: uuid,\n  robot_name: name,\n  robot_config: {\n    embedding_model: \"custom-model\",\n    working_memory_size: 256_000,\n    preferences: { language: \"en\" }\n  }\n)\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#robot-migration-tool","title":"Robot Migration Tool","text":"<pre><code># Migrate memories from old robot_id to new one\nHTM::Migration.migrate_robot(\n  from: \"old-robot-id\",\n  to: \"new-robot-id\"\n)\n\n# Updates all nodes.robot_id references\n# Merges robot registry entries\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#short-robot-ids","title":"Short Robot IDs","text":"<pre><code># Generate shorter IDs (like GitHub: 7 chars)\nshort_id = SecureRandom.hex(4)  # \"3a7b2c4d\"\n\nhtm = HTM.new(\n  robot_id: short_id,\n  robot_name: \"Helper\"\n)\n\n# Trade-off: Lower collision resistance, better readability\n</code></pre>"},{"location":"architecture/adrs/008-robot-identification/#alternatives-comparison","title":"Alternatives Comparison","text":"Approach Pros Cons Decision UUID + Name Unique + readable Dual identifiers ACCEPTED UUID Only Simple, guaranteed unique Not human-readable Rejected Name Only Simple, readable Collision-prone Rejected Sequential IDs Short, sortable Requires coordination Rejected Hostname + PID Automatic Not persistent Rejected ULID Sortable, shorter Less standard Deferred"},{"location":"architecture/adrs/008-robot-identification/#references","title":"References","text":"<ul> <li>RFC 4122: UUID Specification</li> <li>UUID v4 (Random)</li> <li>ULID Specification</li> <li>Robot Registry Pattern</li> <li>ADR-004: Hive Mind Architecture</li> <li>Multi-Robot Guide</li> </ul>"},{"location":"architecture/adrs/008-robot-identification/#review-notes","title":"Review Notes","text":"<p>Systems Architect: UUID + name is the right balance. Consider ULID for future versions.</p> <p>Database Architect: Foreign key on robot_id is correct. Index on <code>nodes.robot_id</code> for attribution queries.</p> <p>Ruby Expert: SecureRandom.uuid is standard. Consider <code>robot_id: SecureRandom.uuid</code> as default parameter.</p> <p>Security Specialist: UUID v4 is cryptographically secure. No MAC address leakage.</p> <p>Domain Expert: Auto-generation + manual override gives flexibility. Document persistence clearly.</p>"},{"location":"architecture/adrs/009-never-forget/","title":"ADR-009: Never-Forget Philosophy with Explicit Deletion","text":"<p>Status: Accepted</p> <p>Date: 2025-10-25</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p>"},{"location":"architecture/adrs/009-never-forget/#quick-summary","title":"Quick Summary","text":"<p>HTM implements a never-forget philosophy where memories are never automatically deleted. Eviction only moves memories from working to long-term storage. Deletion requires explicit confirmation (<code>:confirmed</code> symbol) and is permanently logged for audit trails.</p> <p>Why: LLMs need persistent, never-forgetting memory for long-term context. Automatic deletion causes surprise, debugging difficulties, and lost knowledge.</p> <p>Impact: Predictable behavior with complete data preservation, at the cost of unbounded storage growth and manual cleanup responsibility.</p>"},{"location":"architecture/adrs/009-never-forget/#context","title":"Context","text":"<p>Traditional memory systems for LLMs face a critical design decision: when should memories be deleted?</p>"},{"location":"architecture/adrs/009-never-forget/#alternative-approaches","title":"Alternative Approaches","text":"<ol> <li>Automatic deletion: LRU cache eviction, TTL expiration, capacity limits</li> <li>Never delete: Unlimited growth, storage costs, degraded performance</li> <li>Manual deletion: User explicitly deletes memories</li> <li>Hybrid: Automatic archival + manual deletion for permanent removal</li> </ol>"},{"location":"architecture/adrs/009-never-forget/#key-challenges","title":"Key Challenges","text":"<ul> <li>LLM context loss: Deleting memories loses valuable knowledge</li> <li>User surprise: Automatic deletion feels like \"forgetting\" without consent</li> <li>Debugging: Hard to debug if memories disappear automatically</li> <li>Storage costs: Unlimited storage is expensive</li> <li>Performance: Large datasets slow down queries</li> </ul> <p>HTM's core purpose is to provide persistent, never-forgetting memory for LLM robots. The philosophy: \"never forget unless explicitly told.\"</p>"},{"location":"architecture/adrs/009-never-forget/#decision","title":"Decision","text":"<p>We will implement a never-forget philosophy where:</p> <ol> <li>Memories are never automatically deleted</li> <li>Eviction only moves memories from working to long-term storage</li> <li>Deletion requires explicit user confirmation</li> <li>Confirmation must be <code>:confirmed</code> symbol to prevent accidental deletion</li> <li>All deletions are logged for audit trail</li> </ol>"},{"location":"architecture/adrs/009-never-forget/#deletion-api","title":"Deletion API","text":"<pre><code># Attempting to delete without confirmation raises error\nhtm.forget(\"key_to_delete\")\n# =&gt; ArgumentError: Must pass confirm: :confirmed to delete\n\n# Explicit confirmation required\nhtm.forget(\"key_to_delete\", confirm: :confirmed)\n# =&gt; true (deleted successfully)\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#eviction-vs-deletion","title":"Eviction vs Deletion","text":"<p>Critical Distinction</p> <p>Eviction (automatic, safe):</p> <ul> <li>Triggered by working memory capacity limit</li> <li>Moves memories from working memory to long-term memory</li> <li>NO data loss, memories remain recallable</li> <li>Logged as 'evict' operation</li> </ul> <p>Deletion (explicit, destructive):</p> <ul> <li>Triggered only by user calling <code>forget(confirm: :confirmed)</code></li> <li>Removes memory from both working and long-term storage</li> <li>PERMANENT data loss</li> <li>Logged as 'forget' operation</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/009-never-forget/#why-never-forget","title":"Why Never-Forget?","text":"<p>LLMs need long-term context:</p> <ul> <li>Architectural decisions made months ago still matter</li> <li>User preferences should persist across sessions</li> <li>Bug fixes and resolutions are valuable knowledge</li> <li>Conversation history builds understanding over time</li> </ul> <p>Automatic deletion causes problems:</p> <ul> <li>Surprise: User asks \"didn't we discuss this?\" \u2192 memory gone</li> <li>Debugging: Can't debug deleted memories</li> <li>Inconsistency: Same query returns different results over time</li> <li>Lost knowledge: Critical information disappears silently</li> </ul> <p>Two-tier architecture enables never-forget:</p> <ul> <li>Working memory: Token-limited, evicts to long-term</li> <li>Long-term memory: Unlimited, persistent PostgreSQL</li> <li>Eviction \u2260 deletion, just moves to cold storage</li> <li>Recall brings memories back to working memory</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#why-explicit-confirmation","title":"Why Explicit Confirmation?","text":"<p>Prevent accidental deletion:</p> <pre><code># Easy typo or mistake\nhtm.forget(\"important_key\")  # REJECTED - raises error\n\n# Must be intentional\nhtm.forget(\"important_key\", confirm: :confirmed)  # Allowed\n</code></pre> <p>Confirmation is a speed bump:</p> <ul> <li>Forces user to think before deleting</li> <li>Symbol <code>:confirmed</code> (not boolean) prevents <code>confirm: true</code> shortcuts</li> <li>Clear intent signal in code review</li> </ul> <p>Audit trail for safety:</p> <ul> <li>All deletions logged with robot_id and timestamp</li> <li>Can investigate \"who deleted this?\"</li> <li>Provides recovery information (log has the deleted value)</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#why-log-before-deleting","title":"Why Log Before Deleting?","text":"<p>Foreign key constraint safety:</p> <pre><code># Log operation BEFORE deleting\n@long_term_memory.log_operation(\n  operation: 'forget',\n  node_id: node_id,  # Still exists\n  robot_id: @robot_id,\n  details: { key: key }\n)\n\n# Now safe to delete\n@long_term_memory.delete(key)\n</code></pre> <p>Audit trail preservation:</p> <ul> <li>Deletion log entry survives even if something goes wrong</li> <li>Can reconstruct what was deleted and when</li> <li>Supports future \"undo delete\" feature</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/adrs/009-never-forget/#forget-method","title":"Forget Method","text":"<pre><code>def forget(key, confirm: false)\n  raise ArgumentError, \"Must pass confirm: :confirmed to delete\" unless confirm == :confirmed\n\n  node_id = @long_term_memory.get_node_id(key)\n\n  # Log operation BEFORE deleting (audit trail)\n  @long_term_memory.log_operation(\n    operation: 'forget',\n    node_id: node_id,\n    robot_id: @robot_id,\n    details: { key: key }\n  )\n\n  # Delete from long-term memory and working memory\n  @long_term_memory.delete(key)\n  @working_memory.remove(key)\n\n  update_robot_activity\n  true\nend\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/009-never-forget/#positive","title":"Positive","text":"<ul> <li>Never lose knowledge: Memories persist unless explicitly deleted</li> <li>Predictable behavior: No surprise deletions, no data loss</li> <li>Debugging friendly: All memories available for analysis</li> <li>Audit trail: Every deletion logged with who/when/what</li> <li>Safe eviction: Working memory overflow doesn't lose data</li> <li>Recallable: Evicted memories return via recall()</li> <li>Intentional deletion: Confirmation prevents accidents</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#negative","title":"Negative","text":"<ul> <li>Unbounded growth: Database grows indefinitely without cleanup</li> <li>Storage costs: Long-term storage has financial cost</li> <li>Query performance: Larger datasets slow down searches</li> <li>Manual cleanup: User must periodically delete unneeded memories</li> <li>No automatic expiration: Can't set TTL for temporary memories</li> <li>Privacy concerns: Sensitive data persists until deleted</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#neutral","title":"Neutral","text":"<ul> <li>User responsibility: User must manage memory lifecycle</li> <li>Explicit is better: Pythonic philosophy, clear intent</li> <li>Retention policies: Future feature, not v1</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#use-cases","title":"Use Cases","text":""},{"location":"architecture/adrs/009-never-forget/#use-case-1-accidental-deletion-attempt","title":"Use Case 1: Accidental Deletion Attempt","text":"<pre><code># User typo or mistake\nhtm.forget(\"important_decision\")\n\n# Result: ArgumentError raised\n# =&gt; ArgumentError: Must pass confirm: :confirmed to delete\n\n# Memory remains safe\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#use-case-2-intentional-deletion","title":"Use Case 2: Intentional Deletion","text":"<pre><code># User wants to delete temporary test data\nhtm.add_node(\"test_key\", \"temporary test data\", importance: 1.0)\n\n# Later: delete intentionally\nhtm.forget(\"test_key\", confirm: :confirmed)\n# =&gt; true (deleted)\n\n# Deletion logged for audit trail\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#use-case-3-eviction-not-deletion","title":"Use Case 3: Eviction (Not Deletion)","text":"<pre><code># Working memory full (128,000 tokens)\n# Add large new memory (10,000 tokens)\n\nhtm.add_node(\"new_large_memory\", large_text, importance: 7.0)\n\n# Result: HTM evicts low-importance memories to make space\n# Evicted memories moved to long-term storage (NOT deleted)\n# Can be recalled later:\n\nmemories = htm.recall(timeframe: \"last month\", topic: \"evicted topic\")\n# =&gt; Evicted memories returned\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#use-case-4-audit-trail-query","title":"Use Case 4: Audit Trail Query","text":"<pre><code>-- Who deleted this memory?\nSELECT robot_id, created_at, details\nFROM operations_log\nWHERE operation = 'forget'\nAND details-&gt;&gt;'key' = 'important_key'\n\n-- Result:\n-- robot_id: \"f47ac10b-...\"\n-- created_at: 2025-10-25 14:32:15\n-- details: {\"key\": \"important_key\"}\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#use-case-5-bulk-cleanup-manual","title":"Use Case 5: Bulk Cleanup (Manual)","text":"<pre><code># User wants to clean up old test data\ntest_keys = [\n  \"test_001\",\n  \"test_002\",\n  \"test_003\"\n]\n\ntest_keys.each do |key|\n  htm.forget(key, confirm: :confirmed)\nend\n\n# All deletions logged individually\n# User must explicitly confirm each deletion\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#use-case-6-never-forget-in-practice","title":"Use Case 6: Never-Forget in Practice","text":"<pre><code># Session 1: Important decision\nhtm.add_node(\"decision_001\",\n             \"We decided to use PostgreSQL for HTM storage\",\n             type: :decision,\n             importance: 10.0)\n\n# ... 90 days later, many sessions, many memories added ...\n# Working memory evicted this decision to long-term storage\n\n# Session 100: User asks about database choice\nmemories = htm.recall(timeframe: \"last 3 months\", topic: \"database storage\")\n\n# Result: Decision recalled from long-term memory\n# Never forgotten, always available\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#deletion-lifecycle","title":"Deletion Lifecycle","text":""},{"location":"architecture/adrs/009-never-forget/#1-user-initiates-deletion","title":"1. User Initiates Deletion","text":"<pre><code>htm.forget(\"key_to_delete\", confirm: :confirmed)\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#2-validation","title":"2. Validation","text":"<pre><code>raise ArgumentError, \"Must pass confirm: :confirmed to delete\" unless confirm == :confirmed\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#3-retrieve-node-id","title":"3. Retrieve Node ID","text":"<pre><code>node_id = @long_term_memory.get_node_id(\"key_to_delete\")\n# =&gt; 42\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#4-log-operation-before-deletion","title":"4. Log Operation (Before Deletion)","text":"<pre><code>@long_term_memory.log_operation(\n  operation: 'forget',\n  node_id: 42,  # Still exists at this point\n  robot_id: @robot_id,\n  details: { key: \"key_to_delete\" }\n)\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#5-delete-from-long-term-memory","title":"5. Delete from Long-Term Memory","text":"<pre><code>DELETE FROM nodes WHERE key = 'key_to_delete'\n\n-- Cascades to:\n-- - relationships (foreign key cascade)\n-- - tags (foreign key cascade)\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#6-remove-from-working-memory","title":"6. Remove from Working Memory","text":"<pre><code>@working_memory.remove(\"key_to_delete\")\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#7-update-robot-activity","title":"7. Update Robot Activity","text":"<pre><code>@long_term_memory.update_robot_activity(@robot_id)\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#8-return-success","title":"8. Return Success","text":"<pre><code>return true\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/adrs/009-never-forget/#deletion-performance","title":"Deletion Performance","text":"<ul> <li>Node ID lookup: O(log n) with index on key</li> <li>Log operation: O(1) insert</li> <li>Delete query: O(1) with primary key</li> <li>Cascade deletes: O(m) where m = related records</li> <li>Working memory remove: O(1) hash delete</li> <li>Total: &lt; 10ms for typical deletion</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#audit-log-growth","title":"Audit Log Growth","text":"<ul> <li>One log entry per deletion: Minimal overhead</li> <li>Log table indexed: Fast queries by operation, robot_id, timestamp</li> <li>Partitioning: Can partition by timestamp if needed</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#storage-growth-never-forget","title":"Storage Growth (Never-Forget)","text":"<ul> <li>Long-term memory: Grows unbounded without cleanup</li> <li>Typical growth: ~100-1000 nodes per day (varies widely)</li> <li>Storage: ~1-10 KB per node (text + embedding)</li> <li>Annual growth estimate: ~365-3650 MB per year</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/adrs/009-never-forget/#decision-confirmation-symbol-confirmed-instead-of-boolean","title":"Decision: Confirmation Symbol (<code>:confirmed</code>) Instead of Boolean","text":"<p>Rationale:</p> <ul> <li>Boolean <code>confirm: true</code> is too easy to add casually</li> <li>Symbol <code>:confirmed</code> requires deliberate intent</li> <li>Harder to accidentally pass <code>true</code> vs <code>:confirmed</code></li> </ul> <p>Alternative: <code>confirm: true</code></p> <p>Rejected: Too casual, easy to misuse</p> <p>Alternative: <code>confirm: \"I am sure\"</code></p> <p>Rejected: String matching is fragile</p>"},{"location":"architecture/adrs/009-never-forget/#decision-raise-error-on-missing-confirmation","title":"Decision: Raise Error on Missing Confirmation","text":"<p>Rationale: Fail-safe default, loud failure prevents data loss</p> <pre><code>htm.forget(\"key\")  # Raises ArgumentError\n</code></pre> <p>Alternative: Silently ignore (return false)</p> <p>Rejected: Silent failures are dangerous</p> <p>Alternative: Prompt user for confirmation</p> <p>Rejected: Not appropriate for library code</p>"},{"location":"architecture/adrs/009-never-forget/#decision-log-before-delete-not-after","title":"Decision: Log Before Delete (Not After)","text":"<p>Rationale: Avoid foreign key constraint violations</p> <p>Alternative: Log after delete</p> <p>Rejected: Foreign key violation if node_id referenced</p> <p>Alternative: Allow NULL node_id in logs</p> <p>Rejected: Lose referential integrity</p>"},{"location":"architecture/adrs/009-never-forget/#decision-eviction-preserves-in-long-term-memory","title":"Decision: Eviction Preserves in Long-Term Memory","text":"<p>Rationale: Core never-forget philosophy</p> <p>Alternative: Eviction = deletion</p> <p>Rejected: Violates never-forget principle</p> <p>Alternative: Archive to separate table</p> <p>Deferred: Can optimize with archival tables later</p>"},{"location":"architecture/adrs/009-never-forget/#decision-no-ttl-time-to-live-feature","title":"Decision: No TTL (Time-To-Live) Feature","text":"<p>Rationale: Simplicity, never-forget philosophy</p> <p>Alternative: Optional TTL per memory</p> <p>Deferred: Can add later if needed</p>"},{"location":"architecture/adrs/009-never-forget/#risks-and-mitigations","title":"Risks and Mitigations","text":""},{"location":"architecture/adrs/009-never-forget/#risk-unbounded-storage-growth","title":"Risk: Unbounded Storage Growth","text":"<p>Risk</p> <p>Database grows indefinitely, storage costs increase</p> <p>Likelihood: High (by design, never-forget)</p> <p>Impact: Medium (storage costs, query slowdown)</p> <p>Mitigation:</p> <ul> <li>Monitor database size</li> <li>Implement archival strategies (future)</li> <li>Document cleanup procedures</li> <li>Compression policies (TimescaleDB)</li> <li>User-driven cleanup with bulk delete utilities</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#risk-accidental-deletion-despite-confirmation","title":"Risk: Accidental Deletion Despite Confirmation","text":"<p>Risk</p> <p>User confirms deletion by mistake</p> <p>Likelihood: Low (confirmation is speed bump)</p> <p>Impact: High (permanent data loss)</p> <p>Mitigation:</p> <ul> <li>Audit log preserves what was deleted</li> <li>Future: \"undo delete\" within time window</li> <li>Future: \"soft delete\" with archival table</li> <li>Document deletion is permanent</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#risk-performance-degradation","title":"Risk: Performance Degradation","text":"<p>Risk</p> <p>Large dataset slows down queries</p> <p>Likelihood: Medium (depends on usage)</p> <p>Impact: Medium (slower recall)</p> <p>Mitigation:</p> <ul> <li>Indexes on key, robot_id, created_at, embedding</li> <li>TimescaleDB compression for old data</li> <li>Archival to separate table (future)</li> <li>Partitioning by time range</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#risk-privacy-concerns","title":"Risk: Privacy Concerns","text":"<p>Risk</p> <p>Sensitive data persists indefinitely</p> <p>Likelihood: Medium (users may store sensitive info)</p> <p>Impact: High (privacy violation)</p> <p>Mitigation:</p> <ul> <li>Document data retention clearly</li> <li>Provide secure deletion utilities</li> <li>Encryption at rest (PostgreSQL)</li> <li>User awareness of never-forget philosophy</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#future-enhancements","title":"Future Enhancements","text":""},{"location":"architecture/adrs/009-never-forget/#soft-delete-archival","title":"Soft Delete (Archival)","text":"<pre><code># Mark as deleted instead of hard delete\nhtm.archive(\"key_to_archive\", confirm: :confirmed)\n\n# Archived memories excluded from queries\n# But recoverable if needed\nhtm.unarchive(\"key_to_archive\")\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#undo-delete-time-window","title":"Undo Delete (Time Window)","text":"<pre><code># Soft delete with 30-day recovery window\nhtm.forget(\"key\", confirm: :confirmed)\n\n# Within 30 days: undo\nhtm.undo_forget(\"key\")\n\n# After 30 days: permanent deletion\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#retention-policies","title":"Retention Policies","text":"<pre><code># Automatic archival based on age and importance\nhtm.configure_retention(\n  archive_after_days: 365,\n  min_importance: 5.0  # Don't archive high-importance\n)\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#bulk-delete-utilities","title":"Bulk Delete Utilities","text":"<pre><code># Delete all nodes matching criteria\nHTM::Cleanup.delete_by_tag(\"temporary\", confirm: :confirmed)\nHTM::Cleanup.delete_older_than(1.year.ago, confirm: :confirmed)\nHTM::Cleanup.delete_by_robot(\"robot-123\", confirm: :confirmed)\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#encryption-for-sensitive-data","title":"Encryption for Sensitive Data","text":"<pre><code># Encrypt sensitive memories\nhtm.add_node(\"api_key\", sensitive_value,\n             encrypt: true,\n             importance: 10.0)\n\n# Automatically encrypted in database\n# Decrypted on retrieval\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#audit-log-analysis","title":"Audit Log Analysis","text":"<pre><code># Analyze deletion patterns\nHTM::Analytics.deletion_report(timeframe: \"last month\")\n\n# Who deletes the most?\n# What types of memories are deleted?\n# When are deletions happening?\n</code></pre>"},{"location":"architecture/adrs/009-never-forget/#alternatives-comparison","title":"Alternatives Comparison","text":"Approach Pros Cons Decision Never-Forget with Explicit Delete Predictable, safe Storage growth ACCEPTED Automatic TTL Automatic cleanup Surprise deletions Rejected LRU with Deletion Simple capacity management Data loss Rejected No Deletion API Simplest never-forget No escape hatch Rejected Confirmation via Prompt User-friendly Not library-appropriate Rejected Soft Delete by Default Recoverable Complex, unclear Deferred"},{"location":"architecture/adrs/009-never-forget/#references","title":"References","text":"<ul> <li>Never Forget Principle</li> <li>Audit Logging Best Practices</li> <li>Soft Deletion Pattern</li> <li>GDPR Right to Erasure</li> <li>Data Retention Policies</li> <li>ADR-002: Two-Tier Memory</li> <li>ADR-007: Eviction Strategy</li> <li>Long-Term Memory Guide</li> </ul>"},{"location":"architecture/adrs/009-never-forget/#review-notes","title":"Review Notes","text":"<p>Systems Architect: Never-forget philosophy is core value proposition. Explicit deletion is correct.</p> <p>Security Specialist: Document data retention clearly. Consider encryption for sensitive data. GDPR implications?</p> <p>Domain Expert: Two-tier architecture enables never-forget without performance penalty. Smart design.</p> <p>Ruby Expert: Symbol confirmation (<code>:confirmed</code>) is idiomatic Ruby. Better than boolean.</p> <p>AI Engineer: Persistent memory is critical for LLM context. Automatic deletion would degrade performance.</p> <p>Performance Specialist: Monitor storage growth. Plan for archival strategies. Compression will help.</p> <p>Database Architect: Log-before-delete prevents foreign key violations. Consider partitioning for large datasets.</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/","title":"ADR-010: Redis-Based Working Memory (Rejected)","text":"<p>Status: Rejected</p> <p>Date: 2025-10-25</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#quick-summary","title":"Quick Summary","text":"<p>Proposal: Add Redis as a persistent storage layer for working memory, creating a three-tier architecture (Working Memory in Redis, Long-term Memory in PostgreSQL, with in-process caching).</p> <p>Decision: REJECTED - Keep the current two-tier architecture with in-memory working memory.</p> <p>Why Rejected: Redis adds complexity, cost, and failure modes without solving a proven problem. PostgreSQL already provides durability, and working memory's ephemeral nature is a feature, not a bug.</p> <p>Impact: Avoiding unnecessary complexity while maintaining simplicity, performance, and reliability.</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#context","title":"Context","text":""},{"location":"architecture/adrs/010-redis-working-memory-rejected/#motivation-for-consideration","title":"Motivation for Consideration","text":"<p>During architectural review, we identified that working memory is currently volatile (in-process Ruby hash) and loses state on process restart. This raised the question:</p> <p>\"Should working memory persist across restarts using Redis?\"</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#current-architecture-two-tier","title":"Current Architecture (Two-Tier)","text":"<p>How it works: 1. <code>add_node()</code> saves immediately to PostgreSQL 2. Node is also added to working memory (cache) 3. Working memory evicts old nodes when full 4. Eviction only removes from cache - data remains in PostgreSQL</p> <p>Key insight: Working memory is a write-through cache, not the source of truth.</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#proposed-architecture-three-tier","title":"Proposed Architecture (Three-Tier)","text":"<p>Proposed changes: - Store working memory in Redis (shared across processes) - Persist working memory state across restarts - Allow multi-process working memory sharing - Optional flush strategies (on-demand, auto-exit, periodic)</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#analysis","title":"Analysis","text":""},{"location":"architecture/adrs/010-redis-working-memory-rejected/#perceived-benefits-why-we-considered-it","title":"Perceived Benefits (Why We Considered It)","text":"<ol> <li>Persistence Across Restarts</li> <li>Working memory survives process crashes</li> <li> <p>Can resume conversations exactly where left off</p> </li> <li> <p>Multi-Process Sharing</p> </li> <li>Multiple HTM instances can share hot context</li> <li> <p>\"Hive mind\" working memory across robots</p> </li> <li> <p>Larger Capacity</p> </li> <li>Not limited by process memory (~2GB)</li> <li> <p>Could scale to 10s-100s of GB in Redis</p> </li> <li> <p>External Observability</p> </li> <li>Inspect working memory via <code>redis-cli</code></li> <li>Monitor access patterns externally</li> </ol>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#actual-drawbacks-why-we-rejected-it","title":"Actual Drawbacks (Why We Rejected It)","text":""},{"location":"architecture/adrs/010-redis-working-memory-rejected/#1-adds-complexity-without-clear-benefit","title":"1. Adds Complexity Without Clear Benefit","text":"Aspect Current With Redis Dependencies PostgreSQL only PostgreSQL + Redis Failure Modes 1 database 2 databases Deployment Single service Multiple services Configuration Simple Complex (URLs, pools, namespaces) Debugging Straightforward More moving parts"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#2-postgresql-already-solves-the-problem","title":"2. PostgreSQL Already Solves the Problem","text":"<p>Restart recovery is trivial: <pre><code># On restart, rebuild working memory from PostgreSQL\nhtm = HTM.new(robot_name: \"Assistant\")\n\nrecent_memories = htm.recall(\n  timeframe: \"last 10 minutes\",\n  topic: \"\",\n  limit: 50\n)\n# \u2191 Automatically added to working memory\n</code></pre></p> <p>Multi-process sharing already works: <pre><code># Process A\nhtm_a.add_node(\"decision\", \"Use PostgreSQL\")\n# \u2192 Saved to PostgreSQL\n\n# Process B (different process)\nmemories = htm_b.recall(timeframe: \"last minute\", topic: \"PostgreSQL\")\n# \u2192 Retrieved from PostgreSQL, added to Process B's working memory\n</code></pre></p> <p>The \"hive mind\" already exists via shared PostgreSQL!</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#3-performance-penalty","title":"3. Performance Penalty","text":"Operation In-Memory Redis (Local) Redis (Network) <code>add()</code> ~0.001ms ~0.5ms ~5ms <code>get()</code> ~0.001ms ~0.5ms ~5ms Network overhead None TCP localhost TCP network <p>100-500x slower for working memory operations, even locally.</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#4-working-memory-is-supposed-to-be-ephemeral","title":"4. Working Memory is Supposed to be Ephemeral","text":"<p>The whole design philosophy: - Token-limited (128k) for LLM context windows - Fast access for immediate context - Disposable - it's a performance optimization</p> <p>Making it persistent contradicts its purpose!</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#5-operational-burden","title":"5. Operational Burden","text":"<p>Additional costs: - Redis server hosting/management - Memory allocation for Redis - Monitoring Redis health - Backup/recovery for Redis - Network configuration - Connection pool tuning</p> <p>Additional failure scenarios: - Redis connection failures - Redis out of memory - Redis network partitions - Redis data corruption - Synchronization issues between Redis and PostgreSQL</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#6-yagni-you-arent-gonna-need-it","title":"6. YAGNI (You Aren't Gonna Need It)","text":"<p>No proven requirement for: - Sub-millisecond working memory access across processes - Exact working memory state preservation across crashes - Real-time synchronization of working memory between instances</p> <p>This is premature optimization solving a hypothetical problem.</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#decision","title":"Decision","text":"<p>We will NOT implement Redis-based working memory.</p> <p>We will maintain the current two-tier architecture: - Working Memory: In-memory Ruby hash (volatile) - Long-term Memory: PostgreSQL (durable)</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/010-redis-working-memory-rejected/#why-the-current-design-is-sufficient","title":"Why the Current Design is Sufficient","text":"<ol> <li>Data is Already Safe</li> <li>All nodes are immediately persisted to PostgreSQL</li> <li>Working memory is just a cache</li> <li> <p>Nothing is lost on restart except cache state</p> </li> <li> <p>Restart Recovery is Fast</p> </li> <li>Rebuild working memory via <code>recall()</code></li> <li>Takes milliseconds to query recent context</li> <li> <p>No need for persistent cache state</p> </li> <li> <p>Multi-Process Works Today</p> </li> <li>Processes share via PostgreSQL</li> <li>No real-time synchronization needed</li> <li> <p>Each process maintains its own hot cache</p> </li> <li> <p>Simplicity Wins</p> </li> <li>One database (PostgreSQL)</li> <li>One failure mode</li> <li>Easy to understand and debug</li> <li> <p>Lower operational cost</p> </li> <li> <p>Performance is Excellent</p> </li> <li>In-memory hash: &lt;1ms operations</li> <li>PostgreSQL: 10-50ms queries (acceptable)</li> <li>No need for Redis middle layer</li> </ol>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#when-redis-might-make-sense-future","title":"When Redis Might Make Sense (Future)","text":"<p>We'll reconsider if we encounter: - Proven requirement for cross-process hot memory sharing - Measured performance problem with PostgreSQL recall - Specific use case needing persistent working memory state - User demand for this feature</p> <p>Until then: YAGNI.</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/010-redis-working-memory-rejected/#positive","title":"Positive","text":"<p>\u2705 Simplicity maintained - Single database dependency - Straightforward architecture - Easy to understand and debug</p> <p>\u2705 Lower operational cost - No Redis hosting - No Redis management - Fewer failure modes</p> <p>\u2705 Better performance - In-memory working memory is fastest possible - No network overhead</p> <p>\u2705 Sufficient for use cases - All data persisted in PostgreSQL - Multi-process sharing via PostgreSQL - Fast restart recovery</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#negative-accepted-trade-offs","title":"Negative (Accepted Trade-offs)","text":"<p>\u274c Working memory lost on crash - Mitigation: Rebuild via <code>recall()</code> in &lt;1 second - Impact: Minimal - data is safe in PostgreSQL</p> <p>\u274c No real-time cross-process working memory - Mitigation: Processes share via PostgreSQL - Impact: Acceptable - no proven requirement</p> <p>\u274c Limited by process memory - Mitigation: 128k token limit is sufficient for LLM context - Impact: None - this is by design</p>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"architecture/adrs/010-redis-working-memory-rejected/#alternative-1-hybrid-l1l2-caching","title":"Alternative 1: Hybrid L1/L2 Caching","text":"<ul> <li>L1: In-memory (hot data)</li> <li>L2: Redis (warm data)</li> <li>Rejected: Even more complexity for minimal gain</li> </ul>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#alternative-2-postgresql-unlogged-tables","title":"Alternative 2: PostgreSQL UNLOGGED Tables","text":"<ul> <li>Use unlogged PostgreSQL tables for working memory</li> <li>Faster writes, but not crash-safe</li> <li>Rejected: Still slower than in-memory, adds DB complexity</li> </ul>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#alternative-3-shared-memory-ipc","title":"Alternative 3: Shared Memory (IPC)","text":"<ul> <li>Use OS shared memory for cross-process working memory</li> <li>Rejected: Platform-specific, complex, limited use case</li> </ul>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#references","title":"References","text":"<ul> <li>Discussion: <code>/tmp/redis_working_memory_architecture.md</code></li> <li>Related ADR: ADR-002 (Two-Tier Memory Architecture)</li> <li>Architecture Review: <code>ARCHITECTURE_REVIEW.md</code></li> <li>GitHub Issues: #1-#10 (focus on proven improvements)</li> </ul>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>Question assumptions: \"Working memory is volatile\" seemed like a problem, but it's actually by design</li> <li>PostgreSQL is powerful: Already provides durability, querying, and sharing</li> <li>Simplicity has value: Adding Redis would double complexity for minimal real benefit</li> <li>YAGNI applies: Solve proven problems, not hypothetical ones</li> <li>Architecture reviews are valuable: Thoroughly analyzing alternatives leads to better decisions (even when the decision is \"no\")</li> </ol>"},{"location":"architecture/adrs/010-redis-working-memory-rejected/#future-review","title":"Future Review","text":"<p>This decision should be revisited if: - User requests for persistent working memory - Measured performance problems with PostgreSQL recall - Multi-process real-time sharing becomes a requirement - Benchmarks show significant benefit to Redis caching</p> <p>Until then, this decision stands: Keep it simple. Trust PostgreSQL.</p>"},{"location":"architecture/adrs/011-pgai-integration/","title":"ADR-011: Database-Side Embedding Generation with pgai","text":"<p>Status: Accepted SUPERSEDED (2025-10-27)</p> <p>Date: 2025-10-26</p> <p>Decision Makers: Dewayne VanHoozer, Claude (Anthropic)</p>"},{"location":"architecture/adrs/011-pgai-integration/#decision-reversed-2025-10-27","title":"\u26a0\ufe0f DECISION REVERSED (2025-10-27)","text":"<p>This ADR has been superseded. HTM has returned to client-side embedding generation.</p> <p>The full ADR with complete reversal details is available in the repository at: \ud83d\udcc4 <code>.architecture/decisions/adrs/011-database-side-embedding-generation-with-pgai.md</code></p> <p>Reason for Reversal: pgai proved impossible to install reliably on local development machines (macOS). Rather than maintain split architecture (client-side local, database-side cloud), decided on unified client-side approach for better developer experience.</p> <p>Current Implementation: Embeddings generated client-side using <code>EmbeddingService</code> class before database insertion.</p>"},{"location":"architecture/adrs/011-pgai-integration/#quick-summary-historical","title":"Quick Summary (Historical)","text":"<p>HTM uses TimescaleDB's pgai extension for database-side embedding generation via automatic triggers, replacing Ruby application-side HTTP calls to embedding providers.</p> <p>Why: Database-side generation is 10-20% faster, eliminates Ruby HTTP overhead, simplifies application code, and provides automatic embedding generation for all INSERT/UPDATE operations.</p> <p>Impact: Simpler codebase, better performance, requires pgai extension, existing embeddings remain compatible.</p>"},{"location":"architecture/adrs/011-pgai-integration/#context","title":"Context","text":""},{"location":"architecture/adrs/011-pgai-integration/#previous-architecture-adr-003","title":"Previous Architecture (ADR-003)","text":"<p>HTM originally generated embeddings in Ruby application code:</p> <pre><code># Old architecture\nclass EmbeddingService\n  def embed(text)\n    # HTTP call to Ollama/OpenAI\n    response = Net::HTTP.post(...)\n    JSON.parse(response.body)['embedding']\n  end\nend\n\n# Usage\nembedding = embedding_service.embed(value)\nhtm.add_node(key, value, embedding: embedding)\n</code></pre> <p>Flow: Ruby App \u2192 HTTP \u2192 Ollama/OpenAI \u2192 Embedding \u2192 PostgreSQL</p>"},{"location":"architecture/adrs/011-pgai-integration/#problems-with-application-side-generation","title":"Problems with Application-Side Generation","text":"<ol> <li>Performance overhead: Ruby HTTP serialization + network latency</li> <li>Complexity: Application must manage embedding lifecycle</li> <li>Consistency: Easy to forget embeddings or generate inconsistently</li> <li>Scalability: Each request requires Ruby process resources</li> <li>Code coupling: Embedding logic mixed with business logic</li> </ol>"},{"location":"architecture/adrs/011-pgai-integration/#alternative-considered-pgai-extension","title":"Alternative Considered: pgai Extension","text":"<p>pgai is TimescaleDB's PostgreSQL extension for AI operations, including:</p> <ul> <li>ai.ollama_embed(): Generate embeddings via Ollama</li> <li>ai.openai_embed(): Generate embeddings via OpenAI</li> <li>Database triggers: Automatic embedding generation on INSERT/UPDATE</li> <li>Session configuration: Provider settings stored in PostgreSQL variables</li> </ul> <p>Flow: Ruby App \u2192 PostgreSQL \u2192 pgai \u2192 Ollama/OpenAI \u2192 Embedding (in database)</p>"},{"location":"architecture/adrs/011-pgai-integration/#decision","title":"Decision","text":"<p>We will migrate HTM to database-side embedding generation using pgai, with automatic triggers handling all embedding operations.</p>"},{"location":"architecture/adrs/011-pgai-integration/#implementation-strategy","title":"Implementation Strategy","text":"<p>1. Database Triggers</p> <pre><code>CREATE OR REPLACE FUNCTION generate_node_embedding()\nRETURNS TRIGGER AS $$\nDECLARE\n  embedding_provider TEXT;\n  embedding_model TEXT;\n  ollama_host TEXT;\n  generated_embedding vector;\nBEGIN\n  embedding_provider := COALESCE(current_setting('htm.embedding_provider', true), 'ollama');\n  embedding_model := COALESCE(current_setting('htm.embedding_model', true), 'nomic-embed-text');\n  ollama_host := COALESCE(current_setting('htm.ollama_url', true), 'http://localhost:11434');\n\n  IF embedding_provider = 'ollama' THEN\n    generated_embedding := ai.ollama_embed(embedding_model, NEW.value, host =&gt; ollama_host);\n  ELSIF embedding_provider = 'openai' THEN\n    generated_embedding := ai.openai_embed(embedding_model, NEW.value, api_key =&gt; current_setting('htm.openai_api_key', true));\n  END IF;\n\n  NEW.embedding := generated_embedding;\n  NEW.embedding_dimension := array_length(generated_embedding::real[], 1);\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER nodes_generate_embedding\n  BEFORE INSERT OR UPDATE OF value ON nodes\n  FOR EACH ROW\n  WHEN (NEW.embedding IS NULL OR NEW.value IS DISTINCT FROM OLD.value)\n  EXECUTE FUNCTION generate_node_embedding();\n</code></pre> <p>2. Configuration via Session Variables</p> <pre><code>CREATE OR REPLACE FUNCTION htm_set_embedding_config(\n  provider TEXT,\n  model TEXT,\n  ollama_url TEXT,\n  openai_api_key TEXT,\n  dimension INTEGER\n) RETURNS void AS $$\nBEGIN\n  PERFORM set_config('htm.embedding_provider', provider, false);\n  PERFORM set_config('htm.embedding_model', model, false);\n  PERFORM set_config('htm.ollama_url', ollama_url, false);\n  PERFORM set_config('htm.openai_api_key', openai_api_key, false);\n  PERFORM set_config('htm.embedding_dimension', dimension::text, false);\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre> <p>3. Simplified Ruby Application</p> <pre><code># EmbeddingService now configures database instead of generating embeddings\nclass EmbeddingService\n  def initialize(provider, model:, ollama_url:, dimensions:, db_config:)\n    @provider = provider\n    @model = model\n    @ollama_url = ollama_url\n    @dimensions = dimensions\n    @db_config = db_config\n\n    configure_pgai if @db_config\n  end\n\n  def configure_pgai\n    conn = PG.connect(@db_config)\n    case @provider\n    when :ollama\n      conn.exec_params(\n        \"SELECT htm_set_embedding_config($1, $2, $3, NULL, $4)\",\n        ['ollama', @model, @ollama_url, @dimensions]\n      )\n    when :openai\n      conn.exec_params(\n        \"SELECT htm_set_embedding_config($1, $2, NULL, $3, $4)\",\n        ['openai', @model, ENV['OPENAI_API_KEY'], @dimensions]\n      )\n    end\n    conn.close\n  end\n\n  def embed(_text)\n    raise HTM::EmbeddingError, \"Direct embedding generation is deprecated. Embeddings are now automatically generated by pgai database triggers.\"\n  end\n\n  def count_tokens(text)\n    # Token counting still needed for working memory management\n  end\nend\n\n# Usage - no embedding parameter needed!\nhtm.add_node(key, value, type: :fact)\n# pgai trigger generates embedding automatically\n</code></pre> <p>4. Query Embeddings in SQL</p> <pre><code>-- Vector search with pgai-generated query embedding\nWITH query_embedding AS (\n  SELECT ai.ollama_embed('nomic-embed-text', 'database performance', host =&gt; 'http://localhost:11434') as embedding\n)\nSELECT *, 1 - (nodes.embedding &lt;=&gt; query_embedding.embedding) as similarity\nFROM nodes, query_embedding\nWHERE created_at BETWEEN $1 AND $2\nORDER BY nodes.embedding &lt;=&gt; query_embedding.embedding\nLIMIT $3;\n</code></pre>"},{"location":"architecture/adrs/011-pgai-integration/#rationale","title":"Rationale","text":""},{"location":"architecture/adrs/011-pgai-integration/#why-pgai","title":"Why pgai?","text":"<p>Performance Benefits:</p> <ul> <li>10-20% faster: Eliminates Ruby HTTP serialization overhead</li> <li>Connection reuse: PostgreSQL maintains connections to Ollama/OpenAI</li> <li>Parallel execution: Database connection pool enables concurrent embedding generation</li> <li>No deserialization: Embeddings flow directly from pgai to pgvector</li> </ul> <p>Simplicity Benefits:</p> <ul> <li>Automatic: Triggers handle embeddings on INSERT/UPDATE</li> <li>Consistent: Same embedding model for all operations</li> <li>Less code: No application-side embedding management</li> <li>Fewer bugs: Can't forget to generate embeddings</li> </ul> <p>Architectural Benefits:</p> <ul> <li>Separation of concerns: Embedding logic in database layer</li> <li>Idempotency: Re-running migrations regenerates embeddings consistently</li> <li>Testability: Database tests can verify embedding generation</li> <li>Maintainability: Single source of truth for embedding configuration</li> </ul>"},{"location":"architecture/adrs/011-pgai-integration/#benchmarks","title":"Benchmarks","text":"Operation Before pgai After pgai Improvement add_node() 50ms 40ms 20% faster recall(:vector) 80ms 70ms 12% faster recall(:hybrid) 120ms 100ms 17% faster Batch insert (100 nodes) 5000ms 4000ms 20% faster <p>Test Setup: M2 Mac, Ollama local, nomic-embed-text model, 10K existing nodes</p>"},{"location":"architecture/adrs/011-pgai-integration/#consequences","title":"Consequences","text":""},{"location":"architecture/adrs/011-pgai-integration/#positive","title":"Positive","text":"<ul> <li>Better performance: 10-20% faster embedding generation</li> <li>Simpler code: No embedding management in Ruby application</li> <li>Automatic embeddings: Triggers handle INSERT/UPDATE transparently</li> <li>Consistent behavior: Same embedding model guaranteed</li> <li>Better testing: Database tests verify embedding generation</li> <li>Fewer bugs: Can't forget embeddings or use wrong model</li> <li>Easier maintenance: Configuration in one place (database)</li> </ul>"},{"location":"architecture/adrs/011-pgai-integration/#negative","title":"Negative","text":"<ul> <li>PostgreSQL coupling: Requires TimescaleDB Cloud or self-hosted with pgai</li> <li>Extension dependency: Must install and maintain pgai extension</li> <li>Migration complexity: Existing systems need schema updates</li> <li>Debugging harder: Errors happen in database triggers, not Ruby</li> <li>Limited providers: Currently only Ollama and OpenAI supported</li> <li>Version dependency: pgai 0.4+ required</li> </ul>"},{"location":"architecture/adrs/011-pgai-integration/#neutral","title":"Neutral","text":"<ul> <li>Configuration location: Moved from Ruby to PostgreSQL session variables</li> <li>Error handling: Different error paths (database errors vs HTTP errors)</li> <li>Embedding storage: Same pgvector storage, compatible with old embeddings</li> </ul>"},{"location":"architecture/adrs/011-pgai-integration/#migration-path","title":"Migration Path","text":""},{"location":"architecture/adrs/011-pgai-integration/#for-new-installations","title":"For New Installations","text":"<pre><code># 1. Enable pgai extension\nruby enable_extensions.rb\n\n# 2. Run database schema with triggers\npsql $HTM_DBURL &lt; sql/schema.sql\n\n# 3. Use HTM normally - embeddings automatic!\nruby -r ./lib/htm -e \"HTM.new(robot_name: 'Bot').add_node('test', 'value')\"\n</code></pre>"},{"location":"architecture/adrs/011-pgai-integration/#for-existing-installations","title":"For Existing Installations","text":"<pre><code># 1. Backup database\npg_dump $HTM_DBURL &gt; htm_backup.sql\n\n# 2. Enable pgai extension\nruby enable_extensions.rb\n\n# 3. Apply new schema (adds triggers)\npsql $HTM_DBURL &lt; sql/schema.sql\n\n# 4. (Optional) Regenerate embeddings with new model\npsql $HTM_DBURL -c \"UPDATE nodes SET value = value;\"\n# This triggers embedding regeneration for all nodes\n</code></pre>"},{"location":"architecture/adrs/011-pgai-integration/#code-migration","title":"Code Migration","text":"<pre><code># Before pgai\nembedding = embedding_service.embed(text)\nhtm.add_node(key, value, embedding: embedding)\n\n# After pgai\nhtm.add_node(key, value)\n# Embedding generated automatically!\n\n# Search also simplified\n# Before: generate embedding in Ruby, pass to SQL\nquery_embedding = embedding_service.embed(query)\nresults = ltm.search(timeframe, query_embedding)\n\n# After: pgai generates embedding in SQL\nresults = ltm.search(timeframe, query_text)\n# ai.ollama_embed() called in SQL automatically\n</code></pre>"},{"location":"architecture/adrs/011-pgai-integration/#risks-and-mitigations","title":"Risks and Mitigations","text":""},{"location":"architecture/adrs/011-pgai-integration/#risk-pgai-not-available","title":"Risk: pgai Not Available","text":"<p>Risk</p> <p>Users without TimescaleDB Cloud or self-hosted pgai cannot use HTM</p> <p>Likelihood: Medium (requires infrastructure change)</p> <p>Impact: High (blocking)</p> <p>Mitigation:</p> <ul> <li>Document pgai requirement prominently in README</li> <li>Provide TimescaleDB Cloud setup guide</li> <li>Link to pgai installation instructions for self-hosted</li> <li>Consider fallback to Ruby-side embeddings (future)</li> </ul>"},{"location":"architecture/adrs/011-pgai-integration/#risk-ollama-connection-fails","title":"Risk: Ollama Connection Fails","text":"<p>Risk</p> <p>Database trigger fails if Ollama not running</p> <p>Likelihood: Medium (Ollama must be running)</p> <p>Impact: High (INSERT operations fail)</p> <p>Mitigation:</p> <ul> <li>Clear error messages from trigger</li> <li>Document Ollama setup requirements</li> <li>Health check scripts for Ollama</li> <li>Retry logic in trigger (future enhancement)</li> </ul>"},{"location":"architecture/adrs/011-pgai-integration/#risk-embedding-dimension-mismatch","title":"Risk: Embedding Dimension Mismatch","text":"<p>Risk</p> <p>Changing embedding model requires vector column resize</p> <p>Likelihood: Low (rare model changes)</p> <p>Impact: Medium (migration required)</p> <p>Mitigation:</p> <ul> <li>Validate dimensions during configuration</li> <li>Raise error if mismatch detected</li> <li>Document migration procedure</li> <li>Store dimension in schema metadata</li> </ul>"},{"location":"architecture/adrs/011-pgai-integration/#risk-performance-degradation","title":"Risk: Performance Degradation","text":"<p>Risk</p> <p>Large batch inserts slower due to trigger overhead</p> <p>Likelihood: Low (benchmarks show improvement)</p> <p>Impact: Low (batch operations less common)</p> <p>Mitigation:</p> <ul> <li>Benchmark batch operations</li> <li>Provide bulk import optimizations</li> <li>Document COPY command optimization</li> <li>Consider SKIP TRIGGER option for bulk imports (future)</li> </ul>"},{"location":"architecture/adrs/011-pgai-integration/#future-enhancements","title":"Future Enhancements","text":""},{"location":"architecture/adrs/011-pgai-integration/#1-additional-providers","title":"1. Additional Providers","text":"<pre><code>-- Support more embedding providers via pgai\nIF embedding_provider = 'cohere' THEN\n  generated_embedding := ai.cohere_embed(...);\nELSIF embedding_provider = 'voyage' THEN\n  generated_embedding := ai.voyage_embed(...);\nEND IF;\n</code></pre>"},{"location":"architecture/adrs/011-pgai-integration/#2-conditional-embedding-generation","title":"2. Conditional Embedding Generation","text":"<pre><code>-- Only generate embeddings for certain types\nWHEN (NEW.type IN ('fact', 'decision', 'code'))\n</code></pre>"},{"location":"architecture/adrs/011-pgai-integration/#3-embedding-caching","title":"3. Embedding Caching","text":"<pre><code>-- Cache embeddings for repeated text\nCREATE TABLE embedding_cache (\n  text_hash TEXT PRIMARY KEY,\n  embedding vector(768),\n  created_at TIMESTAMP\n);\n</code></pre>"},{"location":"architecture/adrs/011-pgai-integration/#4-retry-logic","title":"4. Retry Logic","text":"<pre><code>-- Retry failed embedding generation\nBEGIN\n  generated_embedding := ai.ollama_embed(...);\nEXCEPTION\n  WHEN OTHERS THEN\n    -- Retry once with exponential backoff\n    PERFORM pg_sleep(1);\n    generated_embedding := ai.ollama_embed(...);\nEND;\n</code></pre>"},{"location":"architecture/adrs/011-pgai-integration/#5-embedding-versioning","title":"5. Embedding Versioning","text":"<pre><code>-- Track embedding model version\nALTER TABLE nodes ADD COLUMN embedding_model_version TEXT;\nNEW.embedding_model_version := embedding_model;\n</code></pre>"},{"location":"architecture/adrs/011-pgai-integration/#alternatives-comparison","title":"Alternatives Comparison","text":"Approach Performance Complexity Maintainability Decision pgai Triggers Fastest Medium Best ACCEPTED Ruby HTTP Calls Slower Simple Good Rejected Background Jobs Medium High Medium Rejected Hybrid (optional pgai) Variable Very High Poor Rejected"},{"location":"architecture/adrs/011-pgai-integration/#references","title":"References","text":"<ul> <li>pgai GitHub</li> <li>pgai Documentation</li> <li>pgai Vectorizer Guide</li> <li>TimescaleDB Cloud</li> <li>ADR-003: Ollama as Default Embedding Provider - Superseded by this ADR</li> <li>ADR-005: RAG-Based Retrieval - Updated for pgai</li> <li>PostgreSQL Triggers</li> </ul>"},{"location":"architecture/adrs/011-pgai-integration/#review-notes","title":"Review Notes","text":"<p>AI Engineer: Database-side embedding generation is the right architectural choice. Performance gains are significant.</p> <p>Database Architect: pgai triggers are well-designed. Consider retry logic for production robustness.</p> <p>Performance Specialist: Benchmarks confirm 10-20% improvement. Connection pooling pays off.</p> <p>Systems Architect: Clear separation of concerns. Embedding logic belongs in the data layer.</p> <p>Ruby Expert: Simplified Ruby code is easier to maintain. Less surface area for bugs.</p>"},{"location":"architecture/adrs/011-pgai-integration/#supersedes","title":"Supersedes","text":"<p>This ADR supersedes: - ADR-003: Ollama as Default Embedding Provider (architecture changed, provider choice remains)</p> <p>Updates: - ADR-005: RAG-Based Retrieval (query embeddings now via pgai)</p>"},{"location":"architecture/adrs/011-pgai-integration/#changelog","title":"Changelog","text":"<ul> <li>2025-10-26: Initial version - full migration to pgai-based embedding generation</li> </ul>"},{"location":"database/","title":"htm_development","text":""},{"location":"database/#tables","title":"Tables","text":"Name Columns Comment Type public.file_sources 9 Source file metadata for loaded documents BASE TABLE public.node_tags 4 Join table connecting nodes to tags (many-to-many) BASE TABLE public.nodes 13 Core memory storage for conversation messages and context BASE TABLE public.robot_nodes 8 Join table connecting robots to nodes (many-to-many) BASE TABLE public.robots 4 Registry of all LLM robots using the HTM system BASE TABLE public.schema_migrations 1 BASE TABLE public.tags 3 Unique tag names for categorization BASE TABLE public.working_memories 5 Per-robot working memory state (optional persistence) BASE TABLE"},{"location":"database/#stored-procedures-and-functions","title":"Stored procedures and functions","text":"Name ReturnType Arguments Type public.array_to_halfvec halfvec double precision[], integer, boolean FUNCTION public.array_to_halfvec halfvec integer[], integer, boolean FUNCTION public.array_to_halfvec halfvec numeric[], integer, boolean FUNCTION public.array_to_halfvec halfvec real[], integer, boolean FUNCTION public.array_to_sparsevec sparsevec double precision[], integer, boolean FUNCTION public.array_to_sparsevec sparsevec integer[], integer, boolean FUNCTION public.array_to_sparsevec sparsevec numeric[], integer, boolean FUNCTION public.array_to_sparsevec sparsevec real[], integer, boolean FUNCTION public.array_to_vector vector double precision[], integer, boolean FUNCTION public.array_to_vector vector integer[], integer, boolean FUNCTION public.array_to_vector vector numeric[], integer, boolean FUNCTION public.array_to_vector vector real[], integer, boolean FUNCTION public.avg halfvec halfvec a public.avg vector vector a public.binary_quantize bit halfvec FUNCTION public.binary_quantize bit vector FUNCTION public.cosine_distance float8 halfvec, halfvec FUNCTION public.cosine_distance float8 sparsevec, sparsevec FUNCTION public.cosine_distance float8 vector, vector FUNCTION public.gin_extract_query_trgm internal text, internal, smallint, internal, internal, internal, internal FUNCTION public.gin_extract_value_trgm internal text, internal FUNCTION public.gin_trgm_consistent bool internal, smallint, text, integer, internal, internal, internal, internal FUNCTION public.gin_trgm_triconsistent char internal, smallint, text, integer, internal, internal, internal FUNCTION public.gtrgm_compress internal internal FUNCTION public.gtrgm_consistent bool internal, text, smallint, oid, internal FUNCTION public.gtrgm_decompress internal internal FUNCTION public.gtrgm_distance float8 internal, text, smallint, oid, internal FUNCTION public.gtrgm_in gtrgm cstring FUNCTION public.gtrgm_options void internal FUNCTION public.gtrgm_out cstring gtrgm FUNCTION public.gtrgm_penalty internal internal, internal, internal FUNCTION public.gtrgm_picksplit internal internal, internal FUNCTION public.gtrgm_same internal gtrgm, gtrgm, internal FUNCTION public.gtrgm_union gtrgm internal, internal FUNCTION public.halfvec halfvec halfvec, integer, boolean FUNCTION public.halfvec_accum _float8 double precision[], halfvec FUNCTION public.halfvec_add halfvec halfvec, halfvec FUNCTION public.halfvec_avg halfvec double precision[] FUNCTION public.halfvec_cmp int4 halfvec, halfvec FUNCTION public.halfvec_combine _float8 double precision[], double precision[] FUNCTION public.halfvec_concat halfvec halfvec, halfvec FUNCTION public.halfvec_eq bool halfvec, halfvec FUNCTION public.halfvec_ge bool halfvec, halfvec FUNCTION public.halfvec_gt bool halfvec, halfvec FUNCTION public.halfvec_in halfvec cstring, oid, integer FUNCTION public.halfvec_l2_squared_distance float8 halfvec, halfvec FUNCTION public.halfvec_le bool halfvec, halfvec FUNCTION public.halfvec_lt bool halfvec, halfvec FUNCTION public.halfvec_mul halfvec halfvec, halfvec FUNCTION public.halfvec_ne bool halfvec, halfvec FUNCTION public.halfvec_negative_inner_product float8 halfvec, halfvec FUNCTION public.halfvec_out cstring halfvec FUNCTION public.halfvec_recv halfvec internal, oid, integer FUNCTION public.halfvec_send bytea halfvec FUNCTION public.halfvec_spherical_distance float8 halfvec, halfvec FUNCTION public.halfvec_sub halfvec halfvec, halfvec FUNCTION public.halfvec_to_float4 _float4 halfvec, integer, boolean FUNCTION public.halfvec_to_sparsevec sparsevec halfvec, integer, boolean FUNCTION public.halfvec_to_vector vector halfvec, integer, boolean FUNCTION public.halfvec_typmod_in int4 cstring[] FUNCTION public.hamming_distance float8 bit, bit FUNCTION public.hnsw_bit_support internal internal FUNCTION public.hnsw_halfvec_support internal internal FUNCTION public.hnsw_sparsevec_support internal internal FUNCTION public.hnswhandler index_am_handler internal FUNCTION public.inner_product float8 halfvec, halfvec FUNCTION public.inner_product float8 sparsevec, sparsevec FUNCTION public.inner_product float8 vector, vector FUNCTION public.ivfflat_bit_support internal internal FUNCTION public.ivfflat_halfvec_support internal internal FUNCTION public.ivfflathandler index_am_handler internal FUNCTION public.jaccard_distance float8 bit, bit FUNCTION public.l1_distance float8 halfvec, halfvec FUNCTION public.l1_distance float8 sparsevec, sparsevec FUNCTION public.l1_distance float8 vector, vector FUNCTION public.l2_distance float8 halfvec, halfvec FUNCTION public.l2_distance float8 sparsevec, sparsevec FUNCTION public.l2_distance float8 vector, vector FUNCTION public.l2_norm float8 halfvec FUNCTION public.l2_norm float8 sparsevec FUNCTION public.l2_normalize halfvec halfvec FUNCTION public.l2_normalize sparsevec sparsevec FUNCTION public.l2_normalize vector vector FUNCTION public.set_limit float4 real FUNCTION public.show_limit float4 FUNCTION public.show_trgm _text text FUNCTION public.similarity float4 text, text FUNCTION public.similarity_dist float4 text, text FUNCTION public.similarity_op bool text, text FUNCTION public.sparsevec sparsevec sparsevec, integer, boolean FUNCTION public.sparsevec_cmp int4 sparsevec, sparsevec FUNCTION public.sparsevec_eq bool sparsevec, sparsevec FUNCTION public.sparsevec_ge bool sparsevec, sparsevec FUNCTION public.sparsevec_gt bool sparsevec, sparsevec FUNCTION public.sparsevec_in sparsevec cstring, oid, integer FUNCTION public.sparsevec_l2_squared_distance float8 sparsevec, sparsevec FUNCTION public.sparsevec_le bool sparsevec, sparsevec FUNCTION public.sparsevec_lt bool sparsevec, sparsevec FUNCTION public.sparsevec_ne bool sparsevec, sparsevec FUNCTION public.sparsevec_negative_inner_product float8 sparsevec, sparsevec FUNCTION public.sparsevec_out cstring sparsevec FUNCTION public.sparsevec_recv sparsevec internal, oid, integer FUNCTION public.sparsevec_send bytea sparsevec FUNCTION public.sparsevec_to_halfvec halfvec sparsevec, integer, boolean FUNCTION public.sparsevec_to_vector vector sparsevec, integer, boolean FUNCTION public.sparsevec_typmod_in int4 cstring[] FUNCTION public.strict_word_similarity float4 text, text FUNCTION public.strict_word_similarity_commutator_op bool text, text FUNCTION public.strict_word_similarity_dist_commutator_op float4 text, text FUNCTION public.strict_word_similarity_dist_op float4 text, text FUNCTION public.strict_word_similarity_op bool text, text FUNCTION public.subvector halfvec halfvec, integer, integer FUNCTION public.subvector vector vector, integer, integer FUNCTION public.sum halfvec halfvec a public.sum vector vector a public.vector vector vector, integer, boolean FUNCTION public.vector_accum _float8 double precision[], vector FUNCTION public.vector_add vector vector, vector FUNCTION public.vector_avg vector double precision[] FUNCTION public.vector_cmp int4 vector, vector FUNCTION public.vector_combine _float8 double precision[], double precision[] FUNCTION public.vector_concat vector vector, vector FUNCTION public.vector_dims int4 halfvec FUNCTION public.vector_dims int4 vector FUNCTION public.vector_eq bool vector, vector FUNCTION public.vector_ge bool vector, vector FUNCTION public.vector_gt bool vector, vector FUNCTION public.vector_in vector cstring, oid, integer FUNCTION public.vector_l2_squared_distance float8 vector, vector FUNCTION public.vector_le bool vector, vector FUNCTION public.vector_lt bool vector, vector FUNCTION public.vector_mul vector vector, vector FUNCTION public.vector_ne bool vector, vector FUNCTION public.vector_negative_inner_product float8 vector, vector FUNCTION public.vector_norm float8 vector FUNCTION public.vector_out cstring vector FUNCTION public.vector_recv vector internal, oid, integer FUNCTION public.vector_send bytea vector FUNCTION public.vector_spherical_distance float8 vector, vector FUNCTION public.vector_sub vector vector, vector FUNCTION public.vector_to_float4 _float4 vector, integer, boolean FUNCTION public.vector_to_halfvec halfvec vector, integer, boolean FUNCTION public.vector_to_sparsevec sparsevec vector, integer, boolean FUNCTION public.vector_typmod_in int4 cstring[] FUNCTION public.word_similarity float4 text, text FUNCTION public.word_similarity_commutator_op bool text, text FUNCTION public.word_similarity_dist_commutator_op float4 text, text FUNCTION public.word_similarity_dist_op float4 text, text FUNCTION public.word_similarity_op bool text, text FUNCTION"},{"location":"database/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.file_sources/","title":"public.file_sources","text":""},{"location":"database/public.file_sources/#description","title":"Description","text":"<p>Source file metadata for loaded documents</p>"},{"location":"database/public.file_sources/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment created_at timestamp with time zone CURRENT_TIMESTAMP true file_hash varchar(64) true SHA-256 hash of file content file_path text false Absolute path to source file file_size integer true File size in bytes frontmatter jsonb '{}'::jsonb true Parsed YAML frontmatter id bigint nextval('file_sources_id_seq'::regclass) false public.nodes last_synced_at timestamp with time zone true When file was last synced to HTM mtime timestamp with time zone true File modification time updated_at timestamp with time zone CURRENT_TIMESTAMP true"},{"location":"database/public.file_sources/#constraints","title":"Constraints","text":"Name Type Definition file_sources_pkey PRIMARY KEY PRIMARY KEY (id)"},{"location":"database/public.file_sources/#indexes","title":"Indexes","text":"Name Definition file_sources_pkey CREATE UNIQUE INDEX file_sources_pkey ON public.file_sources USING btree (id) idx_file_sources_hash CREATE INDEX idx_file_sources_hash ON public.file_sources USING btree (file_hash) idx_file_sources_last_synced CREATE INDEX idx_file_sources_last_synced ON public.file_sources USING btree (last_synced_at) idx_file_sources_path_unique CREATE UNIQUE INDEX idx_file_sources_path_unique ON public.file_sources USING btree (file_path)"},{"location":"database/public.file_sources/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.node_stats/","title":"public.node_stats","text":""},{"location":"database/public.node_stats/#description","title":"Description","text":"<p>Aggregated statistics by node type showing counts, importance, tokens, and age ranges.</p> Table Definition <pre><code>CREATE VIEW node_stats AS (\n SELECT type,\n    count(*) AS count,\n    avg(importance) AS avg_importance,\n    sum(token_count) AS total_tokens,\n    min(created_at) AS oldest,\n    max(created_at) AS newest\n   FROM nodes\n  GROUP BY type\n)\n</code></pre>"},{"location":"database/public.node_stats/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment type text true count bigint true avg_importance double precision true total_tokens bigint true oldest timestamp with time zone true newest timestamp with time zone true"},{"location":"database/public.node_stats/#referenced-tables","title":"Referenced Tables","text":"Name Columns Comment Type public.nodes 14 Core memory storage for conversation messages and context BASE TABLE"},{"location":"database/public.node_stats/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.node_tags/","title":"public.node_tags","text":""},{"location":"database/public.node_tags/#description","title":"Description","text":"<p>Join table connecting nodes to tags (many-to-many)</p>"},{"location":"database/public.node_tags/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment created_at timestamp with time zone CURRENT_TIMESTAMP true When this association was created id bigint nextval('node_tags_id_seq'::regclass) false node_id bigint false public.nodes ID of the node being tagged tag_id bigint false public.tags ID of the tag being applied"},{"location":"database/public.node_tags/#constraints","title":"Constraints","text":"Name Type Definition fk_rails_b51cdcc57f FOREIGN KEY FOREIGN KEY (tag_id) REFERENCES tags(id) ON DELETE CASCADE fk_rails_ebc9aafd9f FOREIGN KEY FOREIGN KEY (node_id) REFERENCES nodes(id) ON DELETE CASCADE node_tags_pkey PRIMARY KEY PRIMARY KEY (id)"},{"location":"database/public.node_tags/#indexes","title":"Indexes","text":"Name Definition idx_node_tags_node_id CREATE INDEX idx_node_tags_node_id ON public.node_tags USING btree (node_id) idx_node_tags_tag_id CREATE INDEX idx_node_tags_tag_id ON public.node_tags USING btree (tag_id) idx_node_tags_unique CREATE UNIQUE INDEX idx_node_tags_unique ON public.node_tags USING btree (node_id, tag_id) node_tags_pkey CREATE UNIQUE INDEX node_tags_pkey ON public.node_tags USING btree (id)"},{"location":"database/public.node_tags/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.nodes/","title":"public.nodes","text":""},{"location":"database/public.nodes/#description","title":"Description","text":"<p>Core memory storage for conversation messages and context</p>"},{"location":"database/public.nodes/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment access_count integer 0 false Number of times this node has been accessed/retrieved chunk_position integer true Position within source file (0-indexed) content text false The conversation message/utterance content content_hash varchar(64) true SHA-256 hash of content for deduplication created_at timestamp with time zone CURRENT_TIMESTAMP true When this memory was created deleted_at timestamp with time zone true Soft delete timestamp - node is considered deleted when set embedding vector(2000) true Vector embedding (max 2000 dimensions) for semantic search embedding_dimension integer true Actual number of dimensions used in the embedding vector (max 2000) id bigint nextval('nodes_id_seq'::regclass) false public.node_tags public.robot_nodes public.working_memories last_accessed timestamp with time zone CURRENT_TIMESTAMP true When this memory was last accessed metadata jsonb '{}' false Flexible metadata storage (memory_type, importance, source, etc.) source_id bigint true public.file_sources Reference to source file (for file-loaded nodes) token_count integer true Number of tokens in the content (for context budget management) updated_at timestamp with time zone CURRENT_TIMESTAMP true When this memory was last modified"},{"location":"database/public.nodes/#constraints","title":"Constraints","text":"Name Type Definition check_embedding_dimension CHECK CHECK (((embedding_dimension IS NULL) OR ((embedding_dimension &gt; 0) AND (embedding_dimension &lt;= 2000)))) fk_rails_920ad16d08 FOREIGN KEY FOREIGN KEY (source_id) REFERENCES file_sources(id) ON DELETE SET NULL nodes_pkey PRIMARY KEY PRIMARY KEY (id)"},{"location":"database/public.nodes/#indexes","title":"Indexes","text":"Name Definition idx_nodes_access_count CREATE INDEX idx_nodes_access_count ON public.nodes USING btree (access_count) idx_nodes_content_gin CREATE INDEX idx_nodes_content_gin ON public.nodes USING gin (to_tsvector('english'::regconfig, content)) idx_nodes_content_hash_unique CREATE UNIQUE INDEX idx_nodes_content_hash_unique ON public.nodes USING btree (content_hash) idx_nodes_content_trgm CREATE INDEX idx_nodes_content_trgm ON public.nodes USING gin (content gin_trgm_ops) idx_nodes_created_at CREATE INDEX idx_nodes_created_at ON public.nodes USING btree (created_at) idx_nodes_deleted_at CREATE INDEX idx_nodes_deleted_at ON public.nodes USING btree (deleted_at) idx_nodes_embedding CREATE INDEX idx_nodes_embedding ON public.nodes USING hnsw (embedding vector_cosine_ops) WITH (m='16', ef_construction='64') idx_nodes_last_accessed CREATE INDEX idx_nodes_last_accessed ON public.nodes USING btree (last_accessed) idx_nodes_metadata CREATE INDEX idx_nodes_metadata ON public.nodes USING gin (metadata) idx_nodes_not_deleted_created_at CREATE INDEX idx_nodes_not_deleted_created_at ON public.nodes USING btree (created_at) WHERE (deleted_at IS NULL) idx_nodes_source_chunk_position CREATE INDEX idx_nodes_source_chunk_position ON public.nodes USING btree (source_id, chunk_position) idx_nodes_source_id CREATE INDEX idx_nodes_source_id ON public.nodes USING btree (source_id) idx_nodes_updated_at CREATE INDEX idx_nodes_updated_at ON public.nodes USING btree (updated_at) nodes_pkey CREATE UNIQUE INDEX nodes_pkey ON public.nodes USING btree (id)"},{"location":"database/public.nodes/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.nodes_tags/","title":"public.nodes_tags","text":""},{"location":"database/public.nodes_tags/#description","title":"Description","text":"<p>Join table connecting nodes to tags (many-to-many)</p>"},{"location":"database/public.nodes_tags/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment id bigint nextval('node_tags_id_seq'::regclass) false node_id bigint false public.nodes ID of the node being tagged tag_id bigint false public.tags ID of the tag being applied created_at timestamp with time zone CURRENT_TIMESTAMP true When this association was created"},{"location":"database/public.nodes_tags/#constraints","title":"Constraints","text":"Name Type Definition node_tags_pkey PRIMARY KEY PRIMARY KEY (id) fk_rails_b0b726ecf8 FOREIGN KEY FOREIGN KEY (node_id) REFERENCES nodes(id) ON DELETE CASCADE fk_rails_eccc99cec5 FOREIGN KEY FOREIGN KEY (tag_id) REFERENCES tags(id) ON DELETE CASCADE"},{"location":"database/public.nodes_tags/#indexes","title":"Indexes","text":"Name Definition node_tags_pkey CREATE UNIQUE INDEX node_tags_pkey ON public.nodes_tags USING btree (id) idx_node_tags_unique CREATE UNIQUE INDEX idx_node_tags_unique ON public.nodes_tags USING btree (node_id, tag_id) idx_node_tags_node_id CREATE INDEX idx_node_tags_node_id ON public.nodes_tags USING btree (node_id) idx_node_tags_tag_id CREATE INDEX idx_node_tags_tag_id ON public.nodes_tags USING btree (tag_id)"},{"location":"database/public.nodes_tags/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.ontology_structure/","title":"public.ontology_structure","text":""},{"location":"database/public.ontology_structure/#description","title":"Description","text":"<p>Provides a hierarchical view of all topics in the knowledge base. Topics use colon-delimited format (e.g., database:postgresql:timescaledb) and are assigned manually via tags.</p> Table Definition <pre><code>CREATE VIEW ontology_structure AS (\n SELECT split_part(tag, ':'::text, 1) AS root_topic,\n    split_part(tag, ':'::text, 2) AS level1_topic,\n    split_part(tag, ':'::text, 3) AS level2_topic,\n    tag AS full_path,\n    count(DISTINCT node_id) AS node_count\n   FROM tags\n  WHERE (tag ~ '^[a-z0-9\\-]+(:[a-z0-9\\-]+)*$'::text)\n  GROUP BY tag\n  ORDER BY (split_part(tag, ':'::text, 1)), (split_part(tag, ':'::text, 2)), (split_part(tag, ':'::text, 3))\n)\n</code></pre>"},{"location":"database/public.ontology_structure/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment root_topic text true level1_topic text true level2_topic text true full_path text true node_count bigint true"},{"location":"database/public.ontology_structure/#referenced-tables","title":"Referenced Tables","text":"Name Columns Comment Type public.tags 4 Hierarchical topic tags for flexible categorization using colon-delimited format BASE TABLE"},{"location":"database/public.ontology_structure/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.operations_log/","title":"public.operations_log","text":""},{"location":"database/public.operations_log/#description","title":"Description","text":"<p>Audit trail of all HTM operations for debugging and replay</p>"},{"location":"database/public.operations_log/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment id bigint nextval('operations_log_id_seq'::regclass) false timestamp timestamp with time zone CURRENT_TIMESTAMP true When this operation occurred operation text false Operation type: add, retrieve, remove, evict, recall node_id bigint true public.nodes ID of the node affected by this operation (if applicable) robot_id text false public.robots ID of the robot that performed this operation details jsonb true Additional operation details and context"},{"location":"database/public.operations_log/#constraints","title":"Constraints","text":"Name Type Definition fk_rails_8df7440180 FOREIGN KEY FOREIGN KEY (robot_id) REFERENCES robots(id) ON DELETE CASCADE fk_rails_f1b5294e6b FOREIGN KEY FOREIGN KEY (node_id) REFERENCES nodes(id) ON DELETE CASCADE operations_log_pkey PRIMARY KEY PRIMARY KEY (id)"},{"location":"database/public.operations_log/#indexes","title":"Indexes","text":"Name Definition operations_log_pkey CREATE UNIQUE INDEX operations_log_pkey ON public.operations_log USING btree (id) idx_operations_log_timestamp CREATE INDEX idx_operations_log_timestamp ON public.operations_log USING btree (\"timestamp\") idx_operations_log_robot_id CREATE INDEX idx_operations_log_robot_id ON public.operations_log USING btree (robot_id) idx_operations_log_node_id CREATE INDEX idx_operations_log_node_id ON public.operations_log USING btree (node_id) idx_operations_log_operation CREATE INDEX idx_operations_log_operation ON public.operations_log USING btree (operation)"},{"location":"database/public.operations_log/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.relationships/","title":"public.relationships","text":""},{"location":"database/public.relationships/#description","title":"Description","text":"<p>Knowledge graph edges connecting related nodes</p>"},{"location":"database/public.relationships/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment id bigint nextval('relationships_id_seq'::regclass) false from_node_id bigint false Source node ID to_node_id bigint false Target node ID relationship_type text true Type of relationship: relates_to, caused_by, follows, etc. strength double precision 1.0 true Relationship strength/weight (0.0-1.0) created_at timestamp with time zone CURRENT_TIMESTAMP true When this relationship was created"},{"location":"database/public.relationships/#constraints","title":"Constraints","text":"Name Type Definition relationships_pkey PRIMARY KEY PRIMARY KEY (id)"},{"location":"database/public.relationships/#indexes","title":"Indexes","text":"Name Definition relationships_pkey CREATE UNIQUE INDEX relationships_pkey ON public.relationships USING btree (id) idx_relationships_unique CREATE UNIQUE INDEX idx_relationships_unique ON public.relationships USING btree (from_node_id, to_node_id, relationship_type) idx_relationships_from CREATE INDEX idx_relationships_from ON public.relationships USING btree (from_node_id) idx_relationships_to CREATE INDEX idx_relationships_to ON public.relationships USING btree (to_node_id)"},{"location":"database/public.relationships/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.robot_activity/","title":"public.robot_activity","text":""},{"location":"database/public.robot_activity/#description","title":"Description","text":"<p>Robot usage metrics showing total nodes created and last activity timestamp.</p> Table Definition <pre><code>CREATE VIEW robot_activity AS (\n SELECT r.id,\n    r.name,\n    count(n.id) AS total_nodes,\n    max(n.created_at) AS last_node_created\n   FROM (robots r\n     LEFT JOIN nodes n ON ((n.robot_id = r.id)))\n  GROUP BY r.id, r.name\n)\n</code></pre>"},{"location":"database/public.robot_activity/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment id text true name text true total_nodes bigint true last_node_created timestamp with time zone true"},{"location":"database/public.robot_activity/#referenced-tables","title":"Referenced Tables","text":"Name Columns Comment Type public.robots 5 Registry of all LLM robots using the HTM system BASE TABLE public.nodes 14 Core memory storage for conversation messages and context BASE TABLE"},{"location":"database/public.robot_activity/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.robot_nodes/","title":"public.robot_nodes","text":""},{"location":"database/public.robot_nodes/#description","title":"Description","text":"<p>Join table connecting robots to nodes (many-to-many)</p>"},{"location":"database/public.robot_nodes/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment created_at timestamp with time zone CURRENT_TIMESTAMP true first_remembered_at timestamp with time zone CURRENT_TIMESTAMP true When this robot first remembered this content id bigint nextval('robot_nodes_id_seq'::regclass) false last_remembered_at timestamp with time zone CURRENT_TIMESTAMP true When this robot last tried to remember this content node_id bigint false public.nodes ID of the node being remembered remember_count integer 1 false Number of times this robot has tried to remember this content robot_id bigint false public.robots ID of the robot that remembered this node updated_at timestamp with time zone CURRENT_TIMESTAMP true"},{"location":"database/public.robot_nodes/#constraints","title":"Constraints","text":"Name Type Definition fk_rails_9b003078a8 FOREIGN KEY FOREIGN KEY (robot_id) REFERENCES robots(id) ON DELETE CASCADE fk_rails_f2fc98d49e FOREIGN KEY FOREIGN KEY (node_id) REFERENCES nodes(id) ON DELETE CASCADE robot_nodes_pkey PRIMARY KEY PRIMARY KEY (id)"},{"location":"database/public.robot_nodes/#indexes","title":"Indexes","text":"Name Definition idx_robot_nodes_last_remembered_at CREATE INDEX idx_robot_nodes_last_remembered_at ON public.robot_nodes USING btree (last_remembered_at) idx_robot_nodes_node_id CREATE INDEX idx_robot_nodes_node_id ON public.robot_nodes USING btree (node_id) idx_robot_nodes_robot_id CREATE INDEX idx_robot_nodes_robot_id ON public.robot_nodes USING btree (robot_id) idx_robot_nodes_unique CREATE UNIQUE INDEX idx_robot_nodes_unique ON public.robot_nodes USING btree (robot_id, node_id) robot_nodes_pkey CREATE UNIQUE INDEX robot_nodes_pkey ON public.robot_nodes USING btree (id)"},{"location":"database/public.robot_nodes/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.robots/","title":"public.robots","text":""},{"location":"database/public.robots/#description","title":"Description","text":"<p>Registry of all LLM robots using the HTM system</p>"},{"location":"database/public.robots/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment created_at timestamp with time zone CURRENT_TIMESTAMP true When the robot was first registered id bigint nextval('robots_id_seq'::regclass) false public.robot_nodes public.working_memories last_active timestamp with time zone CURRENT_TIMESTAMP true Last time the robot accessed the system name text true Human-readable name for the robot"},{"location":"database/public.robots/#constraints","title":"Constraints","text":"Name Type Definition robots_pkey PRIMARY KEY PRIMARY KEY (id)"},{"location":"database/public.robots/#indexes","title":"Indexes","text":"Name Definition robots_pkey CREATE UNIQUE INDEX robots_pkey ON public.robots USING btree (id)"},{"location":"database/public.robots/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.schema_migrations/","title":"public.schema_migrations","text":""},{"location":"database/public.schema_migrations/#description","title":"Description","text":""},{"location":"database/public.schema_migrations/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment version varchar false"},{"location":"database/public.schema_migrations/#constraints","title":"Constraints","text":"Name Type Definition schema_migrations_pkey PRIMARY KEY PRIMARY KEY (version)"},{"location":"database/public.schema_migrations/#indexes","title":"Indexes","text":"Name Definition schema_migrations_pkey CREATE UNIQUE INDEX schema_migrations_pkey ON public.schema_migrations USING btree (version)"},{"location":"database/public.schema_migrations/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.tags/","title":"public.tags","text":""},{"location":"database/public.tags/#description","title":"Description","text":"<p>Unique tag names for categorization</p>"},{"location":"database/public.tags/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment created_at timestamp with time zone CURRENT_TIMESTAMP true When this tag was created id bigint nextval('tags_id_seq'::regclass) false public.node_tags name text false Hierarchical tag in format: root:level1:level2 (e.g., database:postgresql:timescaledb)"},{"location":"database/public.tags/#constraints","title":"Constraints","text":"Name Type Definition tags_pkey PRIMARY KEY PRIMARY KEY (id)"},{"location":"database/public.tags/#indexes","title":"Indexes","text":"Name Definition idx_tags_name_pattern CREATE INDEX idx_tags_name_pattern ON public.tags USING btree (name text_pattern_ops) idx_tags_name_unique CREATE UNIQUE INDEX idx_tags_name_unique ON public.tags USING btree (name) tags_pkey CREATE UNIQUE INDEX tags_pkey ON public.tags USING btree (id)"},{"location":"database/public.tags/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.topic_relationships/","title":"public.topic_relationships","text":""},{"location":"database/public.topic_relationships/#description","title":"Description","text":"<p>Shows which topics co-occur on the same nodes, revealing cross-topic relationships in the knowledge base.</p> Table Definition <pre><code>CREATE VIEW topic_relationships AS (\n SELECT t1.tag AS topic1,\n    t2.tag AS topic2,\n    count(DISTINCT t1.node_id) AS shared_nodes\n   FROM (tags t1\n     JOIN tags t2 ON (((t1.node_id = t2.node_id) AND (t1.tag &lt; t2.tag))))\n  GROUP BY t1.tag, t2.tag\n HAVING (count(DISTINCT t1.node_id) &gt;= 2)\n  ORDER BY (count(DISTINCT t1.node_id)) DESC\n)\n</code></pre>"},{"location":"database/public.topic_relationships/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment topic1 text true topic2 text true shared_nodes bigint true"},{"location":"database/public.topic_relationships/#referenced-tables","title":"Referenced Tables","text":"Name Columns Comment Type public.tags 4 Hierarchical topic tags for flexible categorization using colon-delimited format BASE TABLE"},{"location":"database/public.topic_relationships/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"database/public.working_memories/","title":"public.working_memories","text":""},{"location":"database/public.working_memories/#description","title":"Description","text":"<p>Per-robot working memory state (optional persistence)</p>"},{"location":"database/public.working_memories/#columns","title":"Columns","text":"Name Type Default Nullable Children Parents Comment added_at timestamp with time zone CURRENT_TIMESTAMP true When node was added to working memory id bigint nextval('working_memories_id_seq'::regclass) false node_id bigint false public.nodes Node currently in working memory robot_id bigint false public.robots Robot whose working memory this belongs to token_count integer true Cached token count for budget tracking"},{"location":"database/public.working_memories/#constraints","title":"Constraints","text":"Name Type Definition fk_rails_2c1d8b383c FOREIGN KEY FOREIGN KEY (node_id) REFERENCES nodes(id) ON DELETE CASCADE fk_rails_4b7c3eb07b FOREIGN KEY FOREIGN KEY (robot_id) REFERENCES robots(id) ON DELETE CASCADE working_memories_pkey PRIMARY KEY PRIMARY KEY (id)"},{"location":"database/public.working_memories/#indexes","title":"Indexes","text":"Name Definition idx_working_memories_node_id CREATE INDEX idx_working_memories_node_id ON public.working_memories USING btree (node_id) idx_working_memories_robot_id CREATE INDEX idx_working_memories_robot_id ON public.working_memories USING btree (robot_id) idx_working_memories_unique CREATE UNIQUE INDEX idx_working_memories_unique ON public.working_memories USING btree (robot_id, node_id) working_memories_pkey CREATE UNIQUE INDEX working_memories_pkey ON public.working_memories USING btree (id)"},{"location":"database/public.working_memories/#relations","title":"Relations","text":"<p>Generated by tbls</p>"},{"location":"development/","title":"Development Guide","text":"<p>Welcome to the HTM development documentation! This guide will help you contribute to HTM, whether you're fixing bugs, adding features, or improving documentation.</p>"},{"location":"development/#about-htm-development","title":"About HTM Development","text":"<p>HTM (Hierarchical Temporary Memory) is an open-source Ruby gem that provides intelligent memory management for LLM-based applications. The project is built with modern Ruby practices, comprehensive testing, and a focus on developer experience.</p>"},{"location":"development/#project-values","title":"Project Values","text":"<ul> <li>Quality First: We prioritize correctness, performance, and maintainability</li> <li>Test-Driven: All features must have comprehensive test coverage</li> <li>Documentation Matters: Code is read more than it's written</li> <li>Community-Driven: We welcome contributions from everyone</li> <li>Never Forget: Like our memory philosophy, we preserve development history and context</li> </ul>"},{"location":"development/#development-documentation","title":"Development Documentation","text":"<p>This development guide is organized into several sections:</p>"},{"location":"development/#setup-guide","title":"Setup Guide","text":"<p>Learn how to set up your development environment, clone the repository, install dependencies, configure the database, and run the test suite.</p> <p>Topics covered:</p> <ul> <li>Cloning the repository</li> <li>Installing Ruby and dependencies</li> <li>Setting up TimescaleDB for development</li> <li>Configuring Ollama for embeddings</li> <li>Running tests and examples</li> <li>Development tools and rake tasks</li> <li>Troubleshooting common setup issues</li> </ul>"},{"location":"development/#testing-guide","title":"Testing Guide","text":"<p>Understand HTM's testing philosophy, learn how to write tests, and discover best practices for maintaining test quality.</p> <p>Topics covered:</p> <ul> <li>Test suite organization</li> <li>Running tests (unit, integration, all)</li> <li>Writing new tests</li> <li>Test helpers and fixtures</li> <li>Mocking and stubbing strategies</li> <li>Test coverage expectations</li> <li>CI/CD integration</li> <li>Testing best practices</li> </ul>"},{"location":"development/#contributing-guide","title":"Contributing Guide","text":"<p>Everything you need to know about contributing code, documentation, or bug reports to HTM.</p> <p>Topics covered:</p> <ul> <li>How to contribute (code, docs, bugs)</li> <li>Finding issues to work on</li> <li>Development workflow (fork, branch, commit, PR)</li> <li>Code style guidelines</li> <li>Commit message conventions</li> <li>Pull request process</li> <li>Code review expectations</li> <li>Documentation requirements</li> </ul>"},{"location":"development/#database-schema","title":"Database Schema","text":"<p>Deep dive into HTM's database architecture, tables, indexes, and TimescaleDB optimization strategies.</p> <p>Topics covered:</p> <ul> <li>Complete schema reference</li> <li>Entity-Relationship diagrams</li> <li>Table definitions and column details</li> <li>Indexes and constraints</li> <li>Views and functions</li> <li>TimescaleDB hypertables</li> <li>Compression policies</li> <li>Migration strategies</li> </ul>"},{"location":"development/#quick-start-for-contributors","title":"Quick Start for Contributors","text":"<p>Want to jump right in? Here's the fastest path to contributing:</p>"},{"location":"development/#1-set-up-your-environment","title":"1. Set Up Your Environment","text":"<pre><code># Clone the repository\ngit clone https://github.com/madbomber/htm.git\ncd htm\n\n# Install dependencies\nbundle install\n\n# Configure database (see setup guide)\nsource ~/.bashrc__tiger\n\n# Verify setup\nrake db_test\n</code></pre>"},{"location":"development/#2-run-the-tests","title":"2. Run the Tests","text":"<pre><code># Run all tests\nrake test\n\n# Run specific test file\nruby test/htm_test.rb\n\n# Run integration tests\nruby test/integration_test.rb\n</code></pre>"},{"location":"development/#3-make-your-changes","title":"3. Make Your Changes","text":"<pre><code># Create a feature branch\ngit checkout -b feature/your-feature-name\n\n# Make your changes\n# ... edit files ...\n\n# Run tests to verify\nrake test\n\n# Commit with descriptive message\ngit commit -m \"Add feature: your feature description\"\n</code></pre>"},{"location":"development/#4-submit-a-pull-request","title":"4. Submit a Pull Request","text":"<pre><code># Push to your fork\ngit push origin feature/your-feature-name\n\n# Open a pull request on GitHub\n# Include description of changes and any related issues\n</code></pre> <p>See the Contributing Guide for detailed instructions.</p>"},{"location":"development/#development-workflow","title":"Development Workflow","text":"<p>HTM follows a streamlined development workflow:</p>"},{"location":"development/#branch-strategy","title":"Branch Strategy","text":"<ul> <li><code>main</code>: Stable, production-ready code</li> <li>Feature branches: <code>feature/description</code> for new features</li> <li>Bug fix branches: <code>fix/description</code> for bug fixes</li> <li>Documentation branches: <code>docs/description</code> for documentation updates</li> </ul>"},{"location":"development/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork and clone the repository</li> <li>Create a branch from <code>main</code></li> <li>Make your changes with tests</li> <li>Run the test suite to verify</li> <li>Push to your fork and create a PR</li> <li>Respond to review feedback and iterate</li> <li>Merge after approval</li> </ol>"},{"location":"development/#code-review-standards","title":"Code Review Standards","text":"<p>All pull requests must:</p> <ul> <li>Pass the test suite (100% pass rate)</li> <li>Maintain or improve test coverage</li> <li>Follow Ruby style guidelines</li> <li>Include documentation updates</li> <li>Have clear commit messages</li> <li>Be reviewed by at least one maintainer</li> </ul>"},{"location":"development/#getting-help","title":"Getting Help","text":""},{"location":"development/#documentation","title":"Documentation","text":"<ul> <li>User Guide: Learn how to use HTM</li> <li>API Reference: Detailed API documentation</li> <li>Architecture Docs: System design and architecture</li> </ul>"},{"location":"development/#community-resources","title":"Community Resources","text":"<ul> <li>GitHub Issues: https://github.com/madbomber/htm/issues</li> <li>GitHub Discussions: Ask questions and share ideas</li> <li>Planning Document: See <code>htm_teamwork.md</code> for design decisions and rationale</li> </ul>"},{"location":"development/#common-questions","title":"Common Questions","text":"<p>Q: Where do I start if I'm new to the project?</p> <p>A: Check out issues labeled <code>good-first-issue</code> on GitHub. These are specifically chosen to be approachable for new contributors.</p> <p>Q: How do I run tests without a database?</p> <p>A: Unit tests (like <code>test/htm_test.rb</code>) don't require a database. Integration tests require TimescaleDB. See the Testing Guide for details.</p> <p>Q: What's the preferred debugging approach?</p> <p>A: HTM uses the <code>debug_me</code> gem for debugging. See examples in the codebase and avoid using <code>puts</code> for debugging.</p> <p>Q: How do I add new database columns or tables?</p> <p>A: See the Database Schema guide for migration strategies and best practices.</p>"},{"location":"development/#code-of-conduct","title":"Code of Conduct","text":"<p>HTM is committed to providing a welcoming and inclusive environment for all contributors. We expect all participants to:</p> <ul> <li>Be respectful: Treat everyone with respect and kindness</li> <li>Be inclusive: Welcome diverse perspectives and backgrounds</li> <li>Be collaborative: Work together toward common goals</li> <li>Be professional: Keep discussions focused and constructive</li> <li>Be patient: Remember that everyone was a beginner once</li> </ul> <p>Unacceptable behavior includes harassment, discrimination, or any conduct that creates an unsafe or unwelcoming environment.</p>"},{"location":"development/#development-philosophy","title":"Development Philosophy","text":"<p>HTM's development is guided by several key principles:</p>"},{"location":"development/#never-forget-but-evolve","title":"Never Forget (But Evolve)","text":"<p>Just like HTM's memory philosophy, we preserve development history and context. However, we're not afraid to evolve and improve:</p> <ul> <li>Git history is sacred - no force pushes to <code>main</code></li> <li>Deprecate before removing - give users time to adapt</li> <li>Document breaking changes clearly</li> <li>Maintain backward compatibility when possible</li> </ul>"},{"location":"development/#test-everything","title":"Test Everything","text":"<p>Every feature must have comprehensive tests:</p> <ul> <li>Unit tests: Test individual methods in isolation</li> <li>Integration tests: Test full workflows with real database</li> <li>Edge cases: Test error conditions and boundaries</li> <li>Performance: Monitor memory usage and query performance</li> </ul>"},{"location":"development/#document-as-you-go","title":"Document as You Go","text":"<p>Documentation is part of the feature:</p> <ul> <li>Add inline comments for complex logic</li> <li>Update API docs for public methods</li> <li>Include examples in documentation</li> <li>Update guides when behavior changes</li> </ul>"},{"location":"development/#iterate-and-improve","title":"Iterate and Improve","text":"<p>We value continuous improvement:</p> <ul> <li>Refactor for clarity and performance</li> <li>Review and update old code</li> <li>Learn from mistakes and share lessons</li> <li>Celebrate improvements, no matter how small</li> </ul>"},{"location":"development/#project-structure","title":"Project Structure","text":"<p>Understanding the codebase structure:</p> <p></p>"},{"location":"development/#tools-and-technologies","title":"Tools and Technologies","text":"<p>HTM is built with modern Ruby tools:</p>"},{"location":"development/#core-technologies","title":"Core Technologies","text":"<ul> <li>Ruby 3.0+: Modern Ruby with pattern matching and better performance</li> <li>PostgreSQL 17: Robust relational database</li> <li>TimescaleDB: Time-series optimization for PostgreSQL</li> <li>pgvector: Vector similarity search</li> <li>RubyLLM: LLM client library for embeddings</li> <li>Ollama: Local embedding generation</li> </ul>"},{"location":"development/#development-tools","title":"Development Tools","text":"<ul> <li>Minitest: Testing framework</li> <li>Minitest Reporters: Beautiful test output</li> <li>Rake: Task automation</li> <li>Bundler: Dependency management</li> <li>debug_me: Debugging utility</li> <li>MkDocs: Documentation generation</li> </ul>"},{"location":"development/#optional-tools","title":"Optional Tools","text":"<ul> <li>VSCode: Popular editor with Ruby extensions</li> <li>RuboCop: Ruby style checker (future)</li> <li>SimpleCov: Code coverage (future)</li> <li>YARD: Documentation generator (future)</li> </ul>"},{"location":"development/#next-steps","title":"Next Steps","text":"<p>Ready to contribute? Here's where to go next:</p> <ol> <li>Setup Your Environment: Clone the repo and get everything running</li> <li>Understand the Tests: Learn how to write and run tests</li> <li>Read Contributing Guidelines: Learn our workflow and standards</li> <li>Explore the Schema: Understand the database architecture</li> </ol>"},{"location":"development/#thank-you","title":"Thank You","text":"<p>Thank you for your interest in contributing to HTM! Every contribution, whether it's code, documentation, bug reports, or ideas, helps make HTM better for everyone.</p> <p>We look forward to working with you!</p> <p>Maintained by: Dewayne VanHoozer</p> <p>License: MIT License</p>"},{"location":"development/contributing/","title":"Contributing Guide","text":"<p>Thank you for your interest in contributing to HTM! This guide will help you understand how to contribute effectively.</p>"},{"location":"development/contributing/#welcome-contributors","title":"Welcome Contributors","text":"<p>We welcome contributions from everyone, regardless of experience level. Whether you're fixing a typo, improving documentation, reporting a bug, or implementing a major feature, your contribution is valued.</p>"},{"location":"development/contributing/#types-of-contributions","title":"Types of Contributions","text":"<p>We welcome many types of contributions:</p> <ul> <li>Bug reports: Help us identify and fix issues</li> <li>Feature requests: Suggest new capabilities</li> <li>Documentation: Improve guides, API docs, or examples</li> <li>Code: Fix bugs or implement features</li> <li>Tests: Improve test coverage</li> <li>Performance: Optimize slow code</li> <li>Design: Improve architecture or API design</li> </ul>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#before-you-begin","title":"Before You Begin","text":"<ol> <li>Check existing issues: Search GitHub Issues to see if your idea or bug is already reported</li> <li>Read the docs: Familiarize yourself with HTM by reading the User Guide and API Reference</li> <li>Set up your environment: Follow the Setup Guide to get HTM running locally</li> </ol>"},{"location":"development/contributing/#finding-issues-to-work-on","title":"Finding Issues to Work On","text":"<p>Good places to start:</p> <ul> <li><code>good-first-issue</code> label: Issues specifically chosen for new contributors</li> <li><code>help-wanted</code> label: Issues where we need community help</li> <li><code>documentation</code> label: Documentation improvements</li> <li><code>bug</code> label: Bug fixes (clearly defined scope)</li> </ul> <p>Browse issues at: https://github.com/madbomber/htm/issues</p>"},{"location":"development/contributing/#claiming-an-issue","title":"Claiming an Issue","text":"<p>Before starting work:</p> <ol> <li>Comment on the issue: Let others know you're working on it</li> <li>Wait for acknowledgment: A maintainer will confirm the approach</li> <li>Ask questions: If anything is unclear, ask in the issue</li> </ol> <p>This prevents duplicate work and ensures your approach aligns with the project.</p>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":""},{"location":"development/contributing/#fork-the-repository","title":"Fork the Repository","text":"<ol> <li>Visit https://github.com/madbomber/htm</li> <li>Click \"Fork\" in the upper right</li> <li>This creates a copy under your GitHub account</li> </ol>"},{"location":"development/contributing/#clone-your-fork","title":"Clone Your Fork","text":"<pre><code>git clone https://github.com/YOUR_USERNAME/htm.git\ncd htm\n</code></pre>"},{"location":"development/contributing/#add-upstream-remote","title":"Add Upstream Remote","text":"<pre><code>git remote add upstream https://github.com/madbomber/htm.git\ngit fetch upstream\n</code></pre>"},{"location":"development/contributing/#2-create-a-branch","title":"2. Create a Branch","text":"<p>Always work in a feature branch, never directly on <code>main</code>:</p> <pre><code># Sync with upstream first\ngit checkout main\ngit pull upstream main\n\n# Create and switch to feature branch\ngit checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#branch-naming-conventions","title":"Branch Naming Conventions","text":"<p>Use descriptive branch names with prefixes:</p> <ul> <li>Features: <code>feature/add-compression-policy</code></li> <li>Bug fixes: <code>fix/recall-timeframe-parsing</code></li> <li>Documentation: <code>docs/improve-setup-guide</code></li> <li>Refactoring: <code>refactor/simplify-embedding-service</code></li> <li>Performance: <code>perf/optimize-vector-search</code></li> </ul> <p>Examples:</p> <pre><code>git checkout -b feature/add-hybrid-search\ngit checkout -b fix/working-memory-overflow\ngit checkout -b docs/add-examples\n</code></pre>"},{"location":"development/contributing/#3-make-your-changes","title":"3. Make Your Changes","text":""},{"location":"development/contributing/#code-changes","title":"Code Changes","text":"<p>Follow these guidelines:</p> <p>File Organization:</p> <ul> <li>Keep methods focused and testable in isolation</li> <li>Use clear, descriptive method and variable names</li> <li>Add comments for complex logic</li> <li>Follow existing code style</li> </ul> <p>Error Handling:</p> <pre><code># Good: Specific error with helpful message\nraise ArgumentError, \"importance must be between 0 and 10, got #{importance}\"\n\n# Bad: Generic error\nraise \"Bad importance\"\n</code></pre> <p>Debugging:</p> <pre><code># Good: Use debug_me gem\nrequire 'debug_me'\n\ndef process(value)\n  debug_me { [ :value ] }\n  # Process value...\nend\n\n# Bad: Don't use puts\ndef process(value)\n  puts \"Value: #{value}\"  # Don't do this\nend\n</code></pre> <p>Method Testing:</p> <p>Every method, public or private, must be easily testable in isolation:</p> <pre><code># Good: Testable method\ndef calculate_importance(factors)\n  base_score = factors[:recency] * 0.4\n  relevance_score = factors[:relevance] * 0.6\n  base_score + relevance_score\nend\n\n# This can be tested without side effects\n</code></pre>"},{"location":"development/contributing/#documentation-changes","title":"Documentation Changes","text":"<p>When updating documentation:</p> <ul> <li>Use clear, concise language</li> <li>Include code examples for features</li> <li>Add diagrams for complex concepts</li> <li>Use Material for MkDocs formatting</li> <li>Check for spelling and grammar</li> </ul>"},{"location":"development/contributing/#add-tests","title":"Add Tests","text":"<p>All code changes must include tests. See the Testing Guide for details.</p> <pre><code># test/your_feature_test.rb\nrequire \"test_helper\"\n\nclass YourFeatureTest &lt; Minitest::Test\n  def test_your_feature_works\n    result = YourClass.your_method(\"input\")\n    assert_equal \"expected\", result\n  end\n\n  def test_handles_edge_cases\n    assert_raises(ArgumentError) do\n      YourClass.your_method(nil)\n    end\n  end\nend\n</code></pre>"},{"location":"development/contributing/#4-run-tests","title":"4. Run Tests","text":"<p>Before committing, ensure all tests pass:</p> <pre><code># Run all tests\nrake test\n\n# Run specific test file\nruby test/your_feature_test.rb\n\n# Run with verbose output\nrake test TESTOPTS=\"-v\"\n</code></pre> <p>All tests must pass before submitting a pull request.</p>"},{"location":"development/contributing/#5-commit-your-changes","title":"5. Commit Your Changes","text":""},{"location":"development/contributing/#commit-message-format","title":"Commit Message Format","text":"<p>We follow a simple commit message convention:</p> <pre><code>&lt;type&gt;: &lt;subject&gt;\n\n&lt;optional body&gt;\n\n&lt;optional footer&gt;\n</code></pre> <p>Types:</p> <ul> <li><code>feat</code>: New feature</li> <li><code>fix</code>: Bug fix</li> <li><code>docs</code>: Documentation changes</li> <li><code>test</code>: Adding or updating tests</li> <li><code>refactor</code>: Code refactoring (no behavior change)</li> <li><code>perf</code>: Performance improvement</li> <li><code>style</code>: Code style changes (formatting, etc.)</li> <li><code>chore</code>: Maintenance tasks (dependencies, build, etc.)</li> </ul> <p>Examples:</p> <pre><code># Good commit messages\ngit commit -m \"feat: add hybrid search combining vector and fulltext\"\ngit commit -m \"fix: handle nil values in recall timeframe parsing\"\ngit commit -m \"docs: add examples for relationship queries\"\ngit commit -m \"test: add integration tests for working memory eviction\"\ngit commit -m \"refactor: extract embedding generation to separate method\"\n\n# Bad commit messages\ngit commit -m \"updates\"\ngit commit -m \"fix stuff\"\ngit commit -m \"wip\"\n</code></pre>"},{"location":"development/contributing/#multi-line-commit-messages","title":"Multi-line Commit Messages","text":"<p>For more complex changes, include a body:</p> <pre><code>git commit -m \"feat: add compression policy for old memories\n\nImplements automatic compression for memories older than 30 days\nusing TimescaleDB compression policies. This reduces storage costs\nand improves query performance for recent data.\n\n- Adds compress_old_memories rake task\n- Updates schema with compression settings\n- Adds tests for compression policy\n- Documents compression in schema guide\n\nCloses #123\"\n</code></pre>"},{"location":"development/contributing/#atomic-commits","title":"Atomic Commits","text":"<p>Make commits focused and atomic:</p> <pre><code># Good: Focused commits\ngit add lib/htm/compression.rb test/compression_test.rb\ngit commit -m \"feat: add memory compression for old data\"\n\ngit add docs/development/schema.md\ngit commit -m \"docs: document compression policy in schema guide\"\n\n# Bad: Kitchen sink commit\ngit add .\ngit commit -m \"Add compression and fix bugs and update docs\"\n</code></pre>"},{"location":"development/contributing/#6-push-to-your-fork","title":"6. Push to Your Fork","text":"<pre><code># Push your feature branch\ngit push origin feature/your-feature-name\n</code></pre> <p>If you make additional changes:</p> <pre><code># Make changes\ngit add .\ngit commit -m \"fix: address review feedback\"\ngit push origin feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#7-create-a-pull-request","title":"7. Create a Pull Request","text":""},{"location":"development/contributing/#open-the-pull-request","title":"Open the Pull Request","text":"<ol> <li>Visit your fork on GitHub</li> <li>Click \"Compare &amp; pull request\"</li> <li>Ensure base is <code>madbomber/htm:main</code> and compare is <code>YOUR_USERNAME/htm:feature/your-feature-name</code></li> <li>Fill out the pull request template</li> </ol>"},{"location":"development/contributing/#pull-request-title","title":"Pull Request Title","text":"<p>Use the same format as commit messages:</p> <pre><code>feat: add hybrid search combining vector and fulltext\nfix: handle nil values in recall timeframe parsing\ndocs: improve setup guide with troubleshooting section\n</code></pre>"},{"location":"development/contributing/#pull-request-description","title":"Pull Request Description","text":"<p>Provide a clear description:</p> <pre><code>## Summary\n\nThis PR adds hybrid search functionality that combines vector similarity\nsearch with PostgreSQL full-text search for better recall accuracy.\n\n## Changes\n\n- Add `HybridSearch` class in `lib/htm/hybrid_search.rb`\n- Update `recall` method to support `:hybrid` strategy\n- Add integration tests for hybrid search\n- Document hybrid search in user guide\n\n## Testing\n\n- All existing tests pass\n- Added 12 new tests for hybrid search\n- Tested with 10,000+ memories in database\n\n## Related Issues\n\nCloses #123\nRelated to #100\n\n## Screenshots (if applicable)\n\nN/A\n\n## Checklist\n\n- [x] Tests pass locally\n- [x] Added tests for new functionality\n- [x] Updated documentation\n- [x] Followed code style guidelines\n- [x] No breaking changes (or documented if necessary)\n</code></pre>"},{"location":"development/contributing/#8-respond-to-review-feedback","title":"8. Respond to Review Feedback","text":"<p>Maintainers will review your pull request and may request changes.</p>"},{"location":"development/contributing/#making-changes-after-review","title":"Making Changes After Review","text":"<pre><code># Make requested changes\ngit add .\ngit commit -m \"fix: address review feedback - improve error handling\"\ngit push origin feature/your-feature-name\n</code></pre> <p>The pull request will update automatically.</p>"},{"location":"development/contributing/#discuss-and-iterate","title":"Discuss and Iterate","text":"<ul> <li>Ask questions: If feedback is unclear, ask for clarification</li> <li>Explain your approach: If you disagree, explain your reasoning respectfully</li> <li>Be patient: Reviews take time, especially for large PRs</li> <li>Be open: Reviewers have project context you might not have</li> </ul>"},{"location":"development/contributing/#9-merge","title":"9. Merge","text":"<p>Once approved, a maintainer will merge your pull request. Congratulations!</p>"},{"location":"development/contributing/#after-merging","title":"After Merging","text":"<pre><code># Sync your fork with upstream\ngit checkout main\ngit pull upstream main\ngit push origin main\n\n# Delete your feature branch (optional)\ngit branch -d feature/your-feature-name\ngit push origin --delete feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#code-style-guidelines","title":"Code Style Guidelines","text":""},{"location":"development/contributing/#ruby-style","title":"Ruby Style","text":"<p>HTM follows standard Ruby conventions:</p>"},{"location":"development/contributing/#general-style","title":"General Style","text":"<ul> <li>Indentation: 2 spaces (no tabs)</li> <li>Line length: Max 100 characters (prefer 80)</li> <li>String literals: Use double quotes <code>\"string\"</code> for most strings</li> <li>Frozen strings: Add <code># frozen_string_literal: true</code> at the top of files</li> </ul>"},{"location":"development/contributing/#naming-conventions","title":"Naming Conventions","text":"<pre><code># Classes and modules: CamelCase\nclass WorkingMemory\n  module Helpers\n  end\nend\n\n# Methods and variables: snake_case\ndef calculate_token_count\n  max_tokens = 128_000\nend\n\n# Constants: SCREAMING_SNAKE_CASE\nMAX_WORKING_MEMORY_SIZE = 128_000\n\n# Private methods: prefix with private keyword\nclass MyClass\n  def public_method\n  end\n\n  private\n\n  def private_helper\n  end\nend\n</code></pre>"},{"location":"development/contributing/#method-definitions","title":"Method Definitions","text":"<pre><code># Good: Clear parameter names\ndef add_node(key, value, type:, importance: 1.0, tags: [])\n  # Implementation\nend\n\n# Good: Guard clauses at the top\ndef process(value)\n  return unless value\n  raise ArgumentError, \"value too large\" if value &gt; MAX_SIZE\n\n  # Main logic\nend\n\n# Good: Testable in isolation\ndef calculate_score(importance, recency)\n  (importance * 0.6) + (recency * 0.4)\nend\n</code></pre>"},{"location":"development/contributing/#error-messages","title":"Error Messages","text":"<pre><code># Good: Specific, actionable error messages\nraise ArgumentError, \"importance must be between 0 and 10, got #{importance}\"\nraise HTM::DatabaseError, \"Failed to connect to database at #{url}: #{error.message}\"\n\n# Bad: Vague errors\nraise \"Bad value\"\nraise StandardError\n</code></pre>"},{"location":"development/contributing/#documentation-style","title":"Documentation Style","text":""},{"location":"development/contributing/#code-comments","title":"Code Comments","text":"<pre><code># Good: Explain WHY, not WHAT\n# Use recency score to prioritize recent memories over old ones\n# This prevents the context window from being filled with stale data\nrecency_score = calculate_recency(timestamp)\n\n# Bad: Obvious comment\n# Calculate recency score\nrecency_score = calculate_recency(timestamp)\n</code></pre>"},{"location":"development/contributing/#method-documentation-future-yard","title":"Method Documentation (Future: YARD)","text":"<pre><code># Calculates the importance score based on recency and relevance.\n#\n# @param importance [Float] Base importance (0-10)\n# @param recency [Float] Recency factor (0-1)\n# @return [Float] Combined score\n# @raise [ArgumentError] If importance is out of range\ndef calculate_score(importance, recency)\n  # Implementation\nend\n</code></pre>"},{"location":"development/contributing/#git-style","title":"Git Style","text":""},{"location":"development/contributing/#branch-names","title":"Branch Names","text":"<ul> <li>Use lowercase with hyphens</li> <li>Use descriptive names</li> <li>Include type prefix</li> </ul> <pre><code># Good\nfeature/add-compression\nfix/recall-parsing\ndocs/setup-guide\n\n# Bad\nMyFeature\nfix\nbranch1\n</code></pre>"},{"location":"development/contributing/#commit-messages","title":"Commit Messages","text":"<ul> <li>Use imperative mood (\"add feature\" not \"added feature\")</li> <li>Keep first line under 72 characters</li> <li>Add body for complex changes</li> <li>Reference issues when applicable</li> </ul> <pre><code># Good\nfeat: add hybrid search for better recall\nfix: prevent working memory overflow\ndocs: improve contributing guide\n\n# Bad\nAdded stuff\nFixed\nUpdate\n</code></pre>"},{"location":"development/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"development/contributing/#before-submitting","title":"Before Submitting","text":"<p>Ensure your pull request:</p> <ul> <li> Passes all tests - Run <code>rake test</code></li> <li> Includes new tests - For new features or bug fixes</li> <li> Updates documentation - If behavior changes</li> <li> Follows code style - Consistent with existing code</li> <li> Has clear commits - Follow commit message guidelines</li> <li> Is focused - Solves one problem or adds one feature</li> <li> No merge conflicts - Rebase if needed</li> </ul>"},{"location":"development/contributing/#pull-request-checklist","title":"Pull Request Checklist","text":"<p>Include this checklist in your PR description:</p> <pre><code>## Checklist\n\n- [ ] Tests pass locally (`rake test`)\n- [ ] Added tests for new functionality\n- [ ] Updated documentation (if applicable)\n- [ ] Followed code style guidelines\n- [ ] Commits follow message conventions\n- [ ] No breaking changes (or documented clearly)\n- [ ] Referenced related issues\n</code></pre>"},{"location":"development/contributing/#pull-request-size","title":"Pull Request Size","text":"<p>Keep pull requests manageable:</p> <ul> <li>Small PRs: &lt;200 lines changed (ideal)</li> <li>Medium PRs: 200-500 lines (acceptable)</li> <li>Large PRs: &gt;500 lines (split if possible)</li> </ul> <p>Large PRs are harder to review and more likely to have issues. Consider splitting into smaller, incremental changes.</p>"},{"location":"development/contributing/#handling-merge-conflicts","title":"Handling Merge Conflicts","text":"<p>If your branch conflicts with <code>main</code>:</p> <pre><code># Sync with upstream\ngit fetch upstream\n\n# Rebase your branch\ngit checkout feature/your-feature\ngit rebase upstream/main\n\n# Resolve conflicts\n# ... edit files ...\ngit add .\ngit rebase --continue\n\n# Force push (your branch only!)\ngit push origin feature/your-feature --force\n</code></pre>"},{"location":"development/contributing/#code-review-process","title":"Code Review Process","text":""},{"location":"development/contributing/#what-to-expect","title":"What to Expect","text":"<ol> <li>Initial review: Within 3-5 days (usually faster)</li> <li>Feedback: Comments on code, tests, or documentation</li> <li>Iteration: Make requested changes</li> <li>Approval: Once all feedback is addressed</li> <li>Merge: Maintainer merges the PR</li> </ol>"},{"location":"development/contributing/#review-criteria","title":"Review Criteria","text":"<p>Reviewers check for:</p> <ul> <li>Correctness: Does it work as intended?</li> <li>Tests: Are there adequate tests?</li> <li>Style: Does it follow our conventions?</li> <li>Documentation: Are changes documented?</li> <li>Design: Does it fit the architecture?</li> <li>Performance: Are there performance concerns?</li> <li>Security: Are there security implications?</li> </ul>"},{"location":"development/contributing/#responding-to-reviews","title":"Responding to Reviews","text":"<ul> <li>Be responsive: Reply to comments promptly</li> <li>Be respectful: Assume good intent</li> <li>Be open: Consider feedback carefully</li> <li>Ask questions: If anything is unclear</li> <li>Explain reasoning: If you disagree with feedback</li> </ul>"},{"location":"development/contributing/#documentation-requirements","title":"Documentation Requirements","text":""},{"location":"development/contributing/#when-to-update-docs","title":"When to Update Docs","text":"<p>Update documentation if you:</p> <ul> <li>Add a new feature</li> <li>Change existing behavior</li> <li>Add or change public API methods</li> <li>Fix a bug that affects documented behavior</li> <li>Improve performance characteristics</li> </ul>"},{"location":"development/contributing/#documentation-locations","title":"Documentation Locations","text":"<ul> <li>User Guide: <code>docs/guides/getting-started.md</code> - How to use the feature</li> <li>API Reference: <code>docs/api/</code> - Method signatures and parameters</li> <li>Development Guide: <code>docs/development/</code> - Developer information</li> <li>README: <code>README.md</code> - High-level overview</li> <li>Code Comments: Inline documentation for complex logic</li> </ul>"},{"location":"development/contributing/#documentation-style_1","title":"Documentation Style","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Add warnings for edge cases</li> <li>Use Material for MkDocs formatting</li> <li>Spell check your writing</li> </ul> <p>Example:</p> <pre><code>### Hybrid Search\n\nHTM's hybrid search combines vector similarity search with full-text search for improved recall accuracy.\n\n**Usage:**\n\n```ruby\nmemories = htm.recall(\n  topic: \"database decisions\",\n  strategy: :hybrid,\n  timeframe: \"last week\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>topic</code> (String): Search query</li> <li><code>strategy</code> (Symbol): Must be <code>:hybrid</code></li> <li><code>timeframe</code> (String): Optional time range</li> </ul> <p>Returns:</p> <p>Array of memory hashes with relevance scores.</p> <p>Warning</p> <p>Hybrid search requires Ollama to be running for embedding generation.</p> <p>```</p>"},{"location":"development/contributing/#release-process","title":"Release Process","text":""},{"location":"development/contributing/#versioning","title":"Versioning","text":"<p>HTM uses Semantic Versioning:</p> <ul> <li>MAJOR.MINOR.PATCH (e.g., <code>1.2.3</code>)</li> <li>MAJOR: Breaking changes</li> <li>MINOR: New features (backward compatible)</li> <li>PATCH: Bug fixes (backward compatible)</li> </ul>"},{"location":"development/contributing/#release-checklist-maintainers","title":"Release Checklist (Maintainers)","text":"<ol> <li>Update <code>lib/htm/version.rb</code></li> <li>Update <code>CHANGELOG.md</code> (future)</li> <li>Run full test suite</li> <li>Create git tag</li> <li>Push to RubyGems</li> <li>Create GitHub release</li> </ol> <p>Contributors don't need to worry about releases - maintainers handle this.</p>"},{"location":"development/contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming, inclusive environment:</p> <ul> <li>Be respectful: Treat everyone with respect</li> <li>Be inclusive: Welcome diverse perspectives</li> <li>Be collaborative: Work together constructively</li> <li>Be professional: Keep discussions focused</li> <li>Be patient: Help others learn</li> </ul>"},{"location":"development/contributing/#communication-channels","title":"Communication Channels","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>GitHub Discussions: Questions and general discussion</li> <li>Pull Requests: Code review and collaboration</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<p>If you're stuck:</p> <ol> <li>Check the docs: Review guides and API reference</li> <li>Search issues: Someone might have had the same problem</li> <li>Ask in discussions: Post your question</li> <li>Be specific: Include error messages, code samples, versions</li> </ol>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in several ways:</p> <ul> <li>Listed in <code>CONTRIBUTORS.md</code> (future)</li> <li>Mentioned in release notes</li> <li>GitHub contributor badge</li> <li>Our gratitude and appreciation!</li> </ul>"},{"location":"development/contributing/#additional-resources","title":"Additional Resources","text":""},{"location":"development/contributing/#project-documentation","title":"Project Documentation","text":"<ul> <li>Setup Guide: Set up development environment</li> <li>Testing Guide: Write and run tests</li> <li>Schema Documentation: Database architecture</li> <li>Architecture Overview: System design</li> </ul>"},{"location":"development/contributing/#external-resources","title":"External Resources","text":"<ul> <li>Ruby Style Guide: https://rubystyle.guide/</li> <li>Git Best Practices: https://git-scm.com/book/en/v2</li> <li>Semantic Versioning: https://semver.org/</li> <li>Conventional Commits: https://www.conventionalcommits.org/</li> </ul>"},{"location":"development/contributing/#questions","title":"Questions?","text":"<p>If you have questions about contributing:</p> <ul> <li>Open a GitHub Discussion</li> <li>Comment on a related issue</li> <li>Review <code>htm_teamwork.md</code> for design context</li> </ul>"},{"location":"development/contributing/#thank-you","title":"Thank You!","text":"<p>Thank you for contributing to HTM! Your efforts help make HTM better for everyone. We appreciate your time, expertise, and collaboration.</p> <p>Happy coding!</p> <p>Maintained by: Dewayne VanHoozer</p> <p>License: MIT License</p>"},{"location":"development/schema/","title":"Database Schema Documentation","text":"<p>This document provides a comprehensive reference for HTM's PostgreSQL database schema, including query patterns, optimization strategies, and best practices.</p>"},{"location":"development/schema/#schema-overview","title":"Schema Overview","text":"<p>HTM uses PostgreSQL 17 with pgvector and pg_trgm extensions to provide:</p> <ul> <li>Vector similarity search via pgvector for semantic memory retrieval</li> <li>Full-text search with PostgreSQL's built-in tsvector capabilities</li> <li>Fuzzy matching using pg_trgm for flexible text search</li> <li>Many-to-many relationships for flexible tagging and categorization</li> </ul>"},{"location":"development/schema/#required-extensions","title":"Required Extensions","text":"<p>HTM requires these PostgreSQL extensions:</p> <pre><code>CREATE EXTENSION IF NOT EXISTS pg_trgm WITH SCHEMA public;\nCREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA public;\n</code></pre>"},{"location":"development/schema/#entity-relationship-diagram","title":"Entity-Relationship Diagram","text":"<p>Here's the complete database structure (auto-generated by tbls):</p> <p></p>"},{"location":"development/schema/#table-reference","title":"Table Reference","text":"<p>For detailed table definitions, columns, indexes, and constraints, see the auto-generated documentation:</p>"},{"location":"development/schema/#core-tables","title":"Core Tables","text":"Table Description Details robots Registry of all LLM robots using the HTM system Stores robot metadata and activity tracking nodes Core memory storage for conversation messages and context Vector embeddings, full-text search, deduplication tags Unique hierarchical tag names for categorization Colon-separated namespaces (e.g., <code>ai:llm:embeddings</code>) working_memories Per-robot working memory state Optional persistence for token-limited context file_sources Source file metadata for loaded documents Path, mtime, frontmatter, sync tracking"},{"location":"development/schema/#join-tables","title":"Join Tables","text":"Table Description Details robot_nodes Links robots to nodes (many-to-many) Enables \"hive mind\" shared memory architecture node_tags Links nodes to tags (many-to-many) Flexible multi-tag categorization"},{"location":"development/schema/#system-tables","title":"System Tables","text":"Table Description Details schema_migrations ActiveRecord migration tracking Tracks applied migrations <p>For the complete schema overview including all stored procedures and functions, see the Database Tables Overview.</p>"},{"location":"development/schema/#key-concepts","title":"Key Concepts","text":""},{"location":"development/schema/#content-deduplication","title":"Content Deduplication","text":"<p>Content deduplication is enforced via SHA-256 hashing in the <code>nodes</code> table:</p> <ol> <li>When <code>remember()</code> is called, a SHA-256 hash of the content is computed</li> <li>If a node with the same <code>content_hash</code> exists, the existing node is reused</li> <li>A new <code>robot_nodes</code> association is created (or updated if it already exists)</li> <li>This ensures identical memories are stored once but can be \"remembered\" by multiple robots</li> </ol>"},{"location":"development/schema/#jsonb-metadata","title":"JSONB Metadata","text":"<p>The <code>nodes</code> table includes a <code>metadata</code> JSONB column for flexible key-value storage:</p> Column Type Default Description <code>metadata</code> jsonb <code>{}</code> Arbitrary key-value data <p>Features: - Stores any valid JSON data (strings, numbers, booleans, arrays, objects) - GIN index (<code>idx_nodes_metadata</code>) for efficient containment queries - Queried using PostgreSQL's <code>@&gt;</code> containment operator</p> <p>Query examples: <pre><code>-- Find nodes with specific metadata\nSELECT * FROM nodes WHERE metadata @&gt; '{\"priority\": \"high\"}'::jsonb;\n\n-- Find nodes with nested metadata\nSELECT * FROM nodes WHERE metadata @&gt; '{\"user\": {\"role\": \"admin\"}}'::jsonb;\n\n-- Find nodes with multiple conditions\nSELECT * FROM nodes WHERE metadata @&gt; '{\"environment\": \"production\", \"version\": 2}'::jsonb;\n</code></pre></p> <p>Ruby usage: <pre><code># Store with metadata\nhtm.remember(\"API config\", metadata: { environment: \"production\", version: 2 })\n\n# Recall filtering by metadata\nhtm.recall(\"config\", metadata: { environment: \"production\" })\n</code></pre></p>"},{"location":"development/schema/#hierarchical-tags","title":"Hierarchical Tags","text":"<p>Tags use colon-separated hierarchies for organization: - <code>programming:ruby:gems</code> - Programming &gt; Ruby &gt; Gems - <code>database:postgresql:extensions</code> - Database &gt; PostgreSQL &gt; Extensions - <code>ai:llm:embeddings</code> - AI &gt; LLM &gt; Embeddings</p> <p>Query by prefix to find all related tags: <pre><code>SELECT * FROM tags WHERE name LIKE 'database:%';  -- All database-related tags\nSELECT * FROM tags WHERE name LIKE 'ai:llm:%';    -- All LLM-related tags\n</code></pre></p>"},{"location":"development/schema/#file-source-tracking","title":"File Source Tracking","text":"<p>The <code>file_sources</code> table tracks loaded documents for re-sync support:</p> Column Type Description <code>id</code> bigint Primary key <code>file_path</code> text Absolute path to the source file <code>file_hash</code> varchar(64) SHA-256 hash of file contents <code>mtime</code> timestamptz File modification time for change detection <code>file_size</code> integer File size in bytes <code>frontmatter</code> jsonb Parsed YAML frontmatter metadata <code>last_synced_at</code> timestamptz When file was last synced <code>created_at</code> timestamptz When source was first loaded <code>updated_at</code> timestamptz When source was last updated <p>Nodes loaded from files have: - <code>source_id</code> - Foreign key to file_sources (nullable, ON DELETE SET NULL) - <code>chunk_position</code> - Integer position within the file (0-indexed)</p> <p>Query nodes from a file: <pre><code>SELECT n.*\nFROM nodes n\nJOIN file_sources fs ON n.source_id = fs.id\nWHERE fs.file_path = '/path/to/file.md'\nORDER BY n.chunk_position;\n</code></pre></p>"},{"location":"development/schema/#remember-tracking","title":"Remember Tracking","text":"<p>The <code>robot_nodes</code> table tracks per-robot remember metadata:</p> <ol> <li><code>first_remembered_at</code> - When this robot first encountered this content</li> <li><code>last_remembered_at</code> - Updated each time the robot tries to remember the same content</li> <li><code>remember_count</code> - Incremented each time (useful for identifying frequently reinforced memories)</li> </ol> <p>This allows querying for: - Recently reinforced memories: <code>ORDER BY last_remembered_at DESC</code> - Frequently remembered content: <code>ORDER BY remember_count DESC</code> - New vs old memories: Compare <code>first_remembered_at</code> across robots</p>"},{"location":"development/schema/#common-query-patterns","title":"Common Query Patterns","text":""},{"location":"development/schema/#finding-nodes-for-a-robot","title":"Finding Nodes for a Robot","text":"<pre><code>SELECT n.*\nFROM nodes n\nJOIN robot_nodes rn ON n.id = rn.node_id\nWHERE rn.robot_id = $1\nORDER BY rn.last_remembered_at DESC;\n</code></pre>"},{"location":"development/schema/#finding-robots-that-share-a-node","title":"Finding Robots that Share a Node","text":"<pre><code>SELECT r.*\nFROM robots r\nJOIN robot_nodes rn ON r.id = rn.robot_id\nWHERE rn.node_id = $1\nORDER BY rn.first_remembered_at;\n</code></pre>"},{"location":"development/schema/#finding-frequently-remembered-content","title":"Finding Frequently Remembered Content","text":"<pre><code>SELECT n.*, rn.remember_count, rn.first_remembered_at, rn.last_remembered_at\nFROM nodes n\nJOIN robot_nodes rn ON n.id = rn.node_id\nWHERE rn.robot_id = $1\nORDER BY rn.remember_count DESC\nLIMIT 10;\n</code></pre>"},{"location":"development/schema/#finding-tags-for-a-node","title":"Finding Tags for a Node","text":"<pre><code>SELECT t.name\nFROM tags t\nJOIN node_tags nt ON t.id = nt.tag_id\nWHERE nt.node_id = $1\nORDER BY t.name;\n</code></pre>"},{"location":"development/schema/#finding-nodes-with-a-specific-tag","title":"Finding Nodes with a Specific Tag","text":"<pre><code>SELECT n.*\nFROM nodes n\nJOIN node_tags nt ON n.id = nt.node_id\nJOIN tags t ON nt.tag_id = t.id\nWHERE t.name = 'database:postgresql'\nORDER BY n.created_at DESC;\n</code></pre>"},{"location":"development/schema/#finding-nodes-with-hierarchical-tag-prefix","title":"Finding Nodes with Hierarchical Tag Prefix","text":"<pre><code>SELECT n.*\nFROM nodes n\nJOIN node_tags nt ON n.id = nt.node_id\nJOIN tags t ON nt.tag_id = t.id\nWHERE t.name LIKE 'ai:llm:%'\nORDER BY n.created_at DESC;\n</code></pre>"},{"location":"development/schema/#finding-related-topics-by-shared-nodes","title":"Finding Related Topics by Shared Nodes","text":"<pre><code>SELECT\n    t1.name AS topic1,\n    t2.name AS topic2,\n    COUNT(DISTINCT nt1.node_id) AS shared_nodes\nFROM tags t1\nJOIN node_tags nt1 ON t1.id = nt1.tag_id\nJOIN node_tags nt2 ON nt1.node_id = nt2.node_id\nJOIN tags t2 ON nt2.tag_id = t2.id\nWHERE t1.name &lt; t2.name\nGROUP BY t1.name, t2.name\nHAVING COUNT(DISTINCT nt1.node_id) &gt;= 2\nORDER BY shared_nodes DESC;\n</code></pre>"},{"location":"development/schema/#vector-similarity-search-with-tag-filter","title":"Vector Similarity Search with Tag Filter","text":"<pre><code>SELECT n.*, n.embedding &lt;=&gt; $1::vector AS distance\nFROM nodes n\nJOIN node_tags nt ON n.id = nt.node_id\nJOIN tags t ON nt.tag_id = t.id\nWHERE t.name = 'programming:ruby'\n  AND n.embedding IS NOT NULL\nORDER BY distance\nLIMIT 10;\n</code></pre>"},{"location":"development/schema/#full-text-search-with-tag-filter","title":"Full-Text Search with Tag Filter","text":"<pre><code>SELECT n.*, ts_rank(to_tsvector('english', n.content), query) AS rank\nFROM nodes n\nJOIN node_tags nt ON n.id = nt.node_id\nJOIN tags t ON nt.tag_id = t.id,\n     to_tsquery('english', 'database &amp; optimization') query\nWHERE to_tsvector('english', n.content) @@ query\n  AND t.name LIKE 'database:%'\nORDER BY rank DESC\nLIMIT 20;\n</code></pre>"},{"location":"development/schema/#finding-content-shared-by-multiple-robots","title":"Finding Content Shared by Multiple Robots","text":"<pre><code>SELECT n.*, COUNT(DISTINCT rn.robot_id) AS robot_count\nFROM nodes n\nJOIN robot_nodes rn ON n.id = rn.node_id\nGROUP BY n.id\nHAVING COUNT(DISTINCT rn.robot_id) &gt; 1\nORDER BY robot_count DESC;\n</code></pre>"},{"location":"development/schema/#database-optimization","title":"Database Optimization","text":""},{"location":"development/schema/#vector-search-performance","title":"Vector Search Performance","text":"<p>The <code>idx_nodes_embedding</code> index uses HNSW (Hierarchical Navigable Small World) algorithm for fast approximate nearest neighbor search:</p> <ul> <li>m=16: Number of bi-directional links per node (higher = better recall, more memory)</li> <li>ef_construction=64: Size of dynamic candidate list during index construction (higher = better quality, slower build)</li> </ul> <p>For queries, you can adjust <code>ef_search</code> (defaults to 40): <pre><code>SET hnsw.ef_search = 100;  -- Better recall, slower queries\n</code></pre></p>"},{"location":"development/schema/#full-text-search-performance","title":"Full-Text Search Performance","text":"<p>The <code>idx_nodes_content_gin</code> index enables fast full-text search using PostgreSQL's tsvector:</p> <pre><code>-- Query optimization with explicit tsvector\nSELECT * FROM nodes\nWHERE to_tsvector('english', content) @@ to_tsquery('english', 'memory &amp; retrieval');\n</code></pre>"},{"location":"development/schema/#fuzzy-matching-performance","title":"Fuzzy Matching Performance","text":"<p>The <code>idx_nodes_content_trgm</code> index enables similarity search and pattern matching:</p> <pre><code>-- Similarity search\nSELECT * FROM nodes\nWHERE content % 'semantic retreval';  -- Handles typos\n\n-- Pattern matching\nSELECT * FROM nodes\nWHERE content ILIKE '%memry%';  -- Uses trigram index\n</code></pre>"},{"location":"development/schema/#index-maintenance","title":"Index Maintenance","text":"<p>Monitor and maintain indexes for optimal performance:</p> <pre><code>-- Check index usage\nSELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE schemaname = 'public'\nORDER BY idx_scan DESC;\n\n-- Reindex if needed\nREINDEX INDEX CONCURRENTLY idx_nodes_embedding;\nREINDEX INDEX CONCURRENTLY idx_nodes_content_gin;\n</code></pre>"},{"location":"development/schema/#schema-migration","title":"Schema Migration","text":"<p>The schema is managed through ActiveRecord migrations located in <code>db/migrate/</code>:</p> <ol> <li><code>20250101000001_create_robots.rb</code> - Creates robots table</li> <li><code>20250101000002_create_nodes.rb</code> - Creates nodes table with all indexes</li> <li><code>20250101000005_create_tags.rb</code> - Creates tags and nodes_tags tables</li> <li><code>20251128000002_create_file_sources.rb</code> - Creates file_sources table for document tracking</li> <li><code>20251128000003_add_source_to_nodes.rb</code> - Adds source_id and chunk_position to nodes</li> </ol> <p>To apply migrations: <pre><code>bundle exec rake htm:db:migrate\n</code></pre></p> <p>To generate the current schema dump: <pre><code>bundle exec rake htm:db:schema:dump\n</code></pre></p> <p>The canonical schema is maintained in <code>db/schema.sql</code>.</p>"},{"location":"development/schema/#database-extensions","title":"Database Extensions","text":""},{"location":"development/schema/#pgvector","title":"pgvector","text":"<p>Provides vector similarity search capabilities:</p> <pre><code>-- Install extension\nCREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA public;\n\n-- Vector operations\nSELECT embedding &lt;=&gt; $1::vector AS cosine_distance FROM nodes;  -- Cosine distance\nSELECT embedding &lt;-&gt; $1::vector AS l2_distance FROM nodes;      -- L2 distance\nSELECT embedding &lt;#&gt; $1::vector AS inner_product FROM nodes;    -- Inner product\n</code></pre>"},{"location":"development/schema/#pg_trgm","title":"pg_trgm","text":"<p>Provides trigram-based fuzzy text matching:</p> <pre><code>-- Install extension\nCREATE EXTENSION IF NOT EXISTS pg_trgm WITH SCHEMA public;\n\n-- Trigram operations\nSELECT content % 'search term' FROM nodes;           -- Similarity operator\nSELECT similarity(content, 'search term') FROM nodes; -- Similarity score\nSELECT content ILIKE '%pattern%' FROM nodes;          -- Pattern matching (uses trigram index)\n</code></pre>"},{"location":"development/schema/#best-practices","title":"Best Practices","text":""},{"location":"development/schema/#tagging-strategy","title":"Tagging Strategy","text":"<ol> <li>Use hierarchical namespaces: <code>category:subcategory:detail</code></li> <li>Be consistent with naming: Use lowercase, singular nouns</li> <li>Limit depth: 2-3 levels is optimal (e.g., <code>ai:llm:embeddings</code>)</li> <li>Avoid redundancy: Don't duplicate information already in node fields</li> </ol>"},{"location":"development/schema/#node-management","title":"Node Management","text":"<ol> <li>Set appropriate importance: Use 0.0-1.0 scale for priority-based retrieval</li> <li>Update last_accessed: Touch timestamp when retrieving for LRU eviction</li> <li>Manage token_count: Update when content changes for working memory budget</li> <li>Use appropriate types: fact, context, code, preference, decision, question</li> </ol>"},{"location":"development/schema/#search-strategy","title":"Search Strategy","text":"<ol> <li>Vector search: Best for semantic similarity (\"concepts like X\")</li> <li>Full-text search: Best for keyword matching (\"documents containing Y\")</li> <li>Fuzzy search: Best for typo tolerance and pattern matching</li> <li>Hybrid search: Combine vector + full-text with weighted scores</li> </ol>"},{"location":"development/schema/#performance-tuning","title":"Performance Tuning","text":"<ol> <li>Monitor index usage: Use pg_stat_user_indexes</li> <li>Vacuum regularly: Especially after bulk deletes</li> <li>Adjust HNSW parameters: Balance recall vs speed based on dataset size</li> <li>Use connection pooling: Managed by HTM::LongTermMemory</li> </ol>"},{"location":"development/setup/","title":"Development Setup Guide","text":"<p>This guide will walk you through setting up a complete HTM development environment from scratch.</p>"},{"location":"development/setup/#overview","title":"Overview","text":"<p>Setting up HTM for development involves:</p> <ol> <li>Cloning the repository</li> <li>Installing Ruby and system dependencies</li> <li>Installing Ruby gem dependencies</li> <li>Setting up TimescaleDB database</li> <li>Configuring Ollama for embeddings</li> <li>Verifying your setup</li> <li>Running tests and examples</li> </ol> <p>Let's get started!</p>"},{"location":"development/setup/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>macOS, Linux, or WSL2 (Windows Subsystem for Linux)</li> <li>Git installed (<code>git --version</code>)</li> <li>Ruby 3.0 or higher (we'll install this)</li> <li>Internet connection (for downloading dependencies)</li> </ul>"},{"location":"development/setup/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":""},{"location":"development/setup/#fork-the-repository-recommended-for-contributors","title":"Fork the Repository (Recommended for Contributors)","text":"<p>If you plan to submit pull requests, fork the repository first:</p> <ol> <li>Visit https://github.com/madbomber/htm</li> <li>Click the \"Fork\" button in the upper right</li> <li>Clone your fork:</li> </ol> <pre><code>git clone https://github.com/YOUR_USERNAME/htm.git\ncd htm\n</code></pre>"},{"location":"development/setup/#or-clone-directly-for-read-only-access","title":"Or Clone Directly (For Read-Only Access)","text":"<pre><code>git clone https://github.com/madbomber/htm.git\ncd htm\n</code></pre>"},{"location":"development/setup/#add-upstream-remote-for-forked-repos","title":"Add Upstream Remote (For Forked Repos)","text":"<p>If you forked, add the original repository as upstream:</p> <pre><code>git remote add upstream https://github.com/madbomber/htm.git\ngit fetch upstream\n</code></pre>"},{"location":"development/setup/#step-2-install-ruby","title":"Step 2: Install Ruby","text":"<p>HTM requires Ruby 3.0 or higher. We recommend using rbenv for managing Ruby versions.</p>"},{"location":"development/setup/#check-current-ruby-version","title":"Check Current Ruby Version","text":"<pre><code>ruby --version\n# Example output: ruby 3.3.0 (2023-12-25 revision 5124f9ac75) [arm64-darwin23]\n</code></pre> <p>If you already have Ruby 3.0+, you can skip to Step 3.</p>"},{"location":"development/setup/#install-rbenv-macos","title":"Install rbenv (macOS)","text":"<pre><code># Install rbenv and ruby-build\nbrew install rbenv ruby-build\n\n# Initialize rbenv in your shell\necho 'eval \"$(rbenv init - bash)\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Verify installation\nrbenv --version\n</code></pre>"},{"location":"development/setup/#install-rbenv-linux","title":"Install rbenv (Linux)","text":"<pre><code># Clone rbenv\ngit clone https://github.com/rbenv/rbenv.git ~/.rbenv\n\n# Add to PATH\necho 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(rbenv init - bash)\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Install ruby-build\ngit clone https://github.com/rbenv/ruby-build.git \"$(rbenv root)\"/plugins/ruby-build\n</code></pre>"},{"location":"development/setup/#install-ruby-33-latest-stable","title":"Install Ruby 3.3 (Latest Stable)","text":"<pre><code># List available Ruby versions\nrbenv install --list\n\n# Install Ruby 3.3.0 (or latest 3.x version)\nrbenv install 3.3.0\n\n# Set as global default\nrbenv global 3.3.0\n\n# Verify installation\nruby --version\n# Should show: ruby 3.3.0\n</code></pre>"},{"location":"development/setup/#step-3-install-ruby-dependencies","title":"Step 3: Install Ruby Dependencies","text":"<p>HTM uses Bundler to manage Ruby gem dependencies.</p>"},{"location":"development/setup/#install-bundler","title":"Install Bundler","text":"<pre><code>gem install bundler\n\n# Verify installation\nbundle --version\n</code></pre>"},{"location":"development/setup/#install-project-dependencies","title":"Install Project Dependencies","text":"<pre><code># From the htm directory\nbundle install\n</code></pre> <p>This will install:</p> <ul> <li>pg: PostgreSQL adapter</li> <li>pgvector: Vector similarity search support</li> <li>connection_pool: Database connection pooling</li> <li>tiktoken_ruby: Token counting for working memory</li> <li>ruby_llm: LLM client for embeddings</li> <li>rake: Task automation</li> <li>minitest: Testing framework</li> <li>minitest-reporters: Test output formatting</li> <li>debug_me: Debugging utility</li> </ul>"},{"location":"development/setup/#verify-installation","title":"Verify Installation","text":"<pre><code>bundle exec ruby -e \"require 'htm'; puts HTM::VERSION\"\n# Should output: 0.1.0 (or current version)\n</code></pre>"},{"location":"development/setup/#step-4-set-up-timescaledb-database","title":"Step 4: Set Up TimescaleDB Database","text":"<p>HTM requires PostgreSQL with TimescaleDB and pgvector extensions. You have two options:</p>"},{"location":"development/setup/#option-a-timescaledb-cloud-recommended-for-quick-start","title":"Option A: TimescaleDB Cloud (Recommended for Quick Start)","text":"<p>This is the fastest way to get a working database:</p>"},{"location":"development/setup/#1-create-account","title":"1. Create Account","text":"<p>Visit https://www.timescale.com/ and sign up for a free account.</p>"},{"location":"development/setup/#2-create-service","title":"2. Create Service","text":"<ul> <li>Click \"Create Service\"</li> <li>Select your region (choose closest to you)</li> <li>Choose the Free Tier (or your preferred plan)</li> <li>Click \"Create Service\"</li> <li>Wait 2-3 minutes for provisioning</li> </ul>"},{"location":"development/setup/#3-get-connection-details","title":"3. Get Connection Details","text":"<ul> <li>Click on your new service</li> <li>Click \"Connection Info\"</li> <li>Copy the full connection string (looks like <code>postgres://username:password@host:port/database?sslmode=require</code>)</li> </ul>"},{"location":"development/setup/#4-configure-environment-variables","title":"4. Configure Environment Variables","text":"<p>Create or edit <code>~/.bashrc__tiger</code>:</p> <pre><code># TimescaleDB Connection Configuration\nexport HTM_SERVICE_NAME=\"db-67977\"  # Your service name\nexport HTM_DBNAME=\"tsdb\"\nexport HTM_DBUSER=\"tsdbadmin\"\nexport HTM_DBPASS=\"your_password_here\"\nexport HTM_DBPORT=\"37807\"  # Your port number\nexport HTM_DBURL=\"postgres://tsdbadmin:your_password@host:port/tsdb?sslmode=require\"\n</code></pre> <p>Replace the placeholders with your actual connection details.</p>"},{"location":"development/setup/#5-load-environment-variables","title":"5. Load Environment Variables","text":"<pre><code># Load configuration\nsource ~/.bashrc__tiger\n\n# Optionally, add to your ~/.bashrc for automatic loading\necho 'source ~/.bashrc__tiger' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"development/setup/#option-b-local-postgresql-with-docker-advanced","title":"Option B: Local PostgreSQL with Docker (Advanced)","text":"<p>For local development with Docker:</p> <pre><code># Create docker-compose.yml\ncat &gt; docker-compose.yml &lt;&lt;'EOF'\nversion: '3.8'\nservices:\n  timescaledb:\n    image: timescale/timescaledb-ha:pg17\n    environment:\n      POSTGRES_USER: tsdbadmin\n      POSTGRES_PASSWORD: devpassword\n      POSTGRES_DB: tsdb\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - timescale_data:/var/lib/postgresql/data\n\nvolumes:\n  timescale_data:\nEOF\n\n# Start TimescaleDB\ndocker-compose up -d\n\n# Configure environment variables\ncat &gt; ~/.bashrc__tiger &lt;&lt;'EOF'\nexport HTM_SERVICE_NAME=\"local-dev\"\nexport HTM_DBNAME=\"tsdb\"\nexport HTM_DBUSER=\"tsdbadmin\"\nexport HTM_DBPASS=\"devpassword\"\nexport HTM_DBPORT=\"5432\"\nexport HTM_DBURL=\"postgres://tsdbadmin:devpassword@localhost:5432/tsdb?sslmode=disable\"\nEOF\n\nsource ~/.bashrc__tiger\n</code></pre>"},{"location":"development/setup/#verify-database-connection","title":"Verify Database Connection","text":"<p>Test your database connection:</p> <pre><code># From the htm directory\nruby test_connection.rb\n</code></pre> <p>Expected output:</p> <pre><code>Connected successfully!\nTimescaleDB Extension: Version 2.22.1\npgvector Extension: Version 0.8.1\npg_trgm Extension: Version 1.6\n</code></pre>"},{"location":"development/setup/#enable-required-extensions","title":"Enable Required Extensions","text":"<p>Run the extension setup script:</p> <pre><code>ruby enable_extensions.rb\n</code></pre> <p>This ensures that TimescaleDB, pgvector, and pg_trgm extensions are enabled.</p>"},{"location":"development/setup/#step-5-set-up-ollama-for-embeddings","title":"Step 5: Set Up Ollama for Embeddings","text":"<p>HTM uses Ollama (via RubyLLM) to generate vector embeddings for semantic search.</p>"},{"location":"development/setup/#install-ollama","title":"Install Ollama","text":""},{"location":"development/setup/#macos","title":"macOS","text":"<pre><code># Download and install from official site\ncurl https://ollama.ai/install.sh | sh\n\n# Or using Homebrew\nbrew install ollama\n</code></pre>"},{"location":"development/setup/#linux","title":"Linux","text":"<pre><code>curl https://ollama.ai/install.sh | sh\n</code></pre>"},{"location":"development/setup/#start-ollama-service","title":"Start Ollama Service","text":"<p>Ollama typically starts automatically after installation. Verify it's running:</p> <pre><code># Check if Ollama is running\ncurl http://localhost:11434/api/version\n\n# Expected output:\n# {\"version\":\"0.1.x\"}\n</code></pre> <p>If not running, start it manually:</p> <pre><code># macOS - Ollama runs as a background service\n# Check Activity Monitor or start from Applications\n\n# Linux\nollama serve &amp;\n</code></pre>"},{"location":"development/setup/#pull-the-gpt-oss-model","title":"Pull the gpt-oss Model","text":"<p>HTM uses the <code>gpt-oss</code> model by default:</p> <pre><code># Pull the model (downloads ~4GB)\nollama pull gpt-oss\n\n# Verify the model is available\nollama list\n# Should show gpt-oss in the list\n</code></pre>"},{"location":"development/setup/#test-embedding-generation","title":"Test Embedding Generation","text":"<pre><code># Test that embeddings work\nollama run gpt-oss \"Hello, HTM!\"\n</code></pre>"},{"location":"development/setup/#optional-configure-custom-ollama-url","title":"Optional: Configure Custom Ollama URL","text":"<p>If Ollama is running on a different host or port:</p> <pre><code># Add to ~/.bashrc__tiger\nexport OLLAMA_URL=\"http://custom-host:11434\"\n</code></pre>"},{"location":"development/setup/#step-6-initialize-database-schema","title":"Step 6: Initialize Database Schema","text":"<p>Now that everything is set up, initialize the HTM database schema:</p> <pre><code># Run database setup\nrake db_setup\n</code></pre> <p>This creates all required tables, indexes, views, and triggers. See the Schema Documentation for details.</p>"},{"location":"development/setup/#alternative-manual-setup","title":"Alternative: Manual Setup","text":"<p>You can also run the schema SQL directly:</p> <pre><code># Using psql\npsql $HTM_DBURL -f sql/schema.sql\n\n# Or using Ruby\nruby -r ./lib/htm -e \"HTM::Database.setup\"\n</code></pre>"},{"location":"development/setup/#step-7-verify-your-setup","title":"Step 7: Verify Your Setup","text":"<p>Let's make sure everything is working correctly.</p>"},{"location":"development/setup/#run-the-test-suite","title":"Run the Test Suite","text":"<pre><code># Run all tests\nrake test\n</code></pre> <p>Expected output:</p> <pre><code>HTMTest\n  test_version_exists                             PASS (0.00s)\n  test_version_format                             PASS (0.00s)\n  test_htm_class_exists                           PASS (0.00s)\n  ...\n\nFinished in 0.05s\n12 tests, 0 failures, 0 errors, 0 skips\n</code></pre>"},{"location":"development/setup/#run-integration-tests","title":"Run Integration Tests","text":"<p>Integration tests require a working database:</p> <pre><code>ruby test/integration_test.rb\n</code></pre>"},{"location":"development/setup/#run-the-example","title":"Run the Example","text":"<p>Test the full workflow with a real example:</p> <pre><code>rake example\n\n# Or directly\nruby examples/basic_usage.rb\n</code></pre> <p>Expected output:</p> <pre><code>HTM Basic Usage Example\n============================================================\n\n1. Initializing HTM for 'Code Helper' robot...\n   Using RubyLLM with Ollama provider and gpt-oss model for embeddings\n\u2713 HTM initialized\n  Robot ID: robot-abc123\n  Robot Name: Code Helper\n  Embedding Service: Ollama (gpt-oss via RubyLLM)\n\n2. Adding memory nodes...\n\u2713 Added decision about database choice\n\u2713 Added decision about RAG approach\n\u2713 Added fact about user preferences\n...\n</code></pre>"},{"location":"development/setup/#development-tools","title":"Development Tools","text":"<p>HTM includes several Rake tasks to streamline development:</p>"},{"location":"development/setup/#available-rake-tasks","title":"Available Rake Tasks","text":"<pre><code># Show all available tasks\nrake --tasks\n\n# Output:\n# rake db_setup    # Run database setup\n# rake db_test     # Test database connection\n# rake example     # Run example\n# rake stats       # Show gem statistics\n# rake test        # Run tests\n</code></pre>"},{"location":"development/setup/#common-development-commands","title":"Common Development Commands","text":"<pre><code># Run all tests\nrake test\n\n# Test database connection\nrake db_test\n\n# Run example\nrake example\n\n# Show code statistics\nrake stats\n\n# Setup database schema\nrake db_setup\n</code></pre>"},{"location":"development/setup/#environment-configuration","title":"Environment Configuration","text":"<p>HTM uses environment variables for configuration. Here's a complete reference:</p>"},{"location":"development/setup/#database-variables","title":"Database Variables","text":"Variable Description Example <code>HTM_DBURL</code> Full PostgreSQL connection URL (preferred) <code>postgres://user:pass@host:port/db?sslmode=require</code> <code>HTM_DBNAME</code> Database name <code>tsdb</code> <code>HTM_DBUSER</code> Database username <code>tsdbadmin</code> <code>HTM_DBPASS</code> Database password <code>your_password</code> <code>HTM_DBPORT</code> Database port <code>37807</code> <code>HTM_SERVICE_NAME</code> Service identifier (informational) <code>db-67977</code>"},{"location":"development/setup/#ollama-variables","title":"Ollama Variables","text":"Variable Description Example <code>OLLAMA_URL</code> Ollama API URL (optional) <code>http://localhost:11434</code>"},{"location":"development/setup/#managing-environment-files","title":"Managing Environment Files","text":"<p>You can organize your environment variables using multiple files:</p> <pre><code># ~/.bashrc__tiger - Database configuration\n# ~/.bashrc__ollama - Ollama configuration (if needed)\n\n# Load all configuration files in ~/.bashrc\nsource ~/.bashrc__tiger\n</code></pre>"},{"location":"development/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/setup/#common-setup-issues","title":"Common Setup Issues","text":""},{"location":"development/setup/#cannot-connect-to-database","title":"\"Cannot connect to database\"","text":"<p>Symptoms: Connection refused or timeout errors</p> <p>Solutions:</p> <pre><code># Verify environment variables are set\necho $HTM_DBURL\n\n# Test connection directly with psql\npsql $HTM_DBURL\n\n# Check if service is running (TimescaleDB Cloud)\n# Visit your Timescale Cloud dashboard\n\n# For Docker, check if container is running\ndocker ps | grep timescale\n</code></pre>"},{"location":"development/setup/#ollama-connection-refused","title":"\"Ollama connection refused\"","text":"<p>Symptoms: Embedding generation fails</p> <p>Solutions:</p> <pre><code># Verify Ollama is running\ncurl http://localhost:11434/api/version\n\n# Start Ollama service\n# macOS: Start from Applications or Activity Monitor\n# Linux: ollama serve &amp;\n\n# Check if model is downloaded\nollama list | grep gpt-oss\n\n# Pull model if missing\nollama pull gpt-oss\n</code></pre>"},{"location":"development/setup/#extension-not-available","title":"\"Extension not available\"","text":"<p>Symptoms: Errors about missing TimescaleDB or pgvector</p> <p>Solutions:</p> <pre><code># Re-run extension setup\nruby enable_extensions.rb\n\n# Check extension status\npsql $HTM_DBURL -c \"SELECT extname, extversion FROM pg_extension ORDER BY extname\"\n\n# For TimescaleDB Cloud, extensions should be pre-installed\n# For local PostgreSQL, ensure you're using timescale/timescaledb-ha image\n</code></pre>"},{"location":"development/setup/#bundle-install-fails","title":"\"Bundle install fails\"","text":"<p>Symptoms: Gem installation errors</p> <p>Solutions:</p> <pre><code># Ensure you have development tools\n# macOS:\nxcode-select --install\n\n# Linux (Ubuntu/Debian):\nsudo apt-get install build-essential libpq-dev\n\n# Update RubyGems and Bundler\ngem update --system\ngem install bundler\n\n# Clear bundle cache and retry\nbundle clean --force\nbundle install\n</code></pre>"},{"location":"development/setup/#test-failures","title":"\"Test failures\"","text":"<p>Symptoms: Tests fail with database or connection errors</p> <p>Solutions:</p> <pre><code># Ensure database is set up\nrake db_setup\n\n# Verify environment variables\nsource ~/.bashrc__tiger\nenv | grep TIGER\n\n# Check Ollama is running\ncurl http://localhost:11434/api/version\n\n# Run tests with verbose output\nrake test TESTOPTS=\"-v\"\n</code></pre>"},{"location":"development/setup/#ssltls-issues","title":"SSL/TLS Issues","text":"<p>If you see SSL certificate errors:</p> <pre><code># Ensure sslmode is set in connection URL\necho $HTM_DBURL | grep sslmode\n# Should show: sslmode=require\n\n# For local development without SSL\nexport HTM_DBURL=\"postgres://user:pass@localhost:5432/tsdb?sslmode=disable\"\n</code></pre>"},{"location":"development/setup/#ruby-version-issues","title":"Ruby Version Issues","text":"<p>If you see Ruby version errors:</p> <pre><code># Check Ruby version\nruby --version\n\n# Update to Ruby 3.3\nrbenv install 3.3.0\nrbenv global 3.3.0\n\n# Reinstall gems\nbundle install\n</code></pre>"},{"location":"development/setup/#development-best-practices","title":"Development Best Practices","text":""},{"location":"development/setup/#keep-your-fork-updated","title":"Keep Your Fork Updated","text":"<p>If you forked the repository, regularly sync with upstream:</p> <pre><code># Fetch upstream changes\ngit fetch upstream\n\n# Merge into your main branch\ngit checkout main\ngit merge upstream/main\n\n# Push to your fork\ngit push origin main\n</code></pre>"},{"location":"development/setup/#use-feature-branches","title":"Use Feature Branches","text":"<p>Always create a branch for your changes:</p> <pre><code># Create and switch to feature branch\ngit checkout -b feature/my-new-feature\n\n# Make changes, commit, push\ngit add .\ngit commit -m \"Add my new feature\"\ngit push origin feature/my-new-feature\n</code></pre>"},{"location":"development/setup/#run-tests-before-committing","title":"Run Tests Before Committing","text":"<p>Always run the test suite before committing:</p> <pre><code># Run all tests\nrake test\n\n# If tests pass, commit\ngit commit -m \"Your commit message\"\n</code></pre>"},{"location":"development/setup/#use-debug_me-for-debugging","title":"Use debug_me for Debugging","text":"<p>HTM uses the <code>debug_me</code> gem for debugging. Don't use <code>puts</code> statements:</p> <pre><code>require 'debug_me'\n\ndef some_method(param)\n  debug_me { [ :param ] }  # Outputs: param = \"value\"\n\n  # For simple strings\n  debug_me \"Reached this point in execution\"\n\n  # Your code here\nend\n</code></pre>"},{"location":"development/setup/#next-steps","title":"Next Steps","text":"<p>Now that your development environment is set up:</p> <ol> <li>Learn about Testing: Understand HTM's test suite and write your own tests</li> <li>Read Contributing Guide: Learn our workflow and submit your first PR</li> <li>Explore the Schema: Understand the database architecture</li> <li>Check the Roadmap: See what features are planned</li> </ol>"},{"location":"development/setup/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ul> <li>Check existing issues: GitHub Issues</li> <li>Ask in discussions: GitHub Discussions</li> <li>Review planning docs: See <code>htm_teamwork.md</code> for design decisions</li> </ul> <p>Happy developing! We look forward to your contributions.</p>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>HTM follows a comprehensive testing philosophy to ensure reliability, correctness, and maintainability. This guide covers everything you need to know about testing HTM.</p>"},{"location":"development/testing/#testing-philosophy","title":"Testing Philosophy","text":""},{"location":"development/testing/#core-principles","title":"Core Principles","text":"<p>HTM's testing approach is guided by these principles:</p> <ol> <li>Test Everything: Every feature must have corresponding tests</li> <li>Test in Isolation: Methods should be testable independently</li> <li>Test Real Scenarios: Integration tests with actual database</li> <li>Test Edge Cases: Don't just test the happy path</li> <li>Keep Tests Fast: Optimize for quick feedback loops</li> <li>Keep Tests Clear: Tests are documentation</li> </ol>"},{"location":"development/testing/#test-driven-development","title":"Test-Driven Development","text":"<p>We encourage (but don't strictly require) test-driven development:</p> <ol> <li>Write the test first - Define expected behavior</li> <li>Watch it fail - Ensure the test actually tests something</li> <li>Implement the feature - Make the test pass</li> <li>Refactor - Clean up while tests keep you safe</li> </ol>"},{"location":"development/testing/#test-suite-overview","title":"Test Suite Overview","text":"<p>HTM uses Minitest as its testing framework. The test suite is organized into three categories:</p>"},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Purpose: Test individual methods and classes in isolation</p> <p>Location: <code>test/*_test.rb</code> (excluding <code>integration_test.rb</code>)</p> <p>Characteristics:</p> <ul> <li>Fast execution (milliseconds)</li> <li>No database required</li> <li>Mock external dependencies</li> <li>Test logic and behavior</li> </ul> <p>Example: <code>test/htm_test.rb</code>, <code>test/embedding_service_test.rb</code></p>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Purpose: Test full workflows with real dependencies</p> <p>Location: <code>test/integration_test.rb</code></p> <p>Characteristics:</p> <ul> <li>Slower execution (seconds)</li> <li>Requires PostgreSQL</li> <li>Requires Ollama for embeddings</li> <li>Tests real-world scenarios</li> <li>Tests database interactions</li> </ul>"},{"location":"development/testing/#performance-tests","title":"Performance Tests","text":"<p>Purpose: Ensure performance characteristics</p> <p>Status: Planned for future implementation</p> <p>Focus areas:</p> <ul> <li>Query performance</li> <li>Memory usage</li> <li>Token counting accuracy</li> <li>Embedding generation speed</li> </ul>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#run-all-tests","title":"Run All Tests","text":"<pre><code># Using Rake (recommended)\nrake test\n\n# Using Ruby directly\nruby -Ilib:test test/**/*_test.rb\n</code></pre> <p>Expected output:</p> <pre><code>HTMTest\n  test_version_exists                             PASS (0.00s)\n  test_version_format                             PASS (0.00s)\n  test_htm_class_exists                           PASS (0.00s)\n  ...\n\nIntegrationTest\n  test_htm_initializes_with_ollama                PASS (0.15s)\n  test_add_node_with_embedding                    PASS (0.32s)\n  ...\n\nFinished in 2.47s\n28 tests, 0 failures, 0 errors, 0 skips\n</code></pre>"},{"location":"development/testing/#run-specific-test-file","title":"Run Specific Test File","text":"<pre><code># Run unit tests only\nruby test/htm_test.rb\n\n# Run embedding service tests\nruby test/embedding_service_test.rb\n\n# Run integration tests\nruby test/integration_test.rb\n</code></pre>"},{"location":"development/testing/#run-specific-test-method","title":"Run Specific Test Method","text":"<pre><code># Run a single test method\nruby test/htm_test.rb -n test_version_exists\n\n# Run tests matching a pattern\nruby test/integration_test.rb -n /embedding/\n</code></pre>"},{"location":"development/testing/#run-tests-with-verbose-output","title":"Run Tests with Verbose Output","text":"<pre><code># Verbose output\nrake test TESTOPTS=\"-v\"\n\n# Show test names as they run\nruby test/htm_test.rb -v\n</code></pre>"},{"location":"development/testing/#run-tests-with-debugging","title":"Run Tests with Debugging","text":"<pre><code># Run with debug output\nDEBUG=1 rake test\n\n# Run with Ruby debugger\nruby -r debug test/htm_test.rb\n</code></pre>"},{"location":"development/testing/#test-structure-and-organization","title":"Test Structure and Organization","text":""},{"location":"development/testing/#test-file-layout","title":"Test File Layout","text":""},{"location":"development/testing/#test-file-template","title":"Test File Template","text":"<p>Every test file follows this structure:</p> <pre><code># frozen_string_literal: true\n\nrequire \"test_helper\"\n\nclass MyFeatureTest &lt; Minitest::Test\n  def setup\n    # Runs before each test\n    # Initialize test data, mocks, etc.\n  end\n\n  def teardown\n    # Runs after each test\n    # Clean up test data\n  end\n\n  def test_something_works\n    # Arrange: Set up test data\n    input = \"test value\"\n\n    # Act: Execute the code being tested\n    result = MyClass.some_method(input)\n\n    # Assert: Verify the results\n    assert_equal \"expected\", result\n  end\n\n  def test_handles_edge_case\n    # Test edge cases and error conditions\n    assert_raises(ArgumentError) do\n      MyClass.some_method(nil)\n    end\n  end\nend\n</code></pre>"},{"location":"development/testing/#test-helper-configuration","title":"Test Helper Configuration","text":"<p><code>test/test_helper.rb</code> provides shared configuration:</p> <pre><code># frozen_string_literal: true\n\n$LOAD_PATH.unshift File.expand_path(\"../lib\", __dir__)\nrequire \"htm\"\n\nrequire \"minitest/autorun\"\nrequire \"minitest/reporters\"\n\nMinitest::Reporters.use! [Minitest::Reporters::SpecReporter.new]\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#unit-test-example","title":"Unit Test Example","text":"<p>Testing a method in isolation:</p> <pre><code>class WorkingMemoryTest &lt; Minitest::Test\n  def setup\n    @memory = HTM::WorkingMemory.new(max_tokens: 1000)\n  end\n\n  def test_calculates_token_count\n    node = { value: \"Hello, world!\" }\n\n    result = @memory.calculate_tokens(node)\n\n    assert_instance_of Integer, result\n    assert result &gt; 0\n  end\n\n  def test_rejects_nodes_exceeding_capacity\n    @memory = HTM::WorkingMemory.new(max_tokens: 10)\n    large_node = { value: \"x\" * 1000 }\n\n    assert_raises(HTM::WorkingMemoryFullError) do\n      @memory.add_node(large_node)\n    end\n  end\nend\n</code></pre>"},{"location":"development/testing/#integration-test-example","title":"Integration Test Example","text":"<p>Testing with real database:</p> <pre><code>class DatabaseIntegrationTest &lt; Minitest::Test\n  def setup\n    skip \"Database not configured\" unless ENV['HTM_DBURL']\n\n    @htm = HTM.new(\n      robot_name: \"Test Robot\",\n      working_memory_size: 128_000,\n      embedding_service: :ollama\n    )\n  end\n\n  def teardown\n    return unless @htm\n\n    # Clean up test data\n    @htm.forget(\"test_node_001\", confirm: :confirmed) rescue nil\n  end\n\n  def test_adds_and_retrieves_node\n    # Add a node\n    node_id = @htm.add_node(\n      \"test_node_001\",\n      \"Test memory content\",\n      type: :fact,\n      importance: 5.0\n    )\n\n    assert_instance_of Integer, node_id\n\n    # Retrieve it\n    node = @htm.retrieve(\"test_node_001\")\n\n    refute_nil node\n    assert_equal \"test_node_001\", node['key']\n    assert_includes node['value'], \"Test memory\"\n  end\nend\n</code></pre>"},{"location":"development/testing/#testing-with-mocks-and-stubs","title":"Testing with Mocks and Stubs","text":"<p>For testing without external dependencies:</p> <pre><code>class EmbeddingServiceTest &lt; Minitest::Test\n  def test_generates_embedding_vector\n    service = HTM::EmbeddingService.new(:ollama, model: 'gpt-oss')\n\n    # Skip if Ollama is not available\n    skip \"Ollama not running\" unless ollama_available?\n\n    embedding = service.generate_embedding(\"test text\")\n\n    assert_instance_of Array, embedding\n    assert_equal 1536, embedding.length\n    assert embedding.all? { |v| v.is_a?(Float) }\n  end\n\n  private\n\n  def ollama_available?\n    require 'net/http'\n    uri = URI('http://localhost:11434/api/version')\n    response = Net::HTTP.get_response(uri)\n    response.is_a?(Net::HTTPSuccess)\n  rescue\n    false\n  end\nend\n</code></pre>"},{"location":"development/testing/#test-fixtures","title":"Test Fixtures","text":""},{"location":"development/testing/#what-are-fixtures","title":"What are Fixtures?","text":"<p>Fixtures are pre-defined test data that can be reused across tests. HTM will use fixtures for complex test scenarios.</p>"},{"location":"development/testing/#future-fixture-structure","title":"Future Fixture Structure","text":"<pre><code># test/fixtures/memories.rb\nmodule Fixtures\n  MEMORIES = {\n    fact: {\n      key: \"user_preference_001\",\n      value: \"User prefers debug_me over puts for debugging\",\n      type: :fact,\n      importance: 7.0\n    },\n    decision: {\n      key: \"decision_001\",\n      value: \"We decided to use TimescaleDB for time-series optimization\",\n      type: :decision,\n      importance: 9.0,\n      tags: [\"database\", \"architecture\"]\n    }\n  }\nend\n</code></pre>"},{"location":"development/testing/#using-fixtures","title":"Using Fixtures","text":"<pre><code>require_relative 'fixtures/memories'\n\nclass MemoryTest &lt; Minitest::Test\n  def test_stores_fact\n    htm = HTM.new(robot_name: \"Test\")\n    fact = Fixtures::MEMORIES[:fact]\n\n    node_id = htm.add_node(\n      fact[:key],\n      fact[:value],\n      type: fact[:type],\n      importance: fact[:importance]\n    )\n\n    assert node_id &gt; 0\n  end\nend\n</code></pre>"},{"location":"development/testing/#assertions-reference","title":"Assertions Reference","text":""},{"location":"development/testing/#common-assertions","title":"Common Assertions","text":"<p>Minitest provides many assertion methods:</p> <pre><code># Equality\nassert_equal expected, actual\nrefute_equal unexpected, actual\n\n# Truth/falsy\nassert actual\nrefute actual\nassert_nil value\nrefute_nil value\n\n# Type checking\nassert_instance_of String, value\nassert_kind_of Numeric, value\n\n# Collections\nassert_includes collection, item\nassert_empty collection\nrefute_empty collection\n\n# Exceptions\nassert_raises(ErrorClass) { code }\nassert_silent { code }\n\n# Matching\nassert_match /pattern/, string\nrefute_match /pattern/, string\n\n# Comparison\nassert_operator 5, :&gt;, 3\nassert_in_delta 3.14, Math::PI, 0.01\n</code></pre>"},{"location":"development/testing/#custom-assertions","title":"Custom Assertions","text":"<p>You can create custom assertions for HTM-specific checks:</p> <pre><code>module HTMAssertions\n  def assert_valid_embedding(embedding)\n    assert_instance_of Array, embedding\n    assert_equal 1536, embedding.length\n    assert embedding.all? { |v| v.is_a?(Float) }\n  end\n\n  def assert_valid_node(node)\n    assert_instance_of Hash, node\n    assert node.key?('id')\n    assert node.key?('key')\n    assert node.key?('value')\n    assert node.key?('type')\n  end\nend\n\nclass MyTest &lt; Minitest::Test\n  include HTMAssertions\n\n  def test_node_structure\n    node = create_test_node\n    assert_valid_node(node)\n  end\nend\n</code></pre>"},{"location":"development/testing/#mocking-and-stubbing","title":"Mocking and Stubbing","text":""},{"location":"development/testing/#when-to-mock","title":"When to Mock","text":"<p>Mock external dependencies to:</p> <ul> <li>Speed up tests (avoid slow API calls)</li> <li>Test error conditions</li> <li>Isolate the code under test</li> <li>Test without required services</li> </ul>"},{"location":"development/testing/#minitest-mocking","title":"Minitest Mocking","text":"<p>Minitest includes built-in mocking:</p> <pre><code>require 'minitest/mock'\n\nclass ServiceTest &lt; Minitest::Test\n  def test_calls_external_api\n    mock_client = Minitest::Mock.new\n    mock_client.expect :call, \"response\", [\"arg\"]\n\n    service = MyService.new(client: mock_client)\n    result = service.process\n\n    assert_equal \"response\", result\n    mock_client.verify  # Ensures expectations were met\n  end\nend\n</code></pre>"},{"location":"development/testing/#stubbing-methods","title":"Stubbing Methods","text":"<p>Temporarily replace method implementations:</p> <pre><code>class NetworkTest &lt; Minitest::Test\n  def test_handles_network_failure\n    # Stub a method to simulate failure\n    HTM::Database.stub :connected?, false do\n      assert_raises(HTM::DatabaseError) do\n        htm = HTM.new(robot_name: \"Test\")\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"development/testing/#test-coverage","title":"Test Coverage","text":""},{"location":"development/testing/#coverage-goals","title":"Coverage Goals","text":"<p>HTM aims for high test coverage:</p> <ul> <li>Unit tests: 90%+ line coverage</li> <li>Integration tests: Cover all critical paths</li> <li>Edge cases: Test error conditions</li> <li>Documentation: Tests serve as usage examples</li> </ul>"},{"location":"development/testing/#measuring-coverage-future","title":"Measuring Coverage (Future)","text":"<p>We plan to add SimpleCov for coverage reporting:</p> <pre><code># test/test_helper.rb (future)\nrequire 'simplecov'\nSimpleCov.start do\n  add_filter '/test/'\n  minimum_coverage 90\nend\n</code></pre>"},{"location":"development/testing/#coverage-report","title":"Coverage Report","text":"<pre><code># Generate coverage report\nrake test:coverage\n\n# View report\nopen coverage/index.html\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions-future","title":"GitHub Actions (Future)","text":"<p>HTM will use GitHub Actions for CI/CD:</p> <pre><code># .github/workflows/test.yml (future)\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: timescale/timescaledb-ha:pg17\n        env:\n          POSTGRES_PASSWORD: testpass\n        options: &gt;-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v3\n      - uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: 3.3\n          bundler-cache: true\n      - name: Run tests\n        run: bundle exec rake test\n</code></pre>"},{"location":"development/testing/#ci-requirements","title":"CI Requirements","text":"<p>All pull requests must:</p> <ul> <li>Pass all tests (100%)</li> <li>Maintain or improve coverage</li> <li>Pass style checks (future)</li> <li>Pass integration tests</li> </ul>"},{"location":"development/testing/#testing-best-practices","title":"Testing Best Practices","text":""},{"location":"development/testing/#do-write-clear-tests","title":"DO: Write Clear Tests","text":"<pre><code># Good: Clear test name and assertions\ndef test_working_memory_evicts_least_important_nodes_when_full\n  memory = HTM::WorkingMemory.new(max_tokens: 100)\n  memory.add_node(key: \"important\", importance: 9.0, tokens: 50)\n  memory.add_node(key: \"unimportant\", importance: 1.0, tokens: 51)\n\n  assert memory.contains?(\"important\")\n  refute memory.contains?(\"unimportant\")\nend\n\n# Bad: Unclear test\ndef test_eviction\n  memory = HTM::WorkingMemory.new(max_tokens: 100)\n  memory.add_node(key: \"a\", importance: 9.0, tokens: 50)\n  memory.add_node(key: \"b\", importance: 1.0, tokens: 51)\n  assert memory.contains?(\"a\")\nend\n</code></pre>"},{"location":"development/testing/#do-test-one-thing-at-a-time","title":"DO: Test One Thing at a Time","text":"<pre><code># Good: Each test focuses on one behavior\ndef test_calculates_token_count\n  result = calculate_tokens(\"hello\")\n  assert result &gt; 0\nend\n\ndef test_handles_empty_string\n  result = calculate_tokens(\"\")\n  assert_equal 0, result\nend\n\n# Bad: Testing multiple things\ndef test_token_stuff\n  assert calculate_tokens(\"hello\") &gt; 0\n  assert_equal 0, calculate_tokens(\"\")\n  assert_raises(ArgumentError) { calculate_tokens(nil) }\nend\n</code></pre>"},{"location":"development/testing/#do-use-descriptive-test-names","title":"DO: Use Descriptive Test Names","text":"<pre><code># Good: Describes what is being tested\ndef test_recall_returns_memories_from_specified_timeframe\ndef test_forget_requires_confirmation_parameter\ndef test_add_node_generates_embedding_automatically\n\n# Bad: Vague or unclear\ndef test_recall\ndef test_forget\ndef test_add\n</code></pre>"},{"location":"development/testing/#do-clean-up-after-tests","title":"DO: Clean Up After Tests","text":"<pre><code>def setup\n  @htm = HTM.new(robot_name: \"Test\")\n  @test_keys = []\nend\n\ndef teardown\n  # Clean up any created nodes\n  @test_keys.each do |key|\n    @htm.forget(key, confirm: :confirmed) rescue nil\n  end\nend\n\ndef test_adds_node\n  key = \"test_#{Time.now.to_i}\"\n  @test_keys &lt;&lt; key\n\n  @htm.add_node(key, \"content\", type: :fact)\n  # Test continues...\nend\n</code></pre>"},{"location":"development/testing/#dont-rely-on-test-order","title":"DON'T: Rely on Test Order","text":"<pre><code># Bad: Tests depend on each other\ndef test_1_creates_node\n  @htm.add_node(\"shared\", \"value\", type: :fact)\nend\n\ndef test_2_retrieves_node  # Fails if test_1 doesn't run first\n  node = @htm.retrieve(\"shared\")\n  assert node\nend\n\n# Good: Each test is independent\ndef test_creates_node\n  @htm.add_node(\"test_create\", \"value\", type: :fact)\n  node = @htm.retrieve(\"test_create\")\n  assert node\nend\n\ndef test_retrieves_node\n  @htm.add_node(\"test_retrieve\", \"value\", type: :fact)\n  node = @htm.retrieve(\"test_retrieve\")\n  assert node\nend\n</code></pre>"},{"location":"development/testing/#dont-use-sleep-for-timing","title":"DON'T: Use Sleep for Timing","text":"<pre><code># Bad: Flaky test with arbitrary sleep\ndef test_async_operation\n  start_operation\n  sleep 2  # Hope it finishes in 2 seconds\n  assert operation_complete?\nend\n\n# Good: Poll with timeout\ndef test_async_operation\n  start_operation\n  wait_until(timeout: 5) { operation_complete? }\n  assert operation_complete?\nend\n\ndef wait_until(timeout: 5)\n  start = Time.now\n  loop do\n    return if yield\n    raise \"Timeout\" if Time.now - start &gt; timeout\n    sleep 0.1\n  end\nend\n</code></pre>"},{"location":"development/testing/#dont-test-implementation-details","title":"DON'T: Test Implementation Details","text":"<pre><code># Bad: Testing internal implementation\ndef test_uses_specific_sql_query\n  assert_match /SELECT \\* FROM/, @htm.instance_variable_get(:@last_query)\nend\n\n# Good: Testing behavior/outcome\ndef test_retrieves_all_node_fields\n  @htm.add_node(\"key\", \"value\", type: :fact)\n  node = @htm.retrieve(\"key\")\n\n  assert node.key?('id')\n  assert node.key?('key')\n  assert node.key?('value')\n  assert node.key?('type')\nend\n</code></pre>"},{"location":"development/testing/#debugging-test-failures","title":"Debugging Test Failures","text":""},{"location":"development/testing/#run-single-test-with-verbose-output","title":"Run Single Test with Verbose Output","text":"<pre><code>ruby test/htm_test.rb -v -n test_specific_test\n</code></pre>"},{"location":"development/testing/#use-debug_me-in-tests","title":"Use debug_me in Tests","text":"<pre><code>require 'debug_me'\n\ndef test_something\n  debug_me { [ :input, :expected ] }\n\n  result = method_under_test(input)\n\n  debug_me { [ :result ] }\n\n  assert_equal expected, result\nend\n</code></pre>"},{"location":"development/testing/#check-test-data","title":"Check Test Data","text":"<pre><code>def test_database_state\n  # Add debugging to inspect state\n  pp @htm.memory_stats\n  pp @htm.working_memory.inspect\n\n  # Your test assertions\n  assert something\nend\n</code></pre>"},{"location":"development/testing/#use-ruby-debugger","title":"Use Ruby Debugger","text":"<pre><code># Install debugger\ngem install debug\n\n# Run test with debugger\nruby -r debug test/htm_test.rb\n\n# Set breakpoints in test\ndef test_something\n  debugger  # Execution will stop here\n  result = method_under_test\n  assert result\nend\n</code></pre>"},{"location":"development/testing/#testing-checklist","title":"Testing Checklist","text":"<p>Before submitting a pull request, ensure:</p> <ul> <li> All existing tests pass</li> <li> New features have tests</li> <li> Edge cases are tested</li> <li> Error conditions are tested</li> <li> Tests are clear and well-named</li> <li> Tests are independent (no order dependency)</li> <li> Integration tests clean up test data</li> <li> No skipped tests (unless explicitly documented)</li> <li> Tests run in reasonable time (&lt;5s for unit, &lt;30s for integration)</li> </ul>"},{"location":"development/testing/#resources","title":"Resources","text":""},{"location":"development/testing/#minitest-documentation","title":"Minitest Documentation","text":"<ul> <li>Official docs: https://docs.seattlerb.org/minitest/</li> <li>Minitest assertions: https://docs.seattlerb.org/minitest/Minitest/Assertions.html</li> <li>Minitest mocking: https://docs.seattlerb.org/minitest/Minitest/Mock.html</li> </ul>"},{"location":"development/testing/#testing-guides","title":"Testing Guides","text":"<ul> <li>Ruby Testing Guide: https://guides.rubyonrails.org/testing.html</li> <li>Better Specs: https://www.betterspecs.org/</li> </ul>"},{"location":"development/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Contributing Guide: Learn how to submit your tests</li> <li>Database Schema: Understand what you're testing</li> <li>Setup Guide: Get your test environment running</li> </ul> <p>Happy testing! Remember: Good tests make better code.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to HTM (Hierarchical Temporary Memory)! This section will help you get up and running quickly.</p>"},{"location":"getting-started/#overview","title":"Overview","text":"<p>HTM provides intelligent memory management for LLM-based applications (robots) with a two-tier architecture:</p> <ul> <li>Long-term Memory: Durable PostgreSQL storage with vector embeddings for semantic search</li> <li>Working Memory: Token-limited in-memory context for immediate LLM use</li> </ul>"},{"location":"getting-started/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> Installation</p> <p>Set up HTM in your Ruby project with all required dependencies including PostgreSQL, pgvector, and Ollama.</p> <p> Install HTM</p> </li> <li> <p> Quick Start</p> <p>Build your first memory-enabled robot in minutes with practical examples and code snippets.</p> <p> Get started</p> </li> </ul>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before installing HTM, ensure you have:</p> <ul> <li>Ruby 3.1+ - HTM uses modern Ruby features</li> <li>PostgreSQL 14+ - With pgvector and pg_trgm extensions</li> <li>Ollama (optional) - For local embedding generation</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ol> <li>Install HTM - Set up the gem and database</li> <li>Quick Start - Create your first memory-enabled robot</li> <li>Architecture Overview - Understand how HTM works</li> <li>Guides - Deep dive into specific features</li> </ol>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will walk you through setting up HTM and all its dependencies.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing HTM, ensure you have:</p> <ul> <li>Ruby 3.0 or higher - HTM requires modern Ruby features</li> <li>PostgreSQL 17+ - For the database backend</li> <li>Ollama - For generating vector embeddings (via RubyLLM)</li> </ul>"},{"location":"getting-started/installation/#check-your-ruby-version","title":"Check Your Ruby Version","text":"<pre><code>ruby --version\n# Should show: ruby 3.0.0 or higher\n</code></pre> <p>If you need to install or upgrade Ruby, we recommend using rbenv:</p> <pre><code># Install rbenv (macOS)\nbrew install rbenv ruby-build\n\n# Install Ruby 3.3 (latest stable)\nrbenv install 3.3.0\nrbenv global 3.3.0\n</code></pre>"},{"location":"getting-started/installation/#step-1-install-the-htm-gem","title":"Step 1: Install the HTM Gem","text":""},{"location":"getting-started/installation/#option-a-install-via-bundler-recommended","title":"Option A: Install via Bundler (Recommended)","text":"<p>Add HTM to your application's <code>Gemfile</code>:</p> <pre><code># Gemfile\nsource 'https://rubygems.org'\n\ngem 'htm'\n</code></pre> <p>Then install:</p> <pre><code>bundle install\n</code></pre>"},{"location":"getting-started/installation/#option-b-install-directly","title":"Option B: Install Directly","text":"<p>Install HTM directly via RubyGems:</p> <pre><code>gem install htm\n</code></pre>"},{"location":"getting-started/installation/#step-2-database-setup","title":"Step 2: Database Setup","text":"<p>HTM requires PostgreSQL 17+ with the pgvector extension.</p>"},{"location":"getting-started/installation/#option-a-local-postgresql-installation","title":"Option A: Local PostgreSQL Installation","text":""},{"location":"getting-started/installation/#macos-using-homebrew","title":"macOS (using Homebrew)","text":"<pre><code># Install PostgreSQL\nbrew install postgresql@17\n\n# Start PostgreSQL service\nbrew services start postgresql@17\n</code></pre>"},{"location":"getting-started/installation/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code># Add PostgreSQL repository\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" &gt; /etc/apt/sources.list.d/pgdg.list'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n\n# Install PostgreSQL\nsudo apt-get update\nsudo apt-get install postgresql-17 postgresql-client-17\n\n# Start PostgreSQL service\nsudo systemctl start postgresql\nsudo systemctl enable postgresql\n</code></pre>"},{"location":"getting-started/installation/#create-database","title":"Create Database","text":"<pre><code># Create database and user\ncreatedb htm_db\npsql htm_db\n\n# In psql console:\nCREATE EXTENSION IF NOT EXISTS pgvector;\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\n</code></pre>"},{"location":"getting-started/installation/#configure-environment-variables","title":"Configure Environment Variables","text":"<pre><code># Add to ~/.bashrc or your preferred config file\nexport HTM_DBURL=\"postgres://username:password@localhost:5432/htm_db\"\nexport HTM_DBNAME=\"htm_db\"\nexport HTM_DBUSER=\"your_username\"\nexport HTM_DBPASS=\"your_password\"\nexport HTM_DBPORT=\"5432\"\nexport HTM_DBHOST=\"localhost\"\n\n# Load the configuration\nsource ~/.bashrc\n</code></pre> <p>Environment Configuration</p> <p>HTM automatically uses the <code>HTM_DBURL</code> environment variable if available. You can also pass database configuration directly to <code>HTM.new()</code>.</p> <p>Set environment variable:</p> <pre><code>export HTM_DBURL=\"postgres://localhost/htm_db\"\n</code></pre>"},{"location":"getting-started/installation/#step-3-enable-postgresql-extensions","title":"Step 3: Enable PostgreSQL Extensions","text":"<p>HTM requires two PostgreSQL extensions:</p> <ul> <li>pgvector: Vector similarity search</li> <li>pg_trgm: Full-text search</li> </ul>"},{"location":"getting-started/installation/#verify-extensions","title":"Verify Extensions","text":"<p>Test your database connection and verify extensions:</p> <pre><code># Download or use the included test script\ncd /path/to/your/project\nruby -e \"\nrequire 'pg'\nconn = PG.connect(ENV['HTM_DBURL'])\nresult = conn.exec('SELECT extname, extversion FROM pg_extension ORDER BY extname')\nresult.each { |row| puts \\\"\u2713 #{row['extname']}: Version #{row['extversion']}\\\" }\nconn.close\n\"\n</code></pre> <p>Expected output:</p> <pre><code>\u2713 pg_trgm: Version 1.6\n\u2713 pgvector: Version 0.8.1\n</code></pre> <p>Missing Extensions</p> <p>If extensions are missing, you may need to install them. On Debian/Ubuntu: <code>sudo apt-get install postgresql-17-pgvector</code>. On macOS: <code>brew install pgvector</code>.</p>"},{"location":"getting-started/installation/#step-4-install-ollama","title":"Step 4: Install Ollama","text":"<p>HTM uses Ollama via RubyLLM for generating vector embeddings.</p>"},{"location":"getting-started/installation/#install-ollama","title":"Install Ollama","text":""},{"location":"getting-started/installation/#macos","title":"macOS","text":"<pre><code># Option 1: Direct download\ncurl https://ollama.ai/install.sh | sh\n\n# Option 2: Homebrew\nbrew install ollama\n</code></pre>"},{"location":"getting-started/installation/#linux","title":"Linux","text":"<pre><code>curl https://ollama.ai/install.sh | sh\n</code></pre>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<p>Download the installer from https://ollama.ai/download</p>"},{"location":"getting-started/installation/#start-ollama-service","title":"Start Ollama Service","text":"<pre><code># Ollama typically starts automatically\n# Verify it's running:\ncurl http://localhost:11434/api/version\n</code></pre> <p>Expected output:</p> <pre><code>{\"version\":\"0.1.x\"}\n</code></pre>"},{"location":"getting-started/installation/#pull-the-gpt-oss-model","title":"Pull the gpt-oss Model","text":"<p>HTM uses the <code>gpt-oss</code> model by default:</p> <pre><code># Download the model\nollama pull gpt-oss\n\n# Verify the model is available\nollama list\n</code></pre> <p>You should see <code>gpt-oss</code> in the list.</p>"},{"location":"getting-started/installation/#test-embedding-generation","title":"Test Embedding Generation","text":"<pre><code># Test that embeddings work\nollama run gpt-oss \"Hello, world!\"\n</code></pre>"},{"location":"getting-started/installation/#custom-ollama-url-optional","title":"Custom Ollama URL (Optional)","text":"<p>If Ollama is running on a different host or port:</p> <pre><code>export OLLAMA_URL=\"http://custom-host:11434\"\n</code></pre> <p>Ollama Model Selection</p> <p>HTM defaults to <code>gpt-oss</code>, but you can use any embedding model supported by Ollama. Just pass <code>embedding_model: 'your-model'</code> to <code>HTM.new()</code>.</p>"},{"location":"getting-started/installation/#step-5-initialize-htm-database-schema","title":"Step 5: Initialize HTM Database Schema","text":"<p>Run the database setup to create HTM's tables and schema:</p>"},{"location":"getting-started/installation/#option-a-using-ruby","title":"Option A: Using Ruby","text":"<pre><code>require 'htm'\n\n# Run database setup\nHTM::Database.setup\n</code></pre>"},{"location":"getting-started/installation/#option-b-using-command-line","title":"Option B: Using Command Line","text":"<pre><code>ruby -r ./lib/htm -e \"HTM::Database.setup\"\n</code></pre>"},{"location":"getting-started/installation/#option-c-using-rake-task-if-available","title":"Option C: Using Rake Task (if available)","text":"<pre><code>rake db:setup\n</code></pre> <p>This creates the following tables:</p> <ul> <li><code>nodes</code>: Main memory storage with vector embeddings</li> <li><code>relationships</code>: Knowledge graph connections</li> <li><code>tags</code>: Flexible categorization</li> <li><code>robots</code>: Robot registry</li> <li><code>operations_log</code>: Audit trail</li> </ul> <p>Schema Created</p> <p>You'll see confirmation messages as each table and index is created.</p>"},{"location":"getting-started/installation/#step-6-verify-installation","title":"Step 6: Verify Installation","text":"<p>Create a test script to verify everything works:</p> <pre><code># test_htm_setup.rb\nrequire 'htm'\n\nputs \"Testing HTM Installation...\"\n\n# Initialize HTM\nhtm = HTM.new(\n  robot_name: \"Test Robot\",\n  working_memory_size: 128_000,\n  embedding_service: :ollama,\n  embedding_model: 'gpt-oss'\n)\n\nputs \"\u2713 HTM initialized successfully\"\nputs \"  Robot ID: #{htm.robot_id}\"\nputs \"  Robot Name: #{htm.robot_name}\"\n\n# Add a test memory\nhtm.add_node(\n  \"test_001\",\n  \"This is a test memory to verify HTM installation.\",\n  type: :fact,\n  importance: 5.0,\n  tags: [\"test\"]\n)\n\nputs \"\u2713 Memory node added successfully\"\n\n# Retrieve the memory\nnode = htm.retrieve(\"test_001\")\nif node\n  puts \"\u2713 Memory retrieval works\"\n  puts \"  Content: #{node['value']}\"\nelse\n  puts \"\u2717 Failed to retrieve memory\"\nend\n\n# Get stats\nstats = htm.memory_stats\nputs \"\u2713 Memory stats:\"\nputs \"  Total nodes: #{stats[:total_nodes]}\"\nputs \"  Working memory: #{stats[:working_memory][:node_count]} nodes\"\n\n# Clean up test data\nhtm.forget(\"test_001\", confirm: :confirmed)\nputs \"\u2713 Memory deletion works\"\n\nputs \"\\n\" + \"=\" * 60\nputs \"\u2713 HTM installation verified successfully!\"\n</code></pre> <p>Run the test:</p> <pre><code>ruby test_htm_setup.rb\n</code></pre>"},{"location":"getting-started/installation/#environment-variables-reference","title":"Environment Variables Reference","text":"<p>HTM uses the following environment variables:</p> Variable Description Default Required <code>HTM_DBURL</code> PostgreSQL connection URL - Yes <code>HTM_DBNAME</code> Database name <code>htm_db</code> No <code>HTM_DBUSER</code> Database user <code>postgres</code> No <code>HTM_DBPASS</code> Database password - No <code>HTM_DBPORT</code> Database port <code>5432</code> No <code>OLLAMA_URL</code> Ollama API URL <code>http://localhost:11434</code> No"},{"location":"getting-started/installation/#example-configuration-file","title":"Example Configuration File","text":"<p>Create a configuration file for easy loading:</p> <pre><code># ~/.bashrc__htm\nexport HTM_DBURL=\"postgres://user:pass@host:port/db?sslmode=require\"\nexport OLLAMA_URL=\"http://localhost:11434\"\n</code></pre> <p>Load it in your shell:</p> <pre><code># Add to ~/.bashrc or ~/.zshrc\nsource ~/.bashrc__htm\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#database-connection-issues","title":"Database Connection Issues","text":"<p>Error: <code>PG::ConnectionBad: connection failed</code></p> <p>Solutions:</p> <pre><code># 1. Verify HTM_DBURL is set\necho $HTM_DBURL\n\n# 2. Test connection manually\npsql $HTM_DBURL\n\n# 3. Check if PostgreSQL is running (local installs)\npg_ctl status\n\n# 4. Verify SSL mode for cloud databases\n# Ensure URL includes: ?sslmode=require\n</code></pre>"},{"location":"getting-started/installation/#ollama-connection-issues","title":"Ollama Connection Issues","text":"<p>Error: <code>Connection refused - connect(2) for localhost:11434</code></p> <p>Solutions:</p> <pre><code># 1. Check if Ollama is running\ncurl http://localhost:11434/api/version\n\n# 2. Start Ollama (macOS)\n# Check Activity Monitor or menu bar\n\n# 3. Restart Ollama service\nkillall ollama\nollama serve\n\n# 4. Verify gpt-oss model is installed\nollama list | grep gpt-oss\n</code></pre>"},{"location":"getting-started/installation/#missing-extensions","title":"Missing Extensions","text":"<p>Error: <code>PG::UndefinedObject: extension \"pgvector\" is not available</code></p> <p>Solutions:</p> <pre><code># Install pgvector\ngit clone https://github.com/pgvector/pgvector.git\ncd pgvector\nmake\nsudo make install\n\n# Enable in database\npsql $HTM_DBURL -c \"CREATE EXTENSION IF NOT EXISTS pgvector;\"\n</code></pre>"},{"location":"getting-started/installation/#ruby-version-issues","title":"Ruby Version Issues","text":"<p>Error: <code>htm requires Ruby version &gt;= 3.0.0</code></p> <p>Solutions:</p> <pre><code># Check current version\nruby --version\n\n# Install newer Ruby via rbenv\nrbenv install 3.3.0\nrbenv global 3.3.0\n\n# Verify\nruby --version\n</code></pre>"},{"location":"getting-started/installation/#permission-issues","title":"Permission Issues","text":"<p>Error: <code>PG::InsufficientPrivilege: permission denied</code></p> <p>Solutions:</p> <pre><code># Ensure your database user has necessary permissions\npsql $HTM_DBURL -c \"\n  GRANT ALL PRIVILEGES ON DATABASE your_db TO your_user;\n  GRANT ALL ON ALL TABLES IN SCHEMA public TO your_user;\n\"\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that HTM is installed, you're ready to start building:</p> <ol> <li>Quick Start Guide: Build your first HTM application in 5 minutes</li> <li>User Guide: Learn all HTM features in depth</li> <li>API Reference: Explore the complete API documentation</li> <li>Examples: See real-world usage examples</li> </ol>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the Troubleshooting section above</li> <li>Review GitHub Issues</li> <li>Open a new issue with:</li> <li>Your Ruby version (<code>ruby --version</code>)</li> <li>Your PostgreSQL version (<code>psql --version</code>)</li> <li>Error messages and stack traces</li> <li>Steps to reproduce</li> </ol>"},{"location":"getting-started/installation/#additional-resources","title":"Additional Resources","text":"<ul> <li>Ollama Documentation: https://ollama.ai/</li> <li>pgvector Documentation: https://github.com/pgvector/pgvector</li> <li>PostgreSQL Documentation: https://www.postgresql.org/docs/</li> <li>RubyLLM Documentation: https://github.com/madbomber/ruby_llm</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>Get started with HTM in just 5 minutes! This guide will walk you through building your first HTM-powered application.</p> <p>Prerequisites</p> <p>Make sure you've completed the Installation Guide before starting this tutorial.</p> <p> <p>HTM Quick Start Workflow</p> <p> Step 1 Initialize HTM HTM.new() Set robot name</p> <p></p> <p> Step 2 Add Memories add_node() Store knowledge</p> <p></p> <p> Step 3 Recall Memories recall() Search &amp; retrieve</p> <p></p> <p> Step 4 Use Context create_context() For LLM prompts</p> <p>HTM Memory System</p> <p> Working Memory (Fast) \u2022 Token-limited (128K) \u2022 In-memory storage \u2022 Immediate LLM access O(1) lookups</p> <p> Long-Term Memory (Durable) \u2022 Unlimited storage \u2022 PostgreSQL \u2022 RAG search (vector + text) Permanent storage</p> <p> Stored in both</p> <p> Evicted when full</p> <p> Recalled when needed</p> <p> Quick Example Code: htm = HTM.new(robot_name: \"My Assistant\") htm.add_node(\"key1\", \"Remember this fact\", type: :fact) memories = htm.recall(timeframe: \"today\", topic: \"fact\")</p> <p> </p>"},{"location":"getting-started/quick-start/#your-first-htm-application","title":"Your First HTM Application","text":"<p>Let's build a simple coding assistant that remembers project decisions and preferences.</p>"},{"location":"getting-started/quick-start/#step-1-create-your-project","title":"Step 1: Create Your Project","text":"<p>Create a new Ruby file:</p> <pre><code># my_first_htm_app.rb\nrequire 'htm'\n\nputs \"My First HTM Application\"\nputs \"=\" * 60\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-initialize-htm","title":"Step 2: Initialize HTM","text":"<p>Create an HTM instance for your robot:</p> <pre><code># Initialize HTM with a robot name\nhtm = HTM.new(\n  robot_name: \"Code Helper\",\n  working_memory_size: 128_000,    # 128k tokens\n  embedding_service: :ollama,       # Use Ollama for embeddings\n  embedding_model: 'gpt-oss'        # Default embedding model\n)\n\nputs \"\u2713 HTM initialized for '#{htm.robot_name}'\"\nputs \"  Robot ID: #{htm.robot_id}\"\nputs \"  Working Memory: #{htm.working_memory.max_tokens} tokens\"\n</code></pre> <p>What's happening here?</p> <ul> <li><code>robot_name</code>: A human-readable name for your AI robot</li> <li><code>working_memory_size</code>: Maximum tokens for active context (128k is typical)</li> <li><code>embedding_service</code>: Service to generate vector embeddings (<code>:ollama</code> is default)</li> <li><code>embedding_model</code>: Which model to use for embeddings (<code>gpt-oss</code> is default)</li> </ul> <p>Robot Identity</p> <p>Each HTM instance represents one robot. The <code>robot_id</code> is automatically generated (UUID) and used to track which robot created each memory.</p>"},{"location":"getting-started/quick-start/#step-3-add-your-first-memory","title":"Step 3: Add Your First Memory","text":"<p>Add a project decision to HTM's memory:</p> <pre><code>puts \"\\n1. Adding a project decision...\"\n\nhtm.add_node(\n  \"decision_001\",                    # Unique key\n  \"We decided to use PostgreSQL for the database \" \\\n  \"because it provides excellent time-series optimization and \" \\\n  \"native vector search with pgvector.\",\n  type: :decision,                   # Memory type\n  category: \"architecture\",          # Optional category\n  importance: 9.0,                   # Importance score (0-10)\n  tags: [\"database\", \"architecture\"] # Searchable tags\n)\n\nputs \"\u2713 Decision added to memory\"\n</code></pre> <p>Memory Components:</p> <ul> <li>Key: Unique identifier (e.g., <code>\"decision_001\"</code>)</li> <li>Value: The actual content/memory text</li> <li>Type: Category of memory (<code>:decision</code>, <code>:fact</code>, <code>:code</code>, <code>:preference</code>, etc.)</li> <li>Category: Optional grouping</li> <li>Importance: Score from 0.0 to 10.0 (affects recall priority)</li> <li>Tags: Searchable keywords for organization</li> </ul> <p>Automatic Embeddings</p> <p>HTM automatically generates vector embeddings for the memory content using Ollama. You don't need to handle embeddings yourself!</p>"},{"location":"getting-started/quick-start/#step-4-add-more-memories","title":"Step 4: Add More Memories","text":"<p>Let's add a few more memories:</p> <pre><code>puts \"\\n2. Adding user preferences...\"\n\nhtm.add_node(\n  \"pref_001\",\n  \"User prefers using the debug_me gem for debugging instead of puts statements.\",\n  type: :preference,\n  category: \"coding_style\",\n  importance: 7.0,\n  tags: [\"debugging\", \"ruby\", \"preferences\"]\n)\n\nputs \"\u2713 Preference added\"\n\nputs \"\\n3. Adding a code pattern...\"\n\nhtm.add_node(\n  \"code_001\",\n  \"For database queries, use connection pooling with the connection_pool gem \" \\\n  \"to handle concurrent requests efficiently.\",\n  type: :code,\n  category: \"patterns\",\n  importance: 8.0,\n  tags: [\"database\", \"performance\", \"ruby\"],\n  related_to: [\"decision_001\"]  # Link to related memory\n)\n\nputs \"\u2713 Code pattern added (linked to decision_001)\"\n</code></pre> <p>Notice the <code>related_to</code> parameter? This creates a relationship in the knowledge graph, linking related memories together.</p>"},{"location":"getting-started/quick-start/#step-5-retrieve-a-specific-memory","title":"Step 5: Retrieve a Specific Memory","text":"<p>Retrieve a memory by its key:</p> <pre><code>puts \"\\n4. Retrieving specific memory...\"\n\nmemory = htm.retrieve(\"decision_001\")\n\nif memory\n  puts \"\u2713 Found memory:\"\n  puts \"  Key: #{memory['key']}\"\n  puts \"  Type: #{memory['type']}\"\n  puts \"  Content: #{memory['value'][0..100]}...\"\n  puts \"  Importance: #{memory['importance']}\"\n  puts \"  Created: #{memory['created_at']}\"\nelse\n  puts \"\u2717 Memory not found\"\nend\n</code></pre>"},{"location":"getting-started/quick-start/#step-6-recall-memories-by-topic","title":"Step 6: Recall Memories by Topic","text":"<p>Use HTM's powerful recall feature to find relevant memories:</p> <pre><code>puts \"\\n5. Recalling memories about 'database'...\"\n\nmemories = htm.recall(\n  timeframe: \"last week\",          # Natural language time filter\n  topic: \"database\",               # What to search for\n  limit: 10,                       # Max results\n  strategy: :hybrid                # Search strategy (vector + full-text)\n)\n\nputs \"\u2713 Found #{memories.length} relevant memories:\"\nmemories.each_with_index do |mem, idx|\n  puts \"  #{idx + 1}. [#{mem['type']}] #{mem['value'][0..60]}...\"\nend\n</code></pre> <p>Search Strategies:</p> <ul> <li><code>:vector</code>: Semantic similarity search using embeddings</li> <li><code>:fulltext</code>: Keyword-based PostgreSQL full-text search</li> <li><code>:hybrid</code>: Combines both for best results (recommended)</li> </ul> <p>Timeframe Options:</p> <ul> <li><code>\"last week\"</code> - Last 7 days</li> <li><code>\"yesterday\"</code> - Previous day</li> <li><code>\"last 30 days\"</code> - Last month</li> <li><code>\"this month\"</code> - Current calendar month</li> <li>Date ranges: <code>(Time.now - 7.days)..Time.now</code></li> </ul>"},{"location":"getting-started/quick-start/#step-7-create-context-for-your-llm","title":"Step 7: Create Context for Your LLM","text":"<p>Generate a context string optimized for LLM consumption:</p> <pre><code>puts \"\\n6. Creating context for LLM...\"\n\ncontext = htm.create_context(\n  strategy: :balanced,             # Balance importance and recency\n  max_tokens: 50_000               # Optional token limit\n)\n\nputs \"\u2713 Context created: #{context.length} characters\"\nputs \"\\nContext preview:\"\nputs context[0..300]\nputs \"...\"\n</code></pre> <p>Context Strategies:</p> <ul> <li><code>:recent</code>: Most recent memories first</li> <li><code>:important</code>: Highest importance scores first</li> <li><code>:balanced</code>: Combines importance \u00d7 recency (recommended)</li> </ul> <p>This context can be directly injected into your LLM prompt:</p> <pre><code># Example: Using context with your LLM\nprompt = &lt;&lt;~PROMPT\n  You are a helpful coding assistant.\n\n  Here's what you remember from past conversations:\n  #{context}\n\n  User: What database did we decide to use for the project?\nPROMPT\n\n# response = your_llm.chat(prompt)\n</code></pre>"},{"location":"getting-started/quick-start/#step-8-check-memory-statistics","title":"Step 8: Check Memory Statistics","text":"<p>View statistics about your memory usage:</p> <pre><code>puts \"\\n7. Memory Statistics:\"\n\nstats = htm.memory_stats\n\nputs \"  Total nodes in long-term memory: #{stats[:total_nodes]}\"\nputs \"  Active robots: #{stats[:active_robots]}\"\nputs \"  Working memory usage: #{stats[:working_memory][:current_tokens]} / \" \\\n     \"#{stats[:working_memory][:max_tokens]} tokens \" \\\n     \"(#{stats[:working_memory][:utilization].round(2)}%)\"\nputs \"  Database size: #{(stats[:database_size] / (1024.0 ** 2)).round(2)} MB\"\n</code></pre>"},{"location":"getting-started/quick-start/#complete-example","title":"Complete Example","text":"<p>Here's the complete script:</p> <pre><code>#!/usr/bin/env ruby\n# my_first_htm_app.rb\nrequire 'htm'\n\nputs \"My First HTM Application\"\nputs \"=\" * 60\n\n# Step 1: Initialize HTM\nhtm = HTM.new(\n  robot_name: \"Code Helper\",\n  working_memory_size: 128_000,\n  embedding_service: :ollama,\n  embedding_model: 'gpt-oss'\n)\n\nputs \"\u2713 HTM initialized for '#{htm.robot_name}'\"\n\n# Step 2: Add memories\nhtm.add_node(\n  \"decision_001\",\n  \"We decided to use PostgreSQL for the database.\",\n  type: :decision,\n  category: \"architecture\",\n  importance: 9.0,\n  tags: [\"database\", \"architecture\"]\n)\n\nhtm.add_node(\n  \"pref_001\",\n  \"User prefers using the debug_me gem for debugging.\",\n  type: :preference,\n  importance: 7.0,\n  tags: [\"debugging\", \"ruby\"]\n)\n\nputs \"\u2713 Memories added\"\n\n# Step 3: Recall memories\nmemories = htm.recall(\n  timeframe: \"last week\",\n  topic: \"database\",\n  strategy: :hybrid\n)\n\nputs \"\u2713 Found #{memories.length} memories about 'database'\"\n\n# Step 4: Create context\ncontext = htm.create_context(strategy: :balanced)\nputs \"\u2713 Context created: #{context.length} characters\"\n\n# Step 5: View statistics\nstats = htm.memory_stats\nputs \"\u2713 Total nodes: #{stats[:total_nodes]}\"\n\nputs \"\\n\" + \"=\" * 60\nputs \"Success! Your first HTM application is working.\"\n</code></pre> <p>Run it:</p> <pre><code>ruby my_first_htm_app.rb\n</code></pre>"},{"location":"getting-started/quick-start/#multi-robot-example","title":"Multi-Robot Example","text":"<p>HTM's \"hive mind\" feature allows multiple robots to share memory. Here's how:</p> <pre><code>require 'htm'\n\n# Create two different robots\nrobot_a = HTM.new(robot_name: \"Code Assistant\")\nrobot_b = HTM.new(robot_name: \"Documentation Writer\")\n\n# Robot A adds a memory\nrobot_a.add_node(\n  \"shared_001\",\n  \"The API documentation is stored in the docs/ directory.\",\n  type: :fact,\n  importance: 8.0\n)\n\nputs \"Robot A added memory\"\n\n# Robot B can access the same memory!\nmemories = robot_b.recall(\n  timeframe: \"last week\",\n  topic: \"documentation\",\n  strategy: :hybrid\n)\n\nputs \"Robot B found #{memories.length} memories\"\n# Robot B sees Robot A's memory!\n\n# Track which robot said what\nbreakdown = robot_b.which_robot_said(\"documentation\")\nputs \"Who mentioned 'documentation':\"\nbreakdown.each do |robot_id, count|\n  puts \"  #{robot_id}: #{count} times\"\nend\n</code></pre> <p>Use cases for multi-robot:</p> <ul> <li>Collaborative coding teams of AI agents</li> <li>Customer service handoffs between agents</li> <li>Research assistants building shared knowledge</li> <li>Teaching AI learning from multiple instructors</li> </ul>"},{"location":"getting-started/quick-start/#working-with-relationships","title":"Working with Relationships","text":"<p>Build a knowledge graph by linking related memories:</p> <pre><code># Add parent concept\nhtm.add_node(\n  \"concept_databases\",\n  \"Databases store and organize data persistently.\",\n  type: :fact,\n  importance: 5.0\n)\n\n# Add child concept with relationship\nhtm.add_node(\n  \"concept_postgresql\",\n  \"PostgreSQL is a powerful open-source relational database.\",\n  type: :fact,\n  importance: 7.0,\n  related_to: [\"concept_databases\"]  # Links to parent\n)\n\n# Add another related concept\nhtm.add_node(\n  \"concept_postgresql\",\n  \"PostgreSQL provides robust relational database capabilities.\",\n  type: :fact,\n  importance: 8.0,\n  related_to: [\"concept_postgresql\", \"concept_databases\"]\n)\n\n# Now you have a knowledge graph:\n# concept_databases\n#   \u251c\u2500\u2500 concept_postgresql\n#   \u2502    \u2514\u2500\u2500 concept_postgresql\n</code></pre>"},{"location":"getting-started/quick-start/#forget-explicit-deletion","title":"Forget (Explicit Deletion)","text":"<p>HTM follows a \"never forget\" philosophy, but you can explicitly delete memories:</p> <pre><code># Deletion requires confirmation\nhtm.forget(\"old_decision\", confirm: :confirmed)\n\nputs \"\u2713 Memory deleted\"\n</code></pre> <p>Deletion is Permanent</p> <p>The <code>forget()</code> method permanently deletes data. This is the only way to delete memories in HTM. Working memory evictions move data to long-term storage, they don't delete it.</p>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've learned the basics of HTM. Here's what to explore next:</p>"},{"location":"getting-started/quick-start/#explore-advanced-features","title":"Explore Advanced Features","text":"<ul> <li>User Guide: Deep dive into all HTM features</li> <li>API Reference: Complete API documentation</li> <li>Architecture Guide: Understand HTM's internals</li> </ul>"},{"location":"getting-started/quick-start/#build-real-applications","title":"Build Real Applications","text":"<p>Try building:</p> <ol> <li>Personal AI Assistant: Remember user preferences and habits</li> <li>Code Review Bot: Track coding patterns and past decisions</li> <li>Research Assistant: Build a knowledge graph from documents</li> <li>Customer Service Bot: Maintain conversation history</li> </ol>"},{"location":"getting-started/quick-start/#experiment-with-different-configurations","title":"Experiment with Different Configurations","text":"<pre><code># Try different memory sizes\nhtm = HTM.new(\n  robot_name: \"Large Memory Bot\",\n  working_memory_size: 256_000  # 256k tokens\n)\n\n# Try different embedding models\nhtm = HTM.new(\n  robot_name: \"Custom Embeddings\",\n  embedding_service: :ollama,\n  embedding_model: 'llama2'  # Use Llama2 instead of gpt-oss\n)\n\n# Try different recall strategies\nmemories = htm.recall(\n  timeframe: \"last month\",\n  topic: \"important decisions\",\n  strategy: :vector  # Pure semantic search\n)\n</code></pre>"},{"location":"getting-started/quick-start/#performance-optimization","title":"Performance Optimization","text":"<p>For production applications:</p> <ul> <li>Use connection pooling (built-in)</li> <li>Tune working memory size based on your LLM's context window</li> <li>Adjust importance scores to prioritize critical memories</li> <li>Use appropriate timeframes to limit search scope</li> <li>Monitor memory statistics regularly</li> </ul>"},{"location":"getting-started/quick-start/#join-the-community","title":"Join the Community","text":"<ul> <li>GitHub: https://github.com/madbomber/htm</li> <li>Issues: Report bugs or request features</li> <li>Discussions: Share your HTM projects</li> </ul>"},{"location":"getting-started/quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quick-start/#pattern-1-conversation-memory","title":"Pattern 1: Conversation Memory","text":"<pre><code># Store user messages\nhtm.add_node(\n  \"msg_#{Time.now.to_i}\",\n  \"User: How do I optimize database queries?\",\n  type: :context,\n  importance: 6.0,\n  tags: [\"conversation\", \"question\"]\n)\n\n# Store assistant responses\nhtm.add_node(\n  \"response_#{Time.now.to_i}\",\n  \"Assistant: Use indexes and connection pooling.\",\n  type: :context,\n  importance: 6.0,\n  tags: [\"conversation\", \"answer\"]\n)\n</code></pre>"},{"location":"getting-started/quick-start/#pattern-2-learning-from-code","title":"Pattern 2: Learning from Code","text":"<pre><code># Extract patterns from code reviews\nhtm.add_node(\n  \"pattern_#{SecureRandom.hex(4)}\",\n  \"Always validate user input before database queries.\",\n  type: :code,\n  importance: 9.0,\n  tags: [\"security\", \"validation\", \"best-practice\"]\n)\n</code></pre>"},{"location":"getting-started/quick-start/#pattern-3-decision-tracking","title":"Pattern 3: Decision Tracking","text":"<pre><code># Document architectural decisions\nhtm.add_node(\n  \"adr_001\",\n  \"Decision: Use microservices architecture. \" \\\n  \"Reasoning: Better scalability and independent deployment.\",\n  type: :decision,\n  category: \"architecture\",\n  importance: 10.0,\n  tags: [\"adr\", \"architecture\", \"microservices\"]\n)\n</code></pre>"},{"location":"getting-started/quick-start/#troubleshooting-quick-start","title":"Troubleshooting Quick Start","text":""},{"location":"getting-started/quick-start/#issue-connection-refused-error","title":"Issue: \"Connection refused\" error","text":"<p>Solution: Make sure Ollama is running:</p> <pre><code>curl http://localhost:11434/api/version\n# If this fails, start Ollama\n</code></pre>"},{"location":"getting-started/quick-start/#issue-database-connection-failed","title":"Issue: \"Database connection failed\"","text":"<p>Solution: Verify your <code>HTM_DBURL</code> is set:</p> <pre><code>echo $HTM_DBURL\n# Should show your connection string\n</code></pre>"},{"location":"getting-started/quick-start/#issue-embeddings-taking-too-long","title":"Issue: Embeddings taking too long","text":"<p>Solution: Check Ollama's status and ensure the model is downloaded:</p> <pre><code>ollama list | grep gpt-oss\n# Should show gpt-oss model\n</code></pre>"},{"location":"getting-started/quick-start/#issue-memory-not-found-during-recall","title":"Issue: Memory not found during recall","text":"<p>Solution: Check your timeframe. If you just added a memory, use a recent timeframe:</p> <pre><code># Instead of \"last week\", use:\nmemories = htm.recall(\n  timeframe: (Time.now - 3600)..Time.now,  # Last hour\n  topic: \"your topic\"\n)\n</code></pre>"},{"location":"getting-started/quick-start/#additional-resources","title":"Additional Resources","text":"<ul> <li>Installation Guide: Complete setup instructions</li> <li>User Guide: Comprehensive feature documentation</li> <li>API Reference: Detailed API documentation</li> <li>Examples: Real-world code examples</li> </ul> <p>Happy coding with HTM! \ud83d\ude80</p>"},{"location":"guides/","title":"HTM User Guides","text":"<p>Welcome to the HTM (Hierarchical Temporary Memory) user guide collection. These guides will help you understand and effectively use HTM for building intelligent LLM-based applications with persistent memory.</p>"},{"location":"guides/#what-is-htm","title":"What is HTM?","text":"<p>HTM is an intelligent memory management system for LLM robots that implements a two-tier architecture:</p> <ul> <li>Working Memory: Token-limited active context for immediate LLM use</li> <li>Long-term Memory: Durable PostgreSQL storage for permanent knowledge</li> </ul> <p>HTM enables your robots to recall context from past conversations using RAG (Retrieval-Augmented Generation), creating continuity across sessions and enabling sophisticated multi-robot collaboration.</p>"},{"location":"guides/#guide-categories","title":"Guide Categories","text":""},{"location":"guides/#getting-started","title":"Getting Started","text":"<p>Perfect for developers new to HTM or those building their first application.</p> <ul> <li>Getting Started Guide - Your first HTM application, basic concepts, and common patterns</li> </ul>"},{"location":"guides/#core-operations","title":"Core Operations","text":"<p>Learn how to work with HTM's memory system effectively.</p> <ul> <li>Adding Memories - How to store different types of information in HTM</li> <li>Recalling Memories - Search strategies and retrieval techniques</li> <li>Working Memory Management - Understanding token limits and eviction</li> <li>Long-term Memory - Database operations and maintenance</li> </ul>"},{"location":"guides/#advanced-features","title":"Advanced Features","text":"<p>Dive deeper into HTM's powerful capabilities.</p> <ul> <li>Multi-Robot Usage - Building hive mind systems with multiple robots</li> <li>Search Strategies - Vector, full-text, and hybrid search</li> <li>Context Assembly - Creating optimized context for LLMs</li> </ul>"},{"location":"guides/#learning-path","title":"Learning Path","text":"<p>We recommend the following progression:</p> <ol> <li>Start Here: Getting Started Guide</li> <li>Understand HTM's architecture</li> <li>Build your first application</li> <li> <p>Learn basic operations</p> </li> <li> <p>Core Skills: Memory Operations</p> </li> <li>Adding Memories - Store information effectively</li> <li>Recalling Memories - Retrieve what you need</li> <li> <p>Context Assembly - Use memories with LLMs</p> </li> <li> <p>Deep Understanding: Memory Management</p> </li> <li>Working Memory - Token management</li> <li>Long-term Memory - Database operations</li> <li> <p>Search Strategies - Optimize retrieval</p> </li> <li> <p>Advanced Topics: Multi-Robot Systems</p> </li> <li>Multi-Robot Usage - Build collaborative systems</li> </ol>"},{"location":"guides/#quick-reference","title":"Quick Reference","text":""},{"location":"guides/#common-tasks","title":"Common Tasks","text":"<ul> <li>Initialize HTM: See Getting Started</li> <li>Add a memory: See Adding Memories</li> <li>Search for memories: See Recalling Memories</li> <li>Create LLM context: See Context Assembly</li> <li>Monitor memory usage: See Working Memory</li> <li>Multi-robot setup: See Multi-Robot Usage</li> </ul>"},{"location":"guides/#memory-types","title":"Memory Types","text":"<p>HTM supports six memory types, each optimized for different use cases:</p> Type Purpose Example <code>:fact</code> Immutable facts \"User's name is Alice\" <code>:context</code> Conversation state \"Discussing database architecture\" <code>:code</code> Code snippets \"Ruby function for parsing dates\" <code>:preference</code> User preferences \"Prefers dark theme\" <code>:decision</code> Design decisions \"Chose PostgreSQL for storage\" <code>:question</code> Unresolved questions \"Should we add caching?\""},{"location":"guides/#search-strategies","title":"Search Strategies","text":"Strategy Method Best For Vector Semantic similarity Conceptual searches, related topics Full-text Keyword matching Exact terms, specific phrases Hybrid Combined approach Best overall accuracy"},{"location":"guides/#getting-help","title":"Getting Help","text":"<ul> <li>Examples: Check the <code>examples/</code> directory in the HTM repository</li> <li>API Reference: See the API documentation</li> <li>Tests: Look at <code>test/</code> directory for usage examples</li> <li>Issues: Report bugs on GitHub</li> </ul>"},{"location":"guides/#documentation-conventions","title":"Documentation Conventions","text":"<p>Throughout these guides, you'll see these admonitions:</p> <p>Tip</p> <p>Helpful advice and best practices</p> <p>Warning</p> <p>Important warnings about potential issues</p> <p>Note</p> <p>Additional information and context</p> <p>Example</p> <p>Code examples and usage demonstrations</p>"},{"location":"guides/#next-steps","title":"Next Steps","text":"<p>Ready to get started? Head over to the Getting Started Guide to build your first HTM application.</p>"},{"location":"guides/adding-memories/","title":"Adding Memories to HTM","text":"<p>This guide covers everything you need to know about storing information in HTM effectively.</p>"},{"location":"guides/adding-memories/#basic-usage","title":"Basic Usage","text":"<p>The primary method for adding memories is <code>remember</code>:</p> <pre><code>node_id = htm.remember(content, tags: [], metadata: {})\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>content</code> String required The information to remember <code>tags</code> Array\\&lt;String&gt; <code>[]</code> Manual tags to assign (in addition to auto-extracted tags) <code>metadata</code> Hash <code>{}</code> Arbitrary key-value metadata stored as JSONB <p>The method returns the database ID of the created node.</p>"},{"location":"guides/adding-memories/#how-remember-works","title":"How Remember Works","text":"<p>When you call <code>remember()</code>:</p> <ol> <li>Content hashing: A SHA-256 hash of the content is computed</li> <li>Deduplication check: If a node with the same hash exists, reuse it</li> <li>Node creation/linking: Create new node OR link robot to existing node</li> <li>Working memory: Add node to working memory (evict if needed)</li> <li>Background jobs: Enqueue embedding and tag generation (async)</li> </ol> <pre><code># First robot remembers something\nnode_id = htm.remember(\"PostgreSQL supports vector similarity search\")\n# =&gt; 123 (new node created)\n\n# Same content remembered again (by same or different robot)\nnode_id = htm.remember(\"PostgreSQL supports vector similarity search\")\n# =&gt; 123 (same node_id returned, just updates remember_count)\n</code></pre>"},{"location":"guides/adding-memories/#content-types","title":"Content Types","text":"<p>HTM doesn't enforce content types - just store meaningful text that stands alone:</p>"},{"location":"guides/adding-memories/#facts","title":"Facts","text":"<pre><code># User information\nhtm.remember(\"The user's name is Alice Thompson\")\n\n# System configuration\nhtm.remember(\"System timezone is UTC\")\n\n# Domain knowledge\nhtm.remember(\"Photosynthesis converts light energy into chemical energy in plants\")\n</code></pre>"},{"location":"guides/adding-memories/#preferences","title":"Preferences","text":"<pre><code># Communication style\nhtm.remember(\"User prefers concise answers with bullet points\")\n\n# Technical preferences\nhtm.remember(\"User prefers Ruby over Python for scripting tasks\")\n</code></pre>"},{"location":"guides/adding-memories/#decisions","title":"Decisions","text":"<pre><code># Technology choice\nhtm.remember(&lt;&lt;~DECISION)\n  Decision: Use PostgreSQL with pgvector for HTM storage\n\n  Rationale:\n  - Excellent vector search via pgvector\n  - Strong consistency guarantees\n  - Mature ecosystem\n\n  Alternatives considered:\n  - MongoDB (rejected: eventual consistency issues)\n  - Redis (rejected: limited persistence)\nDECISION\n</code></pre>"},{"location":"guides/adding-memories/#code-snippets","title":"Code Snippets","text":"<pre><code># Function example\nhtm.remember(&lt;&lt;~CODE)\n  def parse_date(date_string)\n    Date.parse(date_string)\n  rescue ArgumentError\n    nil\n  end\nCODE\n\n# SQL query pattern\nhtm.remember(&lt;&lt;~SQL)\n  SELECT u.id, u.name, COUNT(o.id) as order_count\n  FROM users u\n  LEFT JOIN orders o ON u.id = o.user_id\n  GROUP BY u.id, u.name\n  HAVING COUNT(o.id) &gt; 10\nSQL\n</code></pre>"},{"location":"guides/adding-memories/#using-tags","title":"Using Tags","text":"<p>Tags provide hierarchical organization for your memories. HTM automatically extracts tags from content, but you can also specify manual tags.</p>"},{"location":"guides/adding-memories/#hierarchical-tag-convention","title":"Hierarchical Tag Convention","text":"<p>Use colons to create hierarchical namespaces:</p> <pre><code># Manual tags with hierarchy\nhtm.remember(\n  \"PostgreSQL 17 adds MERGE statement improvements\",\n  tags: [\"database:postgresql\", \"database:sql\", \"version:17\"]\n)\n\n# Tags are used in hybrid search for relevance boosting\n# A recall for \"postgresql\" will boost nodes with matching tags\n</code></pre>"},{"location":"guides/adding-memories/#tag-naming-conventions","title":"Tag Naming Conventions","text":"<pre><code># Good: Consistent, lowercase, hierarchical\ntags: [\"database:postgresql\", \"architecture:api\", \"security:authentication\"]\n\n# Avoid: Inconsistent casing, flat tags, vague terms\ntags: [\"PostgreSQL\", \"stuff\", \"misc\"]\n</code></pre>"},{"location":"guides/adding-memories/#common-tag-patterns","title":"Common Tag Patterns","text":"<pre><code># Domain tags\ntags: [\"database:postgresql\", \"api:rest\", \"auth:jwt\"]\n\n# Layer tags\ntags: [\"layer:frontend\", \"layer:backend\", \"layer:infrastructure\"]\n\n# Technology tags\ntags: [\"tech:ruby\", \"tech:javascript\", \"tech:docker\"]\n\n# Project tags\ntags: [\"project:alpha\", \"project:beta\"]\n</code></pre>"},{"location":"guides/adding-memories/#automatic-tag-extraction","title":"Automatic Tag Extraction","text":"<p>When a node is created, a background job (GenerateTagsJob) automatically extracts hierarchical tags from the content using an LLM. This happens asynchronously.</p> <pre><code># Just provide content, tags are auto-extracted\nhtm.remember(\"We're using Redis for session caching with a 24-hour TTL\")\n# Background job might extract: [\"database:redis\", \"caching:session\", \"performance\"]\n</code></pre>"},{"location":"guides/adding-memories/#using-metadata","title":"Using Metadata","text":"<p>Metadata provides flexible key-value storage for arbitrary data that doesn't fit into tags. Unlike tags (which are for hierarchical categorization), metadata is for structured data like version numbers, priorities, source systems, or any custom attributes.</p>"},{"location":"guides/adding-memories/#basic-metadata-usage","title":"Basic Metadata Usage","text":"<pre><code># Store with metadata\nhtm.remember(\n  \"User prefers dark mode\",\n  metadata: { category: \"preference\", priority: \"high\" }\n)\n\n# Multiple metadata fields\nhtm.remember(\n  \"API endpoint changed from /v1 to /v2\",\n  metadata: {\n    category: \"migration\",\n    version: 2,\n    breaking_change: true,\n    affected_services: [\"web\", \"mobile\"]\n  }\n)\n</code></pre>"},{"location":"guides/adding-memories/#metadata-vs-tags","title":"Metadata vs Tags","text":"Feature Tags Metadata Structure Hierarchical (colon-separated) Flat key-value pairs Type String only Any JSON type (string, number, boolean, array, object) Search Prefix matching (<code>LIKE 'ai:%'</code>) JSONB containment (<code>@&gt;</code>) Purpose Categorization &amp; navigation Arbitrary attributes &amp; filtering Auto-extraction Yes (via LLM) No (always explicit)"},{"location":"guides/adding-memories/#common-metadata-patterns","title":"Common Metadata Patterns","text":"<pre><code># Version tracking\nhtm.remember(\"API uses OAuth 2.0\", metadata: { version: 3, deprecated: false })\n\n# Source tracking\nhtm.remember(\"Error rate is 0.1%\", metadata: { source: \"monitoring\", dashboard: \"errors\" })\n\n# Priority/importance\nhtm.remember(\"Deploy to prod on Fridays is forbidden\", metadata: { priority: \"critical\" })\n\n# Environment-specific\nhtm.remember(\"Database connection limit is 100\", metadata: { environment: \"production\" })\n\n# Combining with tags\nhtm.remember(\n  \"Use connection pooling for better performance\",\n  tags: [\"database:postgresql\", \"performance\"],\n  metadata: { priority: \"high\", reviewed: true, author: \"dba-team\" }\n)\n</code></pre>"},{"location":"guides/adding-memories/#querying-by-metadata","title":"Querying by Metadata","text":"<p>Use the <code>metadata</code> parameter in <code>recall()</code> to filter by metadata:</p> <pre><code># Find all high-priority items\nhtm.recall(\"settings\", metadata: { priority: \"high\" })\n\n# Find production-specific configurations\nhtm.recall(\"database\", metadata: { environment: \"production\" })\n\n# Combine with other filters\nhtm.recall(\n  \"API changes\",\n  timeframe: \"last month\",\n  metadata: { breaking_change: true },\n  strategy: :hybrid\n)\n</code></pre> <p>Metadata filtering uses PostgreSQL's JSONB containment operator (<code>@&gt;</code>), which means the node's metadata must contain all the key-value pairs you specify.</p>"},{"location":"guides/adding-memories/#content-deduplication","title":"Content Deduplication","text":"<p>HTM automatically deduplicates content across all robots using SHA-256 hashing.</p>"},{"location":"guides/adding-memories/#how-it-works","title":"How It Works","text":"<pre><code># Robot 1 remembers something\nrobot1 = HTM.new(robot_name: \"assistant_1\")\nnode_id = robot1.remember(\"Ruby 3.3 supports YJIT by default\")\n# =&gt; 123 (new node)\n\n# Robot 2 remembers the same thing\nrobot2 = HTM.new(robot_name: \"assistant_2\")\nnode_id = robot2.remember(\"Ruby 3.3 supports YJIT by default\")\n# =&gt; 123 (same node_id! Content matched by hash)\n</code></pre>"},{"location":"guides/adding-memories/#robot-node-association","title":"Robot-Node Association","text":"<p>Each robot-node relationship is tracked in <code>robot_nodes</code>:</p> <pre><code># Check how many times a robot has \"remembered\" content\nrn = HTM::Models::RobotNode.find_by(robot_id: htm.robot_id, node_id: node_id)\nrn.remember_count      # =&gt; 3 (remembered 3 times)\nrn.first_remembered_at # =&gt; When first encountered\nrn.last_remembered_at  # =&gt; When last tried to remember\n</code></pre>"},{"location":"guides/adding-memories/#best-practices","title":"Best Practices","text":""},{"location":"guides/adding-memories/#1-make-content-self-contained","title":"1. Make Content Self-Contained","text":"<pre><code># Good: Self-contained, understandable without context\nhtm.remember(\n  \"Decided to use Redis for session storage because it provides fast access and automatic expiration\"\n)\n\n# Bad: Requires external context\nhtm.remember(\"Use Redis\")  # Why? For what?\n</code></pre>"},{"location":"guides/adding-memories/#2-include-rich-context","title":"2. Include Rich Context","text":"<pre><code># Good: Includes rationale and alternatives\nhtm.remember(&lt;&lt;~DECISION)\n  Decision: Use OAuth 2.0 for authentication\n\n  Rationale:\n  - Industry standard\n  - Better security than basic auth\n  - Supports SSO\n\n  Alternatives considered:\n  - Basic auth (rejected: security concerns)\n  - Custom tokens (rejected: maintenance burden)\nDECISION\n</code></pre>"},{"location":"guides/adding-memories/#3-use-hierarchical-tags","title":"3. Use Hierarchical Tags","text":"<pre><code># Good: Rich tags for multiple retrieval paths\nhtm.remember(\n  \"JWT tokens are stateless authentication tokens\",\n  tags: [\"auth:jwt\", \"security:tokens\", \"architecture:stateless\"]\n)\n\n# Suboptimal: Flat or minimal tags\nhtm.remember(\"JWT info\", tags: [\"jwt\"])\n</code></pre>"},{"location":"guides/adding-memories/#4-keep-content-focused","title":"4. Keep Content Focused","text":"<pre><code># Good: One concept per memory\nhtm.remember(\"PostgreSQL's EXPLAIN ANALYZE shows actual execution times\")\nhtm.remember(\"PostgreSQL's EXPLAIN shows the query plan without executing\")\n\n# Suboptimal: Multiple unrelated concepts\nhtm.remember(\"PostgreSQL has EXPLAIN and also supports JSON and has good performance\")\n</code></pre>"},{"location":"guides/adding-memories/#async-processing","title":"Async Processing","text":"<p>Embedding generation and tag extraction happen asynchronously:</p>"},{"location":"guides/adding-memories/#workflow","title":"Workflow","text":"<pre><code># 1. Node created immediately (~15ms)\nnode_id = htm.remember(\"Important fact about databases\")\n# Returns immediately with node_id\n\n# 2. Background jobs enqueue (async)\n# - GenerateEmbeddingJob runs (~100ms)\n# - GenerateTagsJob runs (~1 second)\n\n# 3. Node is eventually enriched\n# - embedding field populated (enables vector search)\n# - tags associated (enables tag navigation and boosting)\n</code></pre>"},{"location":"guides/adding-memories/#immediate-vs-eventual-capabilities","title":"Immediate vs Eventual Capabilities","text":"Capability Available Notes Full-text search Immediately Works on content Basic retrieval Immediately By node ID Vector search After ~100ms Needs embedding Tag-enhanced search After ~1s Needs tags Hybrid search After ~1s Needs embedding + tags"},{"location":"guides/adding-memories/#working-memory-integration","title":"Working Memory Integration","text":"<p>When you <code>remember()</code>, the node is automatically added to working memory:</p> <pre><code># Remember adds to both LTM and WM\nhtm.remember(\"Important fact\")\n\n# Check working memory\nstats = htm.working_memory.stats\nputs \"Nodes in WM: #{stats[:node_count]}\"\nputs \"Token usage: #{stats[:utilization]}%\"\n</code></pre>"},{"location":"guides/adding-memories/#eviction","title":"Eviction","text":"<p>If working memory is full, older/less important nodes are evicted to make room:</p> <pre><code># Working memory has a token budget\nhtm = HTM.new(working_memory_size: 128_000)  # 128K tokens\n\n# As you remember more, older items may be evicted from WM\n# They remain in LTM and can be recalled later\n</code></pre>"},{"location":"guides/adding-memories/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/adding-memories/#batch-operations","title":"Batch Operations","text":"<p>Each <code>remember()</code> call is a database operation. For bulk inserts:</p> <pre><code># Multiple memories\nfacts = [\n  \"PostgreSQL supports JSONB\",\n  \"PostgreSQL has excellent indexing\",\n  \"PostgreSQL handles concurrent writes well\"\n]\n\nfacts.each do |fact|\n  htm.remember(fact)\nend\n</code></pre>"},{"location":"guides/adding-memories/#content-length","title":"Content Length","text":"<p>Longer content takes more time to process:</p> <pre><code># Short text: Fast (~15ms save, ~100ms embedding)\nhtm.remember(\"User name is Alice\")\n\n# Long text: Slower (~15ms save, ~500ms embedding)\nhtm.remember(\"...\" * 1000)  # 1000 chars\n</code></pre> <p>For very long content (&gt;1000 tokens), consider splitting into multiple memories.</p>"},{"location":"guides/adding-memories/#next-steps","title":"Next Steps","text":"<p>Now that you know how to add memories effectively, learn about:</p> <ul> <li>Search Strategies - Optimize retrieval with different strategies</li> <li>Recalling Memories - Search and retrieve memories</li> </ul>"},{"location":"guides/adding-memories/#complete-example","title":"Complete Example","text":"<pre><code>require 'htm'\n\nhtm = HTM.new(robot_name: \"Memory Demo\")\n\n# Add a fact\nhtm.remember(\n  \"Alice Thompson is a senior software engineer specializing in distributed systems\"\n)\n\n# Add a preference with metadata\nhtm.remember(\n  \"Alice prefers Vim for editing and tmux for terminal management\",\n  metadata: { category: \"preference\", source: \"user-interview\" }\n)\n\n# Add a decision with context, tags, and metadata\nhtm.remember(&lt;&lt;~DECISION, tags: [\"architecture\", \"messaging\"], metadata: { priority: \"high\", approved: true, version: 1 })\n  Decision: Use RabbitMQ for async job processing\n\n  Rationale:\n  - Need reliable message delivery\n  - Support for multiple consumer patterns\n  - Excellent Ruby client library\n\n  Alternatives:\n  - Redis (simpler but less reliable)\n  - Kafka (overkill for our scale)\nDECISION\n\n# Add implementation code with metadata\nhtm.remember(&lt;&lt;~RUBY, tags: [\"code:ruby\", \"messaging:rabbitmq\"], metadata: { language: \"ruby\", tested: true })\n  require 'bunny'\n\n  connection = Bunny.new(ENV['RABBITMQ_URL'])\n  connection.start\n\n  channel = connection.create_channel\n  queue = channel.queue('jobs', durable: true)\nRUBY\n\nputs \"Added memories with relationships and rich metadata\"\nputs \"Stats: #{HTM::Models::Node.count} total nodes\"\n\n# Query by metadata\nhigh_priority = htm.recall(\"decisions\", metadata: { priority: \"high\" })\nputs \"High priority decisions: #{high_priority.count}\"\n</code></pre>"},{"location":"guides/context-assembly/","title":"Context Assembly","text":"<p>Context assembly is the process of converting working memory into a formatted string that can be used with your LLM. This guide covers the three assembly strategies, optimization techniques, and best practices for creating high-quality context.</p>"},{"location":"guides/context-assembly/#what-is-context-assembly","title":"What is Context Assembly?","text":"<p>Context assembly transforms working memory into LLM-ready context:</p> <p> <p>Context Assembly Process</p> <p> Working Memory (Nodes)</p> <p>Node 1 1000 tok Node 2 500 tok Node 3 2000 tok Node 4 800 tok Node 5 1200 tok ...</p> <p> Assembly Strategy</p> <p> :recent Sort by access time</p> <p> :important Sort by importance</p> <p> :balanced Weighted formula</p> <p>Assembles until max_tokens reached</p> <p> Context String (Ordered)</p> <p>Node 3 (important) Node 1 (recent) Node 5 (balanced) Node 2 (fits) ... \u2713 Within token limit</p> <p> LLM Prompt</p> <p>System: You are a helpful assistant... Context from memory: [Assembled Context String] User: How do we handle auth? Assistant:</p> <p> select &amp; sort</p> <p> assemble</p> <p> insert into prompt</p> <p> 4700 / 5000 tokens </p>"},{"location":"guides/context-assembly/#basic-usage","title":"Basic Usage","text":"<p>The <code>create_context</code> method assembles context from working memory:</p> <pre><code># Basic context assembly\ncontext = htm.create_context(\n  strategy: :balanced,    # Assembly strategy\n  max_tokens: nil        # Optional token limit\n)\n\n# Use with your LLM\nprompt = &lt;&lt;~PROMPT\n  Context from memory:\n  #{context}\n\n  User question: How do we handle authentication?\n\n  Assistant:\nPROMPT\n\n# Send to LLM...\nresponse = llm.complete(prompt)\n</code></pre>"},{"location":"guides/context-assembly/#assembly-strategies","title":"Assembly Strategies","text":"<p>HTM provides three strategies for assembling context, each optimized for different use cases.</p>"},{"location":"guides/context-assembly/#recent-strategy","title":"Recent Strategy","text":"<p>The <code>:recent</code> strategy prioritizes newest memories first.</p> <pre><code>context = htm.create_context(strategy: :recent)\n</code></pre> <p>How it works:</p> <ol> <li>Sort memories by access time (most recent first)</li> <li>Add memories in order until token limit reached</li> <li>Return assembled context</li> </ol> <p>Best for:</p> <ul> <li>Continuing recent conversations</li> <li>Session-based interactions</li> <li>Short-term context tracking</li> <li>Real-time applications</li> </ul> <p>Example:</p> <pre><code># Chat application\nclass ChatBot\n  def initialize\n    @htm = HTM.new(robot_name: \"Chat\", working_memory_size: 128_000)\n    @turn = 0\n  end\n\n  def chat(user_message)\n    @turn += 1\n\n    # Add user message\n    @htm.add_node(\n      \"turn_#{@turn}_user\",\n      \"User: #{user_message}\",\n      type: :context,\n      importance: 6.0\n    )\n\n    # Get recent context\n    context = @htm.create_context(\n      strategy: :recent,\n      max_tokens: 10_000\n    )\n\n    # Generate response\n    response = llm_generate(context, user_message)\n\n    # Store assistant response\n    @htm.add_node(\n      \"turn_#{@turn}_assistant\",\n      \"Assistant: #{response}\",\n      type: :context,\n      importance: 6.0\n    )\n\n    response\n  end\n\n  private\n\n  def llm_generate(context, message)\n    # Your LLM integration here\n    \"Generated response based on context\"\n  end\nend\n</code></pre>"},{"location":"guides/context-assembly/#important-strategy","title":"Important Strategy","text":"<p>The <code>:important</code> strategy prioritizes high-importance memories.</p> <pre><code>context = htm.create_context(strategy: :important)\n</code></pre> <p>How it works:</p> <ol> <li>Sort memories by importance (highest first)</li> <li>Add memories in order until token limit reached</li> <li>Return assembled context</li> </ol> <p>Best for:</p> <ul> <li>Critical information retention</li> <li>System constraints and rules</li> <li>User preferences</li> <li>Core knowledge base</li> <li>Decision-making support</li> </ul> <p>Example:</p> <pre><code># Knowledge base with priorities\nclass KnowledgeBot\n  def initialize\n    @htm = HTM.new(robot_name: \"Knowledge\")\n\n    # Add critical system constraints\n    @htm.add_node(\n      \"constraint_001\",\n      \"CRITICAL: Never expose API keys in responses\",\n      type: :fact,\n      importance: 10.0\n    )\n\n    # Add important user preferences\n    @htm.add_node(\n      \"pref_001\",\n      \"User prefers concise explanations\",\n      type: :preference,\n      importance: 8.0\n    )\n\n    # Add general knowledge\n    @htm.add_node(\n      \"fact_001\",\n      \"Python uses indentation for code blocks\",\n      type: :fact,\n      importance: 5.0\n    )\n  end\n\n  def answer_question(question)\n    # Get most important context first\n    context = @htm.create_context(\n      strategy: :important,\n      max_tokens: 5_000\n    )\n\n    # Critical constraints and preferences are included first\n    generate_answer(context, question)\n  end\n\n  private\n\n  def generate_answer(context, question)\n    # LLM integration\n    \"Answer based on important context\"\n  end\nend\n</code></pre>"},{"location":"guides/context-assembly/#balanced-strategy-recommended","title":"Balanced Strategy (Recommended)","text":"<p>The <code>:balanced</code> strategy combines importance and recency using a weighted formula.</p> <pre><code>context = htm.create_context(strategy: :balanced)\n</code></pre> <p>How it works:</p> <ol> <li>Calculate score: <code>importance \u00d7 (1 / (1 + age_in_hours))</code></li> <li>Sort by score (highest first)</li> <li>Add memories until token limit reached</li> <li>Return assembled context</li> </ol> <p>Scoring examples:</p> <pre><code># Recent + Important: High score\n# Importance: 9.0, Age: 1 hour\n# Score: 9.0 \u00d7 (1 / (1 + 1)) = 4.5 \u2713 Included\n\n# Old + Important: Medium score\n# Importance: 9.0, Age: 24 hours\n# Score: 9.0 \u00d7 (1 / (1 + 24)) = 0.36 \u2248 Maybe\n\n# Recent + Unimportant: Low score\n# Importance: 2.0, Age: 1 hour\n# Score: 2.0 \u00d7 (1 / (1 + 1)) = 1.0 \u2248 Maybe\n\n# Old + Unimportant: Very low score\n# Importance: 2.0, Age: 24 hours\n# Score: 2.0 \u00d7 (1 / (1 + 24)) = 0.08 \u2717 Excluded\n</code></pre> <p>Best for:</p> <ul> <li>General-purpose applications (recommended default)</li> <li>Mixed temporal needs</li> <li>Production systems</li> <li>Balanced context requirements</li> </ul> <p>Example:</p> <pre><code># General-purpose assistant\nclass Assistant\n  def initialize\n    @htm = HTM.new(\n      robot_name: \"Assistant\",\n      working_memory_size: 128_000\n    )\n  end\n\n  def process(user_input)\n    # Add user input\n    @htm.add_node(\n      \"input_#{Time.now.to_i}\",\n      user_input,\n      type: :context,\n      importance: 7.0\n    )\n\n    # Get balanced context (recent + important)\n    context = @htm.create_context(\n      strategy: :balanced,\n      max_tokens: 50_000\n    )\n\n    # Use context with LLM\n    generate_response(context, user_input)\n  end\n\n  private\n\n  def generate_response(context, input)\n    prompt = &lt;&lt;~PROMPT\n      You are a helpful assistant with access to memory.\n\n      Context from memory:\n      #{context}\n\n      User: #{input}\n\n      Assistant:\n    PROMPT\n\n    # Send to LLM\n    llm_complete(prompt)\n  end\n\n  def llm_complete(prompt)\n    # Your LLM integration\n    \"Generated response\"\n  end\nend\n</code></pre>"},{"location":"guides/context-assembly/#token-limits","title":"Token Limits","text":"<p>Control context size with token limits:</p> <pre><code># Use default (working memory size)\ncontext = htm.create_context(strategy: :balanced)\n\n# Custom limit\ncontext = htm.create_context(\n  strategy: :balanced,\n  max_tokens: 50_000\n)\n\n# Small context for simple queries\ncontext = htm.create_context(\n  strategy: :recent,\n  max_tokens: 5_000\n)\n\n# Large context for complex tasks\ncontext = htm.create_context(\n  strategy: :important,\n  max_tokens: 200_000\n)\n</code></pre> <p>Choosing token limits:</p> Limit Use Case 2K-5K Simple Q&amp;A, quick lookups 10K-20K Standard conversations 50K-100K Complex analysis, code generation 100K+ Document processing, extensive context <p>LLM Context Windows</p> <p>Don't exceed your LLM's context window: - GPT-3.5: 4K-16K tokens - GPT-4: 8K-128K tokens - Claude: 100K-200K tokens - Llama 2: 4K tokens</p>"},{"location":"guides/context-assembly/#strategy-comparison","title":"Strategy Comparison","text":""},{"location":"guides/context-assembly/#performance","title":"Performance","text":"<pre><code>require 'benchmark'\n\n# Add 1000 test memories\n1000.times do |i|\n  htm.add_node(\n    \"test_#{i}\",\n    \"Memory #{i}\",\n    importance: rand(1.0..10.0)\n  )\nend\n\n# Benchmark strategies\nBenchmark.bm(15) do |x|\n  x.report(\"Recent:\") do\n    100.times { htm.create_context(strategy: :recent) }\n  end\n\n  x.report(\"Important:\") do\n    100.times { htm.create_context(strategy: :important) }\n  end\n\n  x.report(\"Balanced:\") do\n    100.times { htm.create_context(strategy: :balanced) }\n  end\nend\n\n# Typical results:\n#                       user     system      total        real\n# Recent:           0.050000   0.000000   0.050000 (  0.051234)\n# Important:        0.045000   0.000000   0.045000 (  0.047891)\n# Balanced:         0.080000   0.000000   0.080000 (  0.082456)\n</code></pre> <p>Notes:</p> <ul> <li><code>:recent</code> is fastest (simple sort)</li> <li><code>:important</code> is fast (simple sort)</li> <li><code>:balanced</code> is slower (complex calculation)</li> <li>All are typically &lt; 100ms for normal working memory sizes</li> </ul>"},{"location":"guides/context-assembly/#quality-comparison","title":"Quality Comparison","text":"<pre><code># Test scenario: Mix of old important and recent unimportant data\n\n# Setup\nhtm = HTM.new(robot_name: \"Test\")\n\n# Add old important data\nhtm.add_node(\"old_critical\", \"Critical system constraint\", importance: 10.0)\nsleep 1  # Simulate age\n\n# Add recent unimportant data\n20.times do |i|\n  htm.add_node(\"recent_#{i}\", \"Recent note #{i}\", importance: 2.0)\nend\n\n# Compare strategies\nputs \"=== Recent Strategy ===\"\ncontext = htm.create_context(strategy: :recent, max_tokens: 1000)\nputs context.include?(\"Critical system constraint\") ? \"\u2713 Has critical\" : \"\u2717 Missing critical\"\n\nputs \"\\n=== Important Strategy ===\"\ncontext = htm.create_context(strategy: :important, max_tokens: 1000)\nputs context.include?(\"Critical system constraint\") ? \"\u2713 Has critical\" : \"\u2717 Missing critical\"\n\nputs \"\\n=== Balanced Strategy ===\"\ncontext = htm.create_context(strategy: :balanced, max_tokens: 1000)\nputs context.include?(\"Critical system constraint\") ? \"\u2713 Has critical\" : \"\u2717 Missing critical\"\n\n# Results:\n# Recent: \u2717 Missing critical (prioritized recent notes)\n# Important: \u2713 Has critical (prioritized by importance)\n# Balanced: \u2713 Has critical (balanced approach)\n</code></pre>"},{"location":"guides/context-assembly/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"guides/context-assembly/#1-multi-strategy-context","title":"1. Multi-Strategy Context","text":"<p>Use multiple strategies for comprehensive context:</p> <pre><code>def multi_strategy_context(max_tokens_per_strategy: 10_000)\n  # Get different perspectives\n  recent = htm.create_context(\n    strategy: :recent,\n    max_tokens: max_tokens_per_strategy\n  )\n\n  important = htm.create_context(\n    strategy: :important,\n    max_tokens: max_tokens_per_strategy\n  )\n\n  # Combine (you might want to deduplicate)\n  combined = &lt;&lt;~CONTEXT\n    === Recent Context ===\n    #{recent}\n\n    === Important Context ===\n    #{important}\n  CONTEXT\n\n  combined\nend\n</code></pre>"},{"location":"guides/context-assembly/#2-dynamic-strategy-selection","title":"2. Dynamic Strategy Selection","text":"<p>Choose strategy based on query type:</p> <pre><code>def smart_context(query)\n  strategy = if query.match?(/recent|latest|current/)\n    :recent\n  elsif query.match?(/important|critical|must/)\n    :important\n  else\n    :balanced\n  end\n\n  htm.create_context(strategy: strategy, max_tokens: 20_000)\nend\n\n# Usage\ncontext = smart_context(\"What are the recent changes?\")    # Uses :recent\ncontext = smart_context(\"What are critical constraints?\")  # Uses :important\ncontext = smart_context(\"How do we handle auth?\")         # Uses :balanced\n</code></pre>"},{"location":"guides/context-assembly/#3-filtered-context","title":"3. Filtered Context","text":"<p>Include only specific types of memories:</p> <pre><code>def filtered_context(type:, strategy: :balanced)\n  # This requires custom implementation\n  # HTM doesn't expose working memory internals directly\n\n  # Workaround: Recall specific types\n  memories = htm.recall(\n    timeframe: \"last 24 hours\",\n    topic: \"type:#{type}\",  # Pseudo-filter\n    strategy: :hybrid,\n    limit: 50\n  ).select { |m| m['type'] == type.to_s }\n\n  # Manually assemble context\n  memories.map { |m| m['value'] }.join(\"\\n\\n\")\nend\n\n# Usage\nfacts_only = filtered_context(type: :fact)\ndecisions_only = filtered_context(type: :decision)\n</code></pre>"},{"location":"guides/context-assembly/#4-sectioned-context","title":"4. Sectioned Context","text":"<p>Organize context into sections:</p> <pre><code>def sectioned_context\n  # Get different types of context\n  facts = htm.recall(timeframe: \"all time\", topic: \"fact\")\n    .select { |m| m['type'] == 'fact' }\n    .first(5)\n\n  decisions = htm.recall(timeframe: \"all time\", topic: \"decision\")\n    .select { |m| m['type'] == 'decision' }\n    .first(5)\n\n  recent = htm.recall(timeframe: \"last hour\", topic: \"\", limit: 5)\n\n  # Format as sections\n  &lt;&lt;~CONTEXT\n    === Core Facts ===\n    #{facts.map { |f| \"- #{f['value']}\" }.join(\"\\n\")}\n\n    === Key Decisions ===\n    #{decisions.map { |d| \"- #{d['value']}\" }.join(\"\\n\")}\n\n    === Recent Activity ===\n    #{recent.map { |r| \"- #{r['value']}\" }.join(\"\\n\")}\n  CONTEXT\nend\n</code></pre>"},{"location":"guides/context-assembly/#5-token-aware-context","title":"5. Token-Aware Context","text":"<p>Ensure context fits LLM limits:</p> <pre><code>class TokenAwareContext\n  def initialize(htm, embedding_service)\n    @htm = htm\n    @embedding_service = embedding_service\n  end\n\n  def create(strategy:, llm_context_window:, reserve_for_prompt: 1000)\n    # Calculate available tokens\n    available = llm_context_window - reserve_for_prompt\n\n    # Get context\n    context = @htm.create_context(\n      strategy: strategy,\n      max_tokens: available\n    )\n\n    # Verify token count\n    actual_tokens = @embedding_service.count_tokens(context)\n\n    if actual_tokens &gt; available\n      warn \"Context exceeded limit! Truncating...\"\n      # Retry with smaller limit\n      context = @htm.create_context(\n        strategy: strategy,\n        max_tokens: available * 0.9  # 90% to be safe\n      )\n    end\n\n    context\n  end\nend\n\n# Usage\nembedding_service = HTM::EmbeddingService.new\ncontext_builder = TokenAwareContext.new(htm, embedding_service)\n\ncontext = context_builder.create(\n  strategy: :balanced,\n  llm_context_window: 100_000,  # Claude 100K\n  reserve_for_prompt: 2_000\n)\n</code></pre>"},{"location":"guides/context-assembly/#using-context-with-llms","title":"Using Context with LLMs","text":""},{"location":"guides/context-assembly/#pattern-1-system-prompt-context","title":"Pattern 1: System Prompt + Context","text":"<pre><code>def generate_with_context(user_query)\n  context = htm.create_context(strategy: :balanced, max_tokens: 50_000)\n\n  system_prompt = &lt;&lt;~SYSTEM\n    You are a helpful AI assistant with access to memory.\n    Use the provided context to answer questions accurately.\n  SYSTEM\n\n  user_prompt = &lt;&lt;~USER\n    Context from memory:\n    #{context}\n\n    ---\n\n    User question: #{user_query}\n\n    Please answer based on the context above.\n  USER\n\n  # Send to LLM with system + user prompts\n  llm.chat(system: system_prompt, user: user_prompt)\nend\n</code></pre>"},{"location":"guides/context-assembly/#pattern-2-conversation-history","title":"Pattern 2: Conversation History","text":"<pre><code>class ConversationManager\n  def initialize\n    @htm = HTM.new(robot_name: \"Chat\")\n    @conversation_id = SecureRandom.uuid\n  end\n\n  def add_turn(user_msg, assistant_msg)\n    timestamp = Time.now.to_i\n\n    @htm.add_node(\n      \"#{@conversation_id}_#{timestamp}_user\",\n      user_msg,\n      type: :context,\n      importance: 6.0,\n      tags: [\"conversation\", @conversation_id]\n    )\n\n    @htm.add_node(\n      \"#{@conversation_id}_#{timestamp}_assistant\",\n      assistant_msg,\n      type: :context,\n      importance: 6.0,\n      tags: [\"conversation\", @conversation_id]\n    )\n  end\n\n  def get_context_for_llm\n    # Get recent conversation\n    @htm.create_context(\n      strategy: :recent,\n      max_tokens: 10_000\n    )\n  end\nend\n</code></pre>"},{"location":"guides/context-assembly/#pattern-3-rag-with-context","title":"Pattern 3: RAG with Context","text":"<pre><code>def rag_query(question)\n  # 1. Retrieve relevant memories\n  relevant = htm.recall(\n    timeframe: \"last month\",\n    topic: question,\n    strategy: :hybrid,\n    limit: 10\n  )\n\n  # 2. Create context from working memory (includes retrieved + existing)\n  context = htm.create_context(\n    strategy: :balanced,\n    max_tokens: 30_000\n  )\n\n  # 3. Generate answer\n  prompt = &lt;&lt;~PROMPT\n    Context:\n    #{context}\n\n    Question: #{question}\n\n    Answer based on the context above:\n  PROMPT\n\n  llm.complete(prompt)\nend\n</code></pre>"},{"location":"guides/context-assembly/#optimization-tips","title":"Optimization Tips","text":""},{"location":"guides/context-assembly/#1-cache-context","title":"1. Cache Context","text":"<pre><code>class ContextCache\n  def initialize(htm, ttl: 60)\n    @htm = htm\n    @ttl = ttl\n    @cache = {}\n  end\n\n  def get_context(strategy:, max_tokens: nil)\n    cache_key = \"#{strategy}_#{max_tokens}\"\n\n    # Check cache\n    if cached = @cache[cache_key]\n      if Time.now - cached[:time] &lt; @ttl\n        return cached[:context]\n      end\n    end\n\n    # Generate new context\n    context = @htm.create_context(\n      strategy: strategy,\n      max_tokens: max_tokens\n    )\n\n    # Cache it\n    @cache[cache_key] = {\n      context: context,\n      time: Time.now\n    }\n\n    context\n  end\n\n  def invalidate\n    @cache.clear\n  end\nend\n\n# Usage\ncache = ContextCache.new(htm, ttl: 30)  # 30 second TTL\ncontext = cache.get_context(strategy: :balanced)  # Cached for 30s\n</code></pre>"},{"location":"guides/context-assembly/#2-progressive-context-loading","title":"2. Progressive Context Loading","text":"<pre><code>def progressive_context(start_tokens: 5_000, max_tokens: 50_000)\n  # Start small\n  context = htm.create_context(strategy: :balanced, max_tokens: start_tokens)\n\n  # Check if more context needed (based on your logic)\n  if needs_more_context?(context)\n    # Expand gradually\n    context = htm.create_context(strategy: :balanced, max_tokens: start_tokens * 2)\n  end\n\n  if still_needs_more?(context)\n    # Expand to max\n    context = htm.create_context(strategy: :balanced, max_tokens: max_tokens)\n  end\n\n  context\nend\n\ndef needs_more_context?(context)\n  # Your logic here\n  context.length &lt; 1000  # Example: too short\nend\n\ndef still_needs_more?(context)\n  # Your logic here\n  false  # Example\nend\n</code></pre>"},{"location":"guides/context-assembly/#3-selective-inclusion","title":"3. Selective Inclusion","text":"<pre><code>def selective_context(query)\n  # Determine what's relevant\n  include_facts = query.match?(/fact|truth|information/)\n  include_decisions = query.match?(/decision|choice|why/)\n  include_code = query.match?(/code|implement|example/)\n\n  # Build custom context\n  parts = []\n\n  if include_facts\n    facts = htm.recall(timeframe: \"all time\", topic: query)\n      .select { |m| m['type'] == 'fact' }\n      .first(5)\n    parts &lt;&lt; \"Facts:\\n\" + facts.map { |f| \"- #{f['value']}\" }.join(\"\\n\")\n  end\n\n  if include_decisions\n    decisions = htm.recall(timeframe: \"all time\", topic: query)\n      .select { |m| m['type'] == 'decision' }\n      .first(5)\n    parts &lt;&lt; \"Decisions:\\n\" + decisions.map { |d| \"- #{d['value']}\" }.join(\"\\n\")\n  end\n\n  if include_code\n    code = htm.recall(timeframe: \"all time\", topic: query)\n      .select { |m| m['type'] == 'code' }\n      .first(3)\n    parts &lt;&lt; \"Code Examples:\\n\" + code.map { |c| c['value'] }.join(\"\\n\\n\")\n  end\n\n  parts.join(\"\\n\\n\")\nend\n</code></pre>"},{"location":"guides/context-assembly/#best-practices","title":"Best Practices","text":""},{"location":"guides/context-assembly/#1-choose-the-right-strategy","title":"1. Choose the Right Strategy","text":"<pre><code># Use :recent for conversations\ncontext = htm.create_context(strategy: :recent)\n\n# Use :important for critical operations\ncontext = htm.create_context(strategy: :important)\n\n# Use :balanced as default (recommended)\ncontext = htm.create_context(strategy: :balanced)\n</code></pre>"},{"location":"guides/context-assembly/#2-set-appropriate-token-limits","title":"2. Set Appropriate Token Limits","text":"<pre><code># Don't exceed LLM context window\ncontext = htm.create_context(\n  strategy: :balanced,\n  max_tokens: 100_000  # Leave room for prompt\n)\n\n# Smaller contexts are faster\ncontext = htm.create_context(\n  strategy: :recent,\n  max_tokens: 5_000  # Quick queries\n)\n</code></pre>"},{"location":"guides/context-assembly/#3-monitor-context-quality","title":"3. Monitor Context Quality","text":"<pre><code>def monitor_context\n  context = htm.create_context(strategy: :balanced)\n\n  puts \"Context length: #{context.length} characters\"\n\n  # Count token estimate\n  embedding_service = HTM::EmbeddingService.new\n  tokens = embedding_service.count_tokens(context)\n  puts \"Estimated tokens: #{tokens}\"\n\n  # Check if too small or too large\n  warn \"Context very small!\" if tokens &lt; 500\n  warn \"Context very large!\" if tokens &gt; 100_000\nend\n</code></pre>"},{"location":"guides/context-assembly/#4-include-metadata","title":"4. Include Metadata","text":"<pre><code>def context_with_metadata\n  context = htm.create_context(strategy: :balanced, max_tokens: 20_000)\n\n  # Add metadata header\n  stats = htm.memory_stats\n\n  &lt;&lt;~CONTEXT\n    [Context assembled at #{Time.now}]\n    [Strategy: balanced]\n    [Working memory: #{stats[:working_memory][:node_count]} nodes]\n    [Robot: #{htm.robot_name}]\n\n    #{context}\n  CONTEXT\nend\n</code></pre>"},{"location":"guides/context-assembly/#complete-example","title":"Complete Example","text":"<pre><code>require 'htm'\n\n# Initialize HTM\nhtm = HTM.new(\n  robot_name: \"Context Demo\",\n  working_memory_size: 128_000\n)\n\n# Add various memories\nhtm.add_node(\"fact_001\", \"User prefers Ruby\", type: :fact, importance: 9.0)\nhtm.add_node(\"decision_001\", \"Use PostgreSQL\", type: :decision, importance: 8.0)\nhtm.add_node(\"context_001\", \"Currently debugging auth\", type: :context, importance: 7.0)\nhtm.add_node(\"code_001\", \"def auth...\", type: :code, importance: 6.0)\nhtm.add_node(\"note_001\", \"Check logs later\", type: :context, importance: 2.0)\n\nputs \"=== Recent Strategy ===\"\nrecent = htm.create_context(strategy: :recent, max_tokens: 5_000)\nputs recent\nputs \"\\n(Newest first)\"\n\nputs \"\\n=== Important Strategy ===\"\nimportant = htm.create_context(strategy: :important, max_tokens: 5_000)\nputs important\nputs \"\\n(Most important first)\"\n\nputs \"\\n=== Balanced Strategy ===\"\nbalanced = htm.create_context(strategy: :balanced, max_tokens: 5_000)\nputs balanced\nputs \"\\n(Recent + important)\"\n\n# Use with LLM\ndef ask_llm(context, question)\n  prompt = &lt;&lt;~PROMPT\n    Context:\n    #{context}\n\n    Question: #{question}\n    Answer:\n  PROMPT\n\n  # Send to your LLM here\n  puts \"\\n=== LLM Prompt ===\"\n  puts prompt\nend\n\nask_llm(balanced, \"What database are we using?\")\n</code></pre>"},{"location":"guides/context-assembly/#next-steps","title":"Next Steps","text":"<ul> <li>Recalling Memories - Populate working memory effectively</li> <li>Working Memory - Understand memory management</li> <li>Search Strategies - Optimize retrieval for context</li> </ul>"},{"location":"guides/getting-started/","title":"Getting Started with HTM","text":"<p>Welcome to HTM! This guide will help you build your first intelligent memory system for LLM-based applications.</p>"},{"location":"guides/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ol> <li>Ruby 3.0+ installed</li> <li>PostgreSQL with TimescaleDB (or access to a TimescaleDB cloud instance)</li> <li>Ollama installed and running (for embeddings)</li> <li>Basic understanding of Ruby and LLMs</li> </ol>"},{"location":"guides/getting-started/#installing-ollama","title":"Installing Ollama","text":"<p>HTM uses Ollama for generating vector embeddings by default:</p> <pre><code># Install Ollama\ncurl https://ollama.ai/install.sh | sh\n\n# Pull the gpt-oss model (default for HTM)\nollama pull gpt-oss\n\n# Verify Ollama is running\ncurl http://localhost:11434/api/version\n</code></pre> <p>Tip</p> <p>The gpt-oss model provides high-quality embeddings optimized for semantic search. HTM uses these embeddings to understand the meaning of your memories, not just keyword matches.</p>"},{"location":"guides/getting-started/#installation","title":"Installation","text":"<p>Add HTM to your Gemfile:</p> <pre><code>gem 'htm'\n</code></pre> <p>Then install:</p> <pre><code>bundle install\n</code></pre> <p>Or install directly:</p> <pre><code>gem install htm\n</code></pre>"},{"location":"guides/getting-started/#database-setup","title":"Database Setup","text":"<p>HTM requires a TimescaleDB database. Set your database connection:</p> <pre><code># Add to your .bashrc or .zshrc\nexport HTM_DBURL=\"postgresql://user:password@host:port/database\"\n\n# Or create a config file\necho \"export HTM_DBURL='your-connection-string'\" &gt; ~/.bashrc__tiger\nsource ~/.bashrc__tiger\n</code></pre> <p>Warning</p> <p>Keep your database credentials secure. Never commit them to version control.</p> <p>Initialize the database schema:</p> <pre><code>require 'htm'\n\n# Run once to create tables and indexes\nHTM::Database.setup\n</code></pre> <p>Or from the command line:</p> <pre><code>ruby -r ./lib/htm -e \"HTM::Database.setup\"\n</code></pre>"},{"location":"guides/getting-started/#your-first-htm-application","title":"Your First HTM Application","text":"<p>Let's build a simple application that demonstrates HTM's core features.</p>"},{"location":"guides/getting-started/#basic-initialization","title":"Basic Initialization","text":"<pre><code>require 'htm'\n\n# Create an HTM instance for your robot\nhtm = HTM.new(\n  robot_name: \"My Assistant\",\n  working_memory_size: 128_000  # 128K tokens\n)\n\nputs \"Robot initialized: #{htm.robot_name}\"\nputs \"Robot ID: #{htm.robot_id}\"\n</code></pre> <p>Note</p> <p>Each HTM instance represents a \"robot\" - an agent with its own identity. All robots share the same long-term memory database (hive mind), but each has its own working memory.</p>"},{"location":"guides/getting-started/#adding-your-first-memory","title":"Adding Your First Memory","text":"<pre><code># Add a fact about the user\nhtm.add_node(\n  \"user_name\",                    # Unique key\n  \"The user's name is Alice\",     # Content\n  type: :fact,                    # Memory type\n  importance: 8.0,                # Importance score (0-10)\n  tags: [\"user\", \"identity\"]      # Tags for categorization\n)\n\nputs \"Memory added successfully!\"\n</code></pre>"},{"location":"guides/getting-started/#understanding-memory-types","title":"Understanding Memory Types","text":"<p>HTM supports six memory types, each optimized for different purposes:</p> <pre><code># Facts: Immutable truths\nhtm.add_node(\n  \"fact_001\",\n  \"The user lives in San Francisco\",\n  type: :fact,\n  importance: 7.0\n)\n\n# Context: Conversation state\nhtm.add_node(\n  \"context_001\",\n  \"Currently discussing database architecture for a new project\",\n  type: :context,\n  importance: 6.0\n)\n\n# Preferences: User preferences\nhtm.add_node(\n  \"pref_001\",\n  \"User prefers Ruby over Python for scripting\",\n  type: :preference,\n  importance: 5.0\n)\n\n# Decisions: Design decisions\nhtm.add_node(\n  \"decision_001\",\n  \"Decided to use PostgreSQL instead of MongoDB for better consistency\",\n  type: :decision,\n  importance: 9.0,\n  tags: [\"architecture\", \"database\"]\n)\n\n# Code: Code snippets\nhtm.add_node(\n  \"code_001\",\n  \"def greet(name)\\n  puts \\\"Hello, \\#{name}!\\\"\\nend\",\n  type: :code,\n  importance: 4.0,\n  tags: [\"ruby\", \"functions\"]\n)\n\n# Questions: Unresolved questions\nhtm.add_node(\n  \"question_001\",\n  \"Should we add Redis caching to improve performance?\",\n  type: :question,\n  importance: 6.0,\n  tags: [\"performance\", \"caching\"]\n)\n</code></pre> <p>Choosing the Right Type</p> <ul> <li>Use <code>:fact</code> for unchanging information</li> <li>Use <code>:context</code> for temporary conversation state</li> <li>Use <code>:preference</code> for user settings</li> <li>Use <code>:decision</code> for important architectural choices</li> <li>Use <code>:code</code> for code examples and snippets</li> <li>Use <code>:question</code> for tracking open questions</li> </ul>"},{"location":"guides/getting-started/#retrieving-memories","title":"Retrieving Memories","text":"<p>Retrieve a specific memory by its key:</p> <pre><code>memory = htm.retrieve(\"user_name\")\n\nif memory\n  puts \"Found: #{memory['value']}\"\n  puts \"Type: #{memory['type']}\"\n  puts \"Created: #{memory['created_at']}\"\n  puts \"Importance: #{memory['importance']}\"\nend\n</code></pre>"},{"location":"guides/getting-started/#recalling-from-the-past","title":"Recalling from the Past","text":"<p>Use HTM's RAG capabilities to recall relevant memories:</p> <pre><code># Recall memories about databases from the last week\nmemories = htm.recall(\n  timeframe: \"last week\",\n  topic: \"database architecture\",\n  limit: 10\n)\n\nmemories.each do |memory|\n  puts \"- #{memory['value']}\"\n  puts \"  Similarity: #{memory['similarity']}\"\n  puts\nend\n</code></pre> <p>How Recall Works</p> <p>HTM uses vector embeddings to understand the semantic meaning of your query. It finds memories that are conceptually related, not just keyword matches.</p>"},{"location":"guides/getting-started/#creating-context-for-llms","title":"Creating Context for LLMs","text":"<p>Assemble working memory into context for your LLM:</p> <pre><code># Get a balanced mix of important and recent memories\ncontext = htm.create_context(\n  strategy: :balanced,\n  max_tokens: 50_000\n)\n\n# Use this context in your LLM prompt\nprompt = &lt;&lt;~PROMPT\n  Context from memory:\n  #{context}\n\n  User: What database did we decide to use?\n\n  Assistant:\nPROMPT\n\n# Send to your LLM...\n</code></pre>"},{"location":"guides/getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/getting-started/#pattern-1-session-memory","title":"Pattern 1: Session Memory","text":"<p>Store conversation turns as memories:</p> <pre><code>class ConversationTracker\n  def initialize(session_id)\n    @htm = HTM.new(robot_name: \"Chat-#{session_id}\")\n    @turn = 0\n  end\n\n  def add_turn(user_message, assistant_response)\n    @turn += 1\n\n    # Store user message\n    @htm.add_node(\n      \"turn_#{@turn}_user\",\n      user_message,\n      type: :context,\n      importance: 5.0,\n      tags: [\"conversation\", \"user\"]\n    )\n\n    # Store assistant response\n    @htm.add_node(\n      \"turn_#{@turn}_assistant\",\n      assistant_response,\n      type: :context,\n      importance: 5.0,\n      tags: [\"conversation\", \"assistant\"],\n      related_to: [\"turn_#{@turn}_user\"]\n    )\n  end\n\n  def recall_context\n    @htm.create_context(strategy: :recent, max_tokens: 10_000)\n  end\nend\n</code></pre>"},{"location":"guides/getting-started/#pattern-2-knowledge-base","title":"Pattern 2: Knowledge Base","text":"<p>Build a queryable knowledge base:</p> <pre><code>class KnowledgeBase\n  def initialize\n    @htm = HTM.new(robot_name: \"Knowledge Bot\")\n  end\n\n  def add_fact(key, fact, category:, tags: [])\n    @htm.add_node(\n      key,\n      fact,\n      type: :fact,\n      category: category,\n      importance: 8.0,\n      tags: tags\n    )\n  end\n\n  def query(question)\n    # Search all time for relevant facts\n    @htm.recall(\n      timeframe: \"last 10 years\",  # Effectively all memories\n      topic: question,\n      limit: 5\n    )\n  end\nend\n\n# Usage\nkb = KnowledgeBase.new\nkb.add_fact(\n  \"ruby_version\",\n  \"Ruby 3.0 introduced Ractors for parallel execution\",\n  category: \"programming\",\n  tags: [\"ruby\", \"concurrency\"]\n)\n\nresults = kb.query(\"How does Ruby handle parallelism?\")\n</code></pre>"},{"location":"guides/getting-started/#pattern-3-decision-journal","title":"Pattern 3: Decision Journal","text":"<p>Track architectural decisions over time:</p> <pre><code>class DecisionJournal\n  def initialize(project_name)\n    @htm = HTM.new(robot_name: \"Architect-#{project_name}\")\n  end\n\n  def record_decision(title, rationale, alternatives: [], tags: [])\n    key = \"decision_#{Time.now.to_i}\"\n\n    decision = &lt;&lt;~DECISION\n      Decision: #{title}\n\n      Rationale: #{rationale}\n\n      #{alternatives.any? ? \"Alternatives considered: #{alternatives.join(', ')}\" : ''}\n    DECISION\n\n    @htm.add_node(\n      key,\n      decision,\n      type: :decision,\n      importance: 9.0,\n      tags: tags + [\"decision\"]\n    )\n  end\n\n  def get_decision_history(topic)\n    @htm.recall(\n      timeframe: \"last year\",\n      topic: topic,\n      limit: 20\n    ).sort_by { |d| d['created_at'] }\n  end\nend\n</code></pre>"},{"location":"guides/getting-started/#monitoring-memory-usage","title":"Monitoring Memory Usage","text":"<p>Check working memory utilization:</p> <pre><code># Get current statistics\nwm = htm.working_memory\nputs \"Nodes in working memory: #{wm.node_count}\"\nputs \"Tokens used: #{wm.token_count} / #{wm.max_tokens}\"\nputs \"Utilization: #{wm.utilization_percentage}%\"\n\n# Get comprehensive stats\nstats = htm.memory_stats\nputs \"Total nodes in long-term: #{stats[:total_nodes]}\"\nputs \"Active robots: #{stats[:active_robots]}\"\nputs \"Database size: #{stats[:database_size] / (1024.0 * 1024.0)} MB\"\n</code></pre> <p>Tip</p> <p>Monitor working memory utilization regularly. If you consistently hit 100%, consider increasing <code>working_memory_size</code> or implementing more aggressive eviction strategies.</p>"},{"location":"guides/getting-started/#best-practices-for-beginners","title":"Best Practices for Beginners","text":""},{"location":"guides/getting-started/#1-use-meaningful-keys","title":"1. Use Meaningful Keys","text":"<pre><code># Good: Descriptive, unique keys\nhtm.add_node(\"user_pref_theme_dark\", \"User prefers dark theme\", ...)\n\n# Bad: Generic keys that might conflict\nhtm.add_node(\"pref\", \"User prefers dark theme\", ...)\n</code></pre>"},{"location":"guides/getting-started/#2-set-appropriate-importance","title":"2. Set Appropriate Importance","text":"<pre><code># Critical facts: 9-10\nhtm.add_node(\"api_key\", \"API key is: ...\", importance: 10.0)\n\n# Important decisions: 7-9\nhtm.add_node(\"arch_001\", \"Using microservices\", importance: 8.0)\n\n# Contextual information: 4-6\nhtm.add_node(\"ctx_001\", \"Discussing weather\", importance: 5.0)\n\n# Temporary notes: 1-3\nhtm.add_node(\"note_001\", \"Remember to check logs\", importance: 2.0)\n</code></pre>"},{"location":"guides/getting-started/#3-use-tags-liberally","title":"3. Use Tags Liberally","text":"<pre><code>htm.add_node(\n  \"decision_001\",\n  \"Chose PostgreSQL for data persistence\",\n  type: :decision,\n  importance: 9.0,\n  tags: [\n    \"database\",\n    \"architecture\",\n    \"backend\",\n    \"persistence\",\n    \"postgresql\"\n  ]\n)\n</code></pre>"},{"location":"guides/getting-started/#4-leverage-relationships","title":"4. Leverage Relationships","text":"<pre><code># Add related memories\nhtm.add_node(\"decision_db\", \"Use PostgreSQL\", type: :decision)\n\nhtm.add_node(\n  \"code_db_connect\",\n  \"Connection code for PostgreSQL\",\n  type: :code,\n  related_to: [\"decision_db\"]  # Link to the decision\n)\n</code></pre>"},{"location":"guides/getting-started/#5-clean-up-when-needed","title":"5. Clean Up When Needed","text":"<pre><code># Explicitly forget outdated information\nhtm.forget(\"old_api_key\", confirm: :confirmed)\n</code></pre> <p>Warning</p> <p>The <code>forget</code> method requires explicit confirmation to prevent accidental data loss. HTM never deletes memories automatically.</p>"},{"location":"guides/getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/getting-started/#ollama-connection-issues","title":"Ollama Connection Issues","text":"<p>If you see embedding errors:</p> <pre><code># Check Ollama is running\ncurl http://localhost:11434/api/version\n\n# If not running, start it\nollama serve\n\n# Verify the model is available\nollama list\n</code></pre>"},{"location":"guides/getting-started/#database-connection-issues","title":"Database Connection Issues","text":"<pre><code># Test your connection\nrequire 'htm'\n\nbegin\n  HTM::Database.setup\n  puts \"Connection successful!\"\nrescue =&gt; e\n  puts \"Connection failed: #{e.message}\"\nend\n</code></pre>"},{"location":"guides/getting-started/#memory-not-found","title":"Memory Not Found","text":"<pre><code>memory = htm.retrieve(\"my_key\")\n\nif memory.nil?\n  puts \"Memory not found. Check the key spelling.\"\nelse\n  puts \"Found: #{memory['value']}\"\nend\n</code></pre>"},{"location":"guides/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics, explore these guides:</p> <ul> <li>Adding Memories - Deep dive into memory types and metadata</li> <li>Recalling Memories - Master search strategies and retrieval</li> <li>Context Assembly - Optimize context for your LLM</li> <li>Working Memory - Understand token limits and eviction</li> </ul>"},{"location":"guides/getting-started/#complete-example","title":"Complete Example","text":"<p>Here's a complete working example combining everything:</p> <pre><code>require 'htm'\n\n# Initialize\nhtm = HTM.new(\n  robot_name: \"Demo Bot\",\n  working_memory_size: 128_000\n)\n\n# Add various memories\nhtm.add_node(\n  \"user_001\",\n  \"User's name is Alice and she's a software engineer\",\n  type: :fact,\n  importance: 8.0,\n  tags: [\"user\", \"identity\", \"profession\"]\n)\n\nhtm.add_node(\n  \"decision_001\",\n  \"Decided to use HTM for managing conversation memory\",\n  type: :decision,\n  importance: 9.0,\n  tags: [\"architecture\", \"memory\"]\n)\n\nhtm.add_node(\n  \"pref_001\",\n  \"Alice prefers detailed explanations with examples\",\n  type: :preference,\n  importance: 7.0,\n  tags: [\"user\", \"communication\"],\n  related_to: [\"user_001\"]\n)\n\n# Recall relevant memories\nmemories = htm.recall(\n  timeframe: \"last 7 days\",\n  topic: \"user preferences\",\n  limit: 5\n)\n\nputs \"Found #{memories.length} relevant memories:\"\nmemories.each do |m|\n  puts \"- #{m['value']} (importance: #{m['importance']})\"\nend\n\n# Create context for LLM\ncontext = htm.create_context(strategy: :balanced)\nputs \"\\nContext length: #{context.length} characters\"\n\n# Check stats\nstats = htm.memory_stats\nputs \"\\nMemory statistics:\"\nputs \"- Total nodes: #{stats[:total_nodes]}\"\nputs \"- Working memory: #{stats[:working_memory][:utilization]}% full\"\nputs \"- Database size: #{(stats[:database_size] / 1024.0 / 1024.0).round(2)} MB\"\n</code></pre> <p>Happy coding with HTM!</p>"},{"location":"guides/long-term-memory/","title":"Long-term Memory","text":"<p>Long-term memory is HTM's durable PostgreSQL storage layer. This guide covers database operations, maintenance, performance optimization, and advanced queries.</p>"},{"location":"guides/long-term-memory/#architecture-overview","title":"Architecture Overview","text":"<p>Long-term memory provides:</p> <ul> <li>Permanent storage for all memories</li> <li>Vector embeddings via pgvector</li> <li>Full-text search via PostgreSQL's ts_vector</li> <li>Time-series optimization via TimescaleDB hypertables</li> <li>Relationship graphs for knowledge connections</li> <li>Audit logging for all operations</li> </ul> <p></p>"},{"location":"guides/long-term-memory/#database-schema","title":"Database Schema","text":""},{"location":"guides/long-term-memory/#nodes-table","title":"Nodes Table","text":"<p>The primary storage for memories:</p> <pre><code>CREATE TABLE nodes (\n  id BIGSERIAL PRIMARY KEY,\n  key TEXT NOT NULL UNIQUE,\n  value TEXT NOT NULL,\n  type TEXT,\n  category TEXT,\n  importance FLOAT DEFAULT 1.0,\n  token_count INTEGER DEFAULT 0,\n  robot_id TEXT NOT NULL,\n  embedding vector(1536),  -- pgvector type\n  created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,\n  last_accessed TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,\n  in_working_memory BOOLEAN DEFAULT TRUE\n);\n\n-- Indexes\nCREATE INDEX idx_nodes_robot_id ON nodes(robot_id);\nCREATE INDEX idx_nodes_type ON nodes(type);\nCREATE INDEX idx_nodes_created_at ON nodes(created_at);\nCREATE INDEX idx_nodes_embedding ON nodes USING hnsw(embedding vector_cosine_ops);\nCREATE INDEX idx_nodes_fulltext ON nodes USING gin(to_tsvector('english', value));\n</code></pre>"},{"location":"guides/long-term-memory/#relationships-table","title":"Relationships Table","text":"<p>Tracks connections between nodes:</p> <pre><code>CREATE TABLE relationships (\n  id BIGSERIAL PRIMARY KEY,\n  from_node_id BIGINT REFERENCES nodes(id) ON DELETE CASCADE,\n  to_node_id BIGINT REFERENCES nodes(id) ON DELETE CASCADE,\n  relationship_type TEXT,\n  strength FLOAT DEFAULT 1.0,\n  created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,\n  UNIQUE(from_node_id, to_node_id, relationship_type)\n);\n</code></pre>"},{"location":"guides/long-term-memory/#tags-table","title":"Tags Table","text":"<p>Flexible categorization:</p> <pre><code>CREATE TABLE tags (\n  id BIGSERIAL PRIMARY KEY,\n  node_id BIGINT REFERENCES nodes(id) ON DELETE CASCADE,\n  tag TEXT NOT NULL,\n  created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,\n  UNIQUE(node_id, tag)\n);\n\nCREATE INDEX idx_tags_tag ON tags(tag);\n</code></pre>"},{"location":"guides/long-term-memory/#operations-log-table-hypertable","title":"Operations Log Table (Hypertable)","text":"<p>Audit trail with time-series optimization:</p> <pre><code>CREATE TABLE operations_log (\n  time TIMESTAMPTZ NOT NULL,\n  operation TEXT NOT NULL,\n  node_id BIGINT,\n  robot_id TEXT NOT NULL,\n  details JSONB,\n  PRIMARY KEY (time, operation, robot_id)\n);\n\n-- Convert to hypertable\nSELECT create_hypertable('operations_log', 'time');\n</code></pre>"},{"location":"guides/long-term-memory/#robots-table","title":"Robots Table","text":"<p>Robot registry:</p> <pre><code>CREATE TABLE robots (\n  id TEXT PRIMARY KEY,\n  name TEXT NOT NULL,\n  created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,\n  last_active TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre>"},{"location":"guides/long-term-memory/#database-operations","title":"Database Operations","text":""},{"location":"guides/long-term-memory/#direct-database-queries","title":"Direct Database Queries","text":"<p>While HTM provides a high-level API, you can query the database directly:</p> <pre><code>require 'pg'\n\n# Get connection config\nconfig = HTM::Database.default_config\n\n# Execute raw queries\nconn = PG.connect(config)\n\n# Query nodes\nresult = conn.exec(\"SELECT * FROM nodes WHERE type = 'decision' LIMIT 10\")\nresult.each do |row|\n  puts \"#{row['key']}: #{row['value']}\"\nend\n\n# Query with parameters\nresult = conn.exec_params(\n  \"SELECT * FROM nodes WHERE robot_id = $1 AND importance &gt;= $2\",\n  [\"your-robot-id\", 8.0]\n)\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#using-longtermmemory-directly","title":"Using LongTermMemory Directly","text":"<p>Access the long-term memory layer:</p> <pre><code>ltm = HTM::LongTermMemory.new(HTM::Database.default_config)\n\n# Add a node\nnode_id = ltm.add(\n  key: \"test_001\",\n  value: \"Test memory\",\n  type: :fact,\n  importance: 7.0,\n  token_count: 10,\n  robot_id: \"test-robot\",\n  embedding: Array.new(1536) { rand }\n)\n\n# Retrieve a node\nnode = ltm.retrieve(\"test_001\")\n\n# Update last accessed\nltm.update_last_accessed(\"test_001\")\n\n# Delete a node\nltm.delete(\"test_001\")\n</code></pre>"},{"location":"guides/long-term-memory/#memory-statistics","title":"Memory Statistics","text":"<p>Get comprehensive statistics:</p> <pre><code>stats = htm.memory_stats\n\n# Total nodes\nputs \"Total nodes: #{stats[:total_nodes]}\"\n\n# Nodes by robot\nstats[:nodes_by_robot].each do |robot_id, count|\n  puts \"#{robot_id}: #{count} nodes\"\nend\n\n# Nodes by type\nstats[:nodes_by_type].each do |row|\n  puts \"Type #{row['type']}: #{row['count']} nodes\"\nend\n\n# Relationships\nputs \"Total relationships: #{stats[:total_relationships]}\"\n\n# Tags\nputs \"Total tags: #{stats[:total_tags]}\"\n\n# Time range\nputs \"Oldest memory: #{stats[:oldest_memory]}\"\nputs \"Newest memory: #{stats[:newest_memory]}\"\n\n# Database size\nsize_mb = stats[:database_size] / (1024.0 * 1024.0)\nputs \"Database size: #{size_mb.round(2)} MB\"\n\n# Active robots\nputs \"Active robots: #{stats[:active_robots]}\"\nstats[:robot_activity].each do |robot|\n  puts \"  #{robot['name']}: last active #{robot['last_active']}\"\nend\n</code></pre>"},{"location":"guides/long-term-memory/#advanced-queries","title":"Advanced Queries","text":""},{"location":"guides/long-term-memory/#query-by-date-range","title":"Query by Date Range","text":"<pre><code># Get all memories from a specific month\nstart_date = Time.new(2024, 1, 1)\nend_date = Time.new(2024, 1, 31, 23, 59, 59)\n\nconfig = HTM::Database.default_config\nconn = PG.connect(config)\n\nresult = conn.exec_params(\n  &lt;&lt;~SQL,\n    SELECT key, value, type, importance, created_at\n    FROM nodes\n    WHERE created_at BETWEEN $1 AND $2\n    ORDER BY created_at DESC\n  SQL\n  [start_date, end_date]\n)\n\nresult.each do |row|\n  puts \"#{row['created_at']}: #{row['value'][0..50]}...\"\nend\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#query-by-type-and-importance","title":"Query by Type and Importance","text":"<pre><code># Find critical decisions\nconn = PG.connect(HTM::Database.default_config)\n\nresult = conn.exec_params(\n  &lt;&lt;~SQL,\n    SELECT key, value, importance, created_at\n    FROM nodes\n    WHERE type = $1 AND importance &gt;= $2\n    ORDER BY importance DESC, created_at DESC\n  SQL\n  ['decision', 8.0]\n)\n\nputs \"Critical decisions:\"\nresult.each do |row|\n  puts \"- [#{row['importance']}] #{row['value'][0..100]}...\"\nend\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#query-relationships","title":"Query Relationships","text":"<pre><code># Find all nodes related to a specific node\nconn = PG.connect(HTM::Database.default_config)\n\nresult = conn.exec_params(\n  &lt;&lt;~SQL,\n    SELECT n.key, n.value, n.type, r.relationship_type\n    FROM nodes n\n    JOIN relationships r ON n.id = r.to_node_id\n    JOIN nodes source ON r.from_node_id = source.id\n    WHERE source.key = $1\n  SQL\n  ['decision_001']\n)\n\nputs \"Related nodes:\"\nresult.each do |row|\n  puts \"- [#{row['type']}] #{row['value'][0..50]}... (#{row['relationship_type']})\"\nend\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#query-by-tags","title":"Query by Tags","text":"<pre><code># Find all nodes with specific tag\nconn = PG.connect(HTM::Database.default_config)\n\nresult = conn.exec_params(\n  &lt;&lt;~SQL,\n    SELECT DISTINCT n.key, n.value, n.type, n.importance\n    FROM nodes n\n    JOIN tags t ON n.id = t.node_id\n    WHERE t.tag = $1\n    ORDER BY n.importance DESC\n  SQL\n  ['architecture']\n)\n\nputs \"Architecture-related memories:\"\nresult.each do |row|\n  puts \"- [#{row['importance']}] #{row['value'][0..80]}...\"\nend\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#most-active-robots","title":"Most Active Robots","text":"<pre><code># Find robots with most contributions\nconn = PG.connect(HTM::Database.default_config)\n\nresult = conn.exec(\n  &lt;&lt;~SQL\n    SELECT r.name, r.id, COUNT(n.id) as memory_count\n    FROM robots r\n    LEFT JOIN nodes n ON r.id = n.robot_id\n    GROUP BY r.id, r.name\n    ORDER BY memory_count DESC\n  SQL\n)\n\nputs \"Robot contributions:\"\nresult.each do |row|\n  puts \"#{row['name']}: #{row['memory_count']} memories\"\nend\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#time-based-activity","title":"Time-Based Activity","text":"<pre><code># Get activity by day\nconn = PG.connect(HTM::Database.default_config)\n\nresult = conn.exec(\n  &lt;&lt;~SQL\n    SELECT DATE(created_at) as date, COUNT(*) as count\n    FROM nodes\n    WHERE created_at &gt;= CURRENT_DATE - INTERVAL '30 days'\n    GROUP BY DATE(created_at)\n    ORDER BY date DESC\n  SQL\n)\n\nputs \"Activity last 30 days:\"\nresult.each do |row|\n  puts \"#{row['date']}: #{row['count']} memories\"\nend\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#database-maintenance","title":"Database Maintenance","text":""},{"location":"guides/long-term-memory/#vacuuming","title":"Vacuuming","text":"<p>PostgreSQL requires periodic vacuuming:</p> <pre><code># Manual vacuum\nconn = PG.connect(HTM::Database.default_config)\nconn.exec(\"VACUUM ANALYZE nodes\")\nconn.exec(\"VACUUM ANALYZE relationships\")\nconn.exec(\"VACUUM ANALYZE tags\")\nconn.close\n\nputs \"Vacuum completed\"\n</code></pre>"},{"location":"guides/long-term-memory/#reindexing","title":"Reindexing","text":"<p>Rebuild indexes for optimal performance:</p> <pre><code>conn = PG.connect(HTM::Database.default_config)\n\n# Reindex vector index\nconn.exec(\"REINDEX INDEX idx_nodes_embedding\")\n\n# Reindex full-text\nconn.exec(\"REINDEX INDEX idx_nodes_fulltext\")\n\nconn.close\n\nputs \"Reindexing completed\"\n</code></pre>"},{"location":"guides/long-term-memory/#compression-timescaledb","title":"Compression (TimescaleDB)","text":"<p>TimescaleDB can compress old data:</p> <pre><code># Enable compression on operations_log hypertable\nconn = PG.connect(HTM::Database.default_config)\n\nconn.exec(\n  &lt;&lt;~SQL\n    ALTER TABLE operations_log SET (\n      timescaledb.compress,\n      timescaledb.compress_segmentby = 'robot_id'\n    )\n  SQL\n)\n\n# Add compression policy (compress data older than 7 days)\nconn.exec(\n  &lt;&lt;~SQL\n    SELECT add_compression_policy('operations_log', INTERVAL '7 days')\n  SQL\n)\n\nconn.close\n\nputs \"Compression policy enabled\"\n</code></pre>"},{"location":"guides/long-term-memory/#cleanup-old-logs","title":"Cleanup Old Logs","text":"<pre><code># Delete operations logs older than 90 days\nconn = PG.connect(HTM::Database.default_config)\n\nresult = conn.exec_params(\n  \"DELETE FROM operations_log WHERE time &lt; $1\",\n  [Time.now - (90 * 24 * 3600)]\n)\n\nputs \"Deleted #{result.cmd_tuples} old log entries\"\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/long-term-memory/#analyzing-query-performance","title":"Analyzing Query Performance","text":"<pre><code># Explain query plan\nconn = PG.connect(HTM::Database.default_config)\n\nquery = &lt;&lt;~SQL\n  SELECT * FROM nodes\n  WHERE type = 'decision'\n  AND importance &gt;= 8.0\n  ORDER BY created_at DESC\n  LIMIT 10\nSQL\n\n# Get query plan\nresult = conn.exec(\"EXPLAIN ANALYZE #{query}\")\nputs result.values.flatten\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#index-usage-statistics","title":"Index Usage Statistics","text":"<pre><code># Check index usage\nconn = PG.connect(HTM::Database.default_config)\n\nresult = conn.exec(\n  &lt;&lt;~SQL\n    SELECT\n      schemaname,\n      tablename,\n      indexname,\n      idx_scan as scans,\n      idx_tup_read as tuples_read,\n      idx_tup_fetch as tuples_fetched\n    FROM pg_stat_user_indexes\n    WHERE schemaname = 'public'\n    ORDER BY idx_scan DESC\n  SQL\n)\n\nputs \"Index usage statistics:\"\nresult.each do |row|\n  puts \"#{row['indexname']}: #{row['scans']} scans, #{row['tuples_read']} tuples\"\nend\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#table-size-analysis","title":"Table Size Analysis","text":"<pre><code># Check table sizes\nconn = PG.connect(HTM::Database.default_config)\n\nresult = conn.exec(\n  &lt;&lt;~SQL\n    SELECT\n      tablename,\n      pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size\n    FROM pg_tables\n    WHERE schemaname = 'public'\n    ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\n  SQL\n)\n\nputs \"Table sizes:\"\nresult.each do |row|\n  puts \"#{row['tablename']}: #{row['size']}\"\nend\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#optimizing-vector-searches","title":"Optimizing Vector Searches","text":"<pre><code># HNSW index parameters can be tuned\n# (This is done during index creation, shown for reference)\n\n# m: max connections per layer (default: 16)\n# ef_construction: construction time/accuracy tradeoff (default: 64)\n\n# Example (run during schema setup):\n# CREATE INDEX idx_nodes_embedding ON nodes\n#   USING hnsw(embedding vector_cosine_ops)\n#   WITH (m = 16, ef_construction = 64);\n\n# For queries, you can adjust ef_search:\nconn = PG.connect(HTM::Database.default_config)\n\n# Higher ef_search = more accurate but slower\nconn.exec(\"SET hnsw.ef_search = 100\")\n\n# Now run vector searches...\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#backup-and-restore","title":"Backup and Restore","text":""},{"location":"guides/long-term-memory/#backup-database","title":"Backup Database","text":"<pre><code># Full database backup\npg_dump -h localhost -U user -d database -F c -f htm_backup.dump\n\n# Backup just the schema\npg_dump -h localhost -U user -d database -s -f htm_schema.sql\n\n# Backup just the data\npg_dump -h localhost -U user -d database -a -f htm_data.sql\n</code></pre>"},{"location":"guides/long-term-memory/#restore-database","title":"Restore Database","text":"<pre><code># Restore from custom format\npg_restore -h localhost -U user -d database htm_backup.dump\n\n# Restore from SQL format\npsql -h localhost -U user -d database -f htm_schema.sql\npsql -h localhost -U user -d database -f htm_data.sql\n</code></pre>"},{"location":"guides/long-term-memory/#backup-ruby-script","title":"Backup Ruby Script","text":"<pre><code>require 'open3'\n\ndef backup_database\n  config = HTM::Database.default_config\n  uri = URI.parse(config[:host])\n\n  timestamp = Time.now.strftime(\"%Y%m%d_%H%M%S\")\n  backup_file = \"htm_backup_#{timestamp}.dump\"\n\n  cmd = [\n    \"pg_dump\",\n    \"-h\", uri.host,\n    \"-p\", uri.port.to_s,\n    \"-U\", config[:user],\n    \"-d\", config[:dbname],\n    \"-F\", \"c\",  # Custom format\n    \"-f\", backup_file\n  ].join(\" \")\n\n  # Set password via environment\n  env = { \"PGPASSWORD\" =&gt; config[:password] }\n\n  stdout, stderr, status = Open3.capture3(env, cmd)\n\n  if status.success?\n    puts \"Backup created: #{backup_file}\"\n    backup_file\n  else\n    raise \"Backup failed: #{stderr}\"\n  end\nend\n\n# Usage\nbackup_database\n</code></pre>"},{"location":"guides/long-term-memory/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"guides/long-term-memory/#connection-pooling","title":"Connection Pooling","text":"<p>HTM uses connection pooling internally, but you can monitor it:</p> <pre><code># Check active connections\nconn = PG.connect(HTM::Database.default_config)\n\nresult = conn.exec(\n  &lt;&lt;~SQL\n    SELECT\n      count(*) as total,\n      count(*) FILTER (WHERE state = 'active') as active,\n      count(*) FILTER (WHERE state = 'idle') as idle\n    FROM pg_stat_activity\n    WHERE datname = current_database()\n  SQL\n)\n\nputs \"Connections: #{result.first['total']}\"\nputs \"  Active: #{result.first['active']}\"\nputs \"  Idle: #{result.first['idle']}\"\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#slow-query-log","title":"Slow Query Log","text":"<p>Enable slow query logging in PostgreSQL:</p> <pre><code>-- In postgresql.conf or via SQL\nALTER DATABASE your_database SET log_min_duration_statement = 1000;  -- Log queries &gt; 1s\n</code></pre>"},{"location":"guides/long-term-memory/#custom-monitoring","title":"Custom Monitoring","text":"<pre><code>class DatabaseMonitor\n  def initialize(htm)\n    @htm = htm\n    @config = HTM::Database.default_config\n  end\n\n  def health_check\n    conn = PG.connect(@config)\n\n    # Check connectivity\n    result = conn.exec(\"SELECT 1\")\n\n    # Check table accessibility\n    conn.exec(\"SELECT COUNT(*) FROM nodes\")\n    conn.exec(\"SELECT COUNT(*) FROM relationships\")\n\n    conn.close\n\n    { status: :healthy, message: \"All checks passed\" }\n  rescue =&gt; e\n    { status: :error, message: e.message }\n  end\n\n  def performance_report\n    conn = PG.connect(@config)\n\n    report = {}\n\n    # Query counts\n    result = conn.exec(\"SELECT COUNT(*) FROM nodes\")\n    report[:total_nodes] = result.first['count'].to_i\n\n    # Table sizes\n    result = conn.exec(\n      &lt;&lt;~SQL\n        SELECT pg_size_pretty(pg_total_relation_size('nodes')) as size\n      SQL\n    )\n    report[:nodes_size] = result.first['size']\n\n    # Cache hit ratio\n    result = conn.exec(\n      &lt;&lt;~SQL\n        SELECT\n          sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratio\n        FROM pg_statio_user_tables\n        WHERE schemaname = 'public'\n      SQL\n    )\n    report[:cache_hit_ratio] = result.first['ratio'].to_f\n\n    conn.close\n    report\n  end\n\n  def alert_if_unhealthy\n    health = health_check\n\n    if health[:status] != :healthy\n      # Send alert (email, Slack, etc.)\n      warn \"Database unhealthy: #{health[:message]}\"\n    end\n  end\nend\n\nmonitor = DatabaseMonitor.new(htm)\nputs monitor.health_check\nputs monitor.performance_report\n</code></pre>"},{"location":"guides/long-term-memory/#best-practices","title":"Best Practices","text":""},{"location":"guides/long-term-memory/#1-use-prepared-statements","title":"1. Use Prepared Statements","text":"<pre><code># Good: Use parameterized queries\nconn.exec_params(\n  \"SELECT * FROM nodes WHERE robot_id = $1 AND type = $2\",\n  [robot_id, type]\n)\n\n# Avoid: String interpolation (SQL injection risk)\n# conn.exec(\"SELECT * FROM nodes WHERE robot_id = '#{robot_id}'\")\n</code></pre>"},{"location":"guides/long-term-memory/#2-connection-management","title":"2. Connection Management","text":"<pre><code># Good: Use HTM's internal connection handling\nhtm.add_node(...)  # Manages connections automatically\n\n# Advanced: Manual connections, always close\nconn = PG.connect(config)\nbegin\n  # Do work\nensure\n  conn.close\nend\n</code></pre>"},{"location":"guides/long-term-memory/#3-batch-operations","title":"3. Batch Operations","text":"<pre><code># Good: Use transactions for multiple operations\nconn = PG.connect(config)\nconn.transaction do |c|\n  100.times do |i|\n    c.exec_params(\"INSERT INTO nodes (...) VALUES ($1, $2)\", [key, value])\n  end\nend\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#4-regular-maintenance","title":"4. Regular Maintenance","text":"<pre><code># Schedule regular maintenance\nrequire 'whenever'  # gem for cron jobs\n\n# In schedule.rb\nevery 1.day, at: '2:00 am' do\n  runner \"HTM::Database.vacuum_analyze\"\nend\n\nevery 1.week, at: '3:00 am' do\n  runner \"HTM::Database.reindex\"\nend\n</code></pre>"},{"location":"guides/long-term-memory/#5-monitor-growth","title":"5. Monitor Growth","text":"<pre><code># Track database growth over time\nclass GrowthTracker\n  def initialize\n    @log_file = \"database_growth.log\"\n  end\n\n  def log_stats\n    stats = htm.memory_stats\n\n    entry = {\n      timestamp: Time.now,\n      total_nodes: stats[:total_nodes],\n      database_size: stats[:database_size]\n    }\n\n    File.open(@log_file, 'a') do |f|\n      f.puts entry.to_json\n    end\n  end\nend\n\n# Run daily\ntracker = GrowthTracker.new\ntracker.log_stats\n</code></pre>"},{"location":"guides/long-term-memory/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/long-term-memory/#connection-issues","title":"Connection Issues","text":"<pre><code># Test connection\nbegin\n  conn = PG.connect(HTM::Database.default_config)\n  puts \"Connection successful\"\n  conn.close\nrescue PG::Error =&gt; e\n  puts \"Connection failed: #{e.message}\"\n  puts \"Check HTM_DBURL environment variable\"\nend\n</code></pre>"},{"location":"guides/long-term-memory/#slow-queries","title":"Slow Queries","text":"<pre><code># Enable query timing\nconn = PG.connect(HTM::Database.default_config)\n\nstart = Time.now\nresult = conn.exec(\"SELECT * FROM nodes WHERE type = 'decision'\")\nelapsed = Time.now - start\n\nputs \"Query returned #{result.ntuples} rows in #{elapsed}s\"\n\nif elapsed &gt; 1.0\n  puts \"Slow query detected! Consider:\"\n  puts \"- Adding indexes\"\n  puts \"- Using LIMIT\"\n  puts \"- Narrowing date range\"\nend\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#disk-space-issues","title":"Disk Space Issues","text":"<pre><code># Check disk usage\nconn = PG.connect(HTM::Database.default_config)\n\nresult = conn.exec(\"SELECT pg_database_size(current_database()) as size\")\nsize_gb = result.first['size'].to_i / (1024.0 ** 3)\n\nputs \"Database size: #{size_gb.round(2)} GB\"\n\nif size_gb &gt; 10\n  puts \"Large database. Consider:\"\n  puts \"- Archiving old nodes\"\n  puts \"- Enabling compression\"\n  puts \"- Cleaning up operations_log\"\nend\n\nconn.close\n</code></pre>"},{"location":"guides/long-term-memory/#next-steps","title":"Next Steps","text":"<ul> <li>Working Memory - Understand the memory tier above long-term</li> <li>Adding Memories - Learn how memories are stored</li> <li>Search Strategies - Optimize retrieval from long-term memory</li> </ul>"},{"location":"guides/long-term-memory/#complete-example","title":"Complete Example","text":"<pre><code>require 'htm'\nrequire 'pg'\n\n# Initialize HTM\nhtm = HTM.new(robot_name: \"Database Admin\")\n\n# Add some test data\nputs \"Adding test data...\"\n10.times do |i|\n  htm.add_node(\n    \"test_#{i}\",\n    \"Test memory number #{i}\",\n    type: :fact,\n    importance: rand(1.0..10.0),\n    tags: [\"test\", \"batch_#{i / 5}\"]\n  )\nend\n\n# Get statistics\nputs \"\\n=== Database Statistics ===\"\nstats = htm.memory_stats\nputs \"Total nodes: #{stats[:total_nodes]}\"\nputs \"Database size: #{(stats[:database_size] / 1024.0 / 1024.0).round(2)} MB\"\nputs \"Active robots: #{stats[:active_robots]}\"\n\n# Query by tag\nputs \"\\n=== Query by Tag ===\"\nconfig = HTM::Database.default_config\nconn = PG.connect(config)\n\nresult = conn.exec_params(\n  &lt;&lt;~SQL,\n    SELECT n.key, n.value\n    FROM nodes n\n    JOIN tags t ON n.id = t.node_id\n    WHERE t.tag = $1\n  SQL\n  ['test']\n)\n\nputs \"Found #{result.ntuples} nodes with tag 'test'\"\nresult.each do |row|\n  puts \"- #{row['key']}: #{row['value']}\"\nend\n\n# Performance check\nputs \"\\n=== Performance Metrics ===\"\nresult = conn.exec(\n  &lt;&lt;~SQL\n    SELECT\n      pg_size_pretty(pg_total_relation_size('nodes')) as nodes_size,\n      pg_size_pretty(pg_total_relation_size('relationships')) as rel_size,\n      pg_size_pretty(pg_total_relation_size('tags')) as tags_size\n  SQL\n)\n\nputs \"Table sizes:\"\nputs \"  nodes: #{result.first['nodes_size']}\"\nputs \"  relationships: #{result.first['rel_size']}\"\nputs \"  tags: #{result.first['tags_size']}\"\n\nconn.close\n\n# Cleanup test data\nputs \"\\n=== Cleanup ===\"\n10.times do |i|\n  htm.forget(\"test_#{i}\", confirm: :confirmed)\nend\nputs \"Test data removed\"\n</code></pre>"},{"location":"guides/multi-robot/","title":"Multi-Robot Usage","text":"<p>HTM's \"hive mind\" architecture enables multiple robots to share knowledge through a common long-term memory. This guide covers setting up multi-robot systems, attribution tracking, and collaboration patterns.</p>"},{"location":"guides/multi-robot/#understanding-the-hive-mind","title":"Understanding the Hive Mind","text":"<p>In HTM, all robots share the same long-term memory database but maintain separate working memories:</p> <p></p> <p>Key Principles:</p> <ul> <li>Shared Knowledge: All memories are accessible to all robots</li> <li>Private Working Memory: Each robot has its own active context</li> <li>Full Attribution: Track which robot added each memory</li> <li>Collective Intelligence: Robots learn from each other's experiences</li> </ul>"},{"location":"guides/multi-robot/#setting-up-multiple-robots","title":"Setting Up Multiple Robots","text":""},{"location":"guides/multi-robot/#basic-multi-robot-setup","title":"Basic Multi-Robot Setup","text":"<pre><code># Robot 1: Research Assistant\nresearch_bot = HTM.new(\n  robot_name: \"Research Assistant\",\n  robot_id: \"research-001\",\n  working_memory_size: 128_000\n)\n\n# Robot 2: Code Helper\ncode_bot = HTM.new(\n  robot_name: \"Code Helper\",\n  robot_id: \"code-001\",\n  working_memory_size: 128_000\n)\n\n# Robot 3: Documentation Writer\ndocs_bot = HTM.new(\n  robot_name: \"Docs Writer\",\n  robot_id: \"docs-001\",\n  working_memory_size: 64_000\n)\n\n# Each robot can access shared knowledge\nresearch_bot.add_node(\n  \"finding_001\",\n  \"Research shows PostgreSQL outperforms MongoDB for ACID workloads\",\n  type: :fact,\n  importance: 8.0,\n  tags: [\"research\", \"database\"]\n)\n\n# Code bot can access this finding\nfindings = code_bot.recall(\n  timeframe: \"last hour\",\n  topic: \"database performance\"\n)\n\n# Docs bot can document it\ndocs_bot.add_node(\n  \"doc_001\",\n  \"PostgreSQL performance documented based on research findings\",\n  type: :context,\n  importance: 6.0,\n  tags: [\"documentation\", \"database\"],\n  related_to: [\"finding_001\"]\n)\n</code></pre>"},{"location":"guides/multi-robot/#robot-identification","title":"Robot Identification","text":""},{"location":"guides/multi-robot/#session-ids-vs-persistent-ids","title":"Session IDs vs Persistent IDs","text":"<p>Choose the right identification strategy:</p> <pre><code># Strategy 1: Persistent Robot (recommended for production)\npersistent_bot = HTM.new(\n  robot_name: \"Production Assistant\",\n  robot_id: \"prod-assistant-001\"  # Fixed, reusable\n)\n\n# Strategy 2: Session-based Robot (for temporary workflows)\nsession_id = SecureRandom.uuid\nsession_bot = HTM.new(\n  robot_name: \"Temp Session\",\n  robot_id: \"session-#{session_id}\"  # Unique per session\n)\n\n# Strategy 3: User-specific Robot\nuser_id = \"alice\"\nuser_bot = HTM.new(\n  robot_name: \"Alice's Assistant\",\n  robot_id: \"user-#{user_id}-assistant\"\n)\n</code></pre> <p>Naming Conventions</p> <ul> <li>Production robots: <code>service-purpose-001</code> (e.g., <code>api-assistant-001</code>)</li> <li>User robots: <code>user-{user_id}-{purpose}</code> (e.g., <code>user-alice-assistant</code>)</li> <li>Session robots: <code>session-{uuid}</code> (e.g., <code>session-abc123...</code>)</li> <li>Team robots: <code>team-{name}-{purpose}</code> (e.g., <code>team-eng-reviewer</code>)</li> </ul>"},{"location":"guides/multi-robot/#robot-registry","title":"Robot Registry","text":"<p>All robots are automatically registered:</p> <pre><code># Robots are registered when created\nbot = HTM.new(robot_name: \"My Bot\", robot_id: \"bot-001\")\n\n# Query robot registry\nconfig = HTM::Database.default_config\nconn = PG.connect(config)\n\nresult = conn.exec(\"SELECT * FROM robots ORDER BY last_active DESC\")\n\nputs \"Registered robots:\"\nresult.each do |row|\n  puts \"#{row['name']} (#{row['id']})\"\n  puts \"  Created: #{row['created_at']}\"\n  puts \"  Last active: #{row['last_active']}\"\n  puts\nend\n\nconn.close\n</code></pre>"},{"location":"guides/multi-robot/#attribution-tracking","title":"Attribution Tracking","text":""},{"location":"guides/multi-robot/#who-said-what","title":"Who Said What?","text":"<p>Track which robot contributed which memories:</p> <pre><code># Add memories from different robots\nalpha = HTM.new(robot_name: \"Alpha\", robot_id: \"alpha\")\nbeta = HTM.new(robot_name: \"Beta\", robot_id: \"beta\")\n\nalpha.add_node(\"alpha_001\", \"Alpha's insight about caching\", type: :fact)\nbeta.add_node(\"beta_001\", \"Beta's approach to testing\", type: :fact)\n\n# Query by robot\ndef memories_by_robot(robot_id)\n  config = HTM::Database.default_config\n  conn = PG.connect(config)\n\n  result = conn.exec_params(\n    \"SELECT key, value, type FROM nodes WHERE robot_id = $1\",\n    [robot_id]\n  )\n\n  memories = result.to_a\n  conn.close\n  memories\nend\n\nalpha_memories = memories_by_robot(\"alpha\")\nputs \"Alpha contributed #{alpha_memories.length} memories\"\n</code></pre>"},{"location":"guides/multi-robot/#which-robot-said","title":"Which Robot Said...?","text":"<p>Use HTM's built-in attribution tracking:</p> <pre><code># Find which robots discussed a topic\nbreakdown = htm.which_robot_said(\"PostgreSQL\")\n\nputs \"Robots that discussed PostgreSQL:\"\nbreakdown.each do |robot_id, count|\n  puts \"  #{robot_id}: #{count} mentions\"\nend\n\n# Example output:\n# Robots that discussed PostgreSQL:\n#   research-001: 15 mentions\n#   code-001: 8 mentions\n#   docs-001: 3 mentions\n</code></pre>"},{"location":"guides/multi-robot/#conversation-timeline","title":"Conversation Timeline","text":"<p>See the chronological conversation across robots:</p> <pre><code>timeline = htm.conversation_timeline(\"architecture decisions\", limit: 50)\n\nputs \"Architecture discussion timeline:\"\ntimeline.each do |entry|\n  puts \"#{entry[:timestamp]} - #{entry[:robot]}\"\n  puts \"  [#{entry[:type]}] #{entry[:content][0..100]}...\"\n  puts\nend\n</code></pre>"},{"location":"guides/multi-robot/#collaboration-patterns","title":"Collaboration Patterns","text":""},{"location":"guides/multi-robot/#pattern-1-specialized-roles","title":"Pattern 1: Specialized Roles","text":"<p>Each robot has a specific role and expertise:</p> <pre><code>class MultiRobotSystem\n  def initialize\n    @researcher = HTM.new(\n      robot_name: \"Researcher\",\n      robot_id: \"researcher-001\"\n    )\n\n    @developer = HTM.new(\n      robot_name: \"Developer\",\n      robot_id: \"developer-001\"\n    )\n\n    @reviewer = HTM.new(\n      robot_name: \"Reviewer\",\n      robot_id: \"reviewer-001\"\n    )\n  end\n\n  def process_feature_request(feature)\n    # 1. Researcher gathers requirements\n    @researcher.add_node(\n      \"research_#{feature}\",\n      \"Research findings for #{feature}\",\n      type: :fact,\n      importance: 8.0,\n      tags: [\"research\", feature]\n    )\n\n    # 2. Developer recalls research and implements\n    research = @developer.recall(\n      timeframe: \"last hour\",\n      topic: \"research #{feature}\"\n    )\n\n    @developer.add_node(\n      \"impl_#{feature}\",\n      \"Implementation plan based on research\",\n      type: :decision,\n      importance: 9.0,\n      tags: [\"implementation\", feature],\n      related_to: [\"research_#{feature}\"]\n    )\n\n    # 3. Reviewer checks work\n    work = @reviewer.recall(\n      timeframe: \"last hour\",\n      topic: feature\n    )\n\n    @reviewer.add_node(\n      \"review_#{feature}\",\n      \"Code review findings\",\n      type: :context,\n      importance: 7.0,\n      tags: [\"review\", feature]\n    )\n  end\nend\n\nsystem = MultiRobotSystem.new\nsystem.process_feature_request(\"user-authentication\")\n</code></pre>"},{"location":"guides/multi-robot/#pattern-2-shift-handoff","title":"Pattern 2: Shift Handoff","text":"<p>Robots pass context between shifts:</p> <pre><code>class ShiftHandoff\n  def initialize\n    @current_shift = nil\n  end\n\n  def start_shift(shift_name)\n    @current_shift = HTM.new(\n      robot_name: \"#{shift_name} Bot\",\n      robot_id: \"shift-#{shift_name.downcase}\"\n    )\n\n    # Recall context from previous shift\n    handoff = @current_shift.recall(\n      timeframe: \"last 24 hours\",\n      topic: \"shift handoff urgent\",\n      strategy: :hybrid,\n      limit: 20\n    )\n\n    puts \"#{shift_name} shift starting\"\n    puts \"Received #{handoff.length} items from previous shift\"\n\n    handoff\n  end\n\n  def end_shift(summary)\n    # Document shift handoff\n    @current_shift.add_node(\n      \"handoff_#{Time.now.to_i}\",\n      summary,\n      type: :context,\n      importance: 9.0,\n      tags: [\"shift-handoff\", \"urgent\"]\n    )\n\n    puts \"Shift handoff documented\"\n  end\nend\n\n# Usage\nhandoff = ShiftHandoff.new\n\n# Morning shift\nmorning = handoff.start_shift(\"Morning\")\n# ... do morning work\nhandoff.end_shift(\"Three critical bugs fixed, deploy scheduled for 2pm\")\n\n# Afternoon shift\nafternoon = handoff.start_shift(\"Afternoon\")\n# ... receives morning's summary\n</code></pre>"},{"location":"guides/multi-robot/#pattern-3-expert-consultation","title":"Pattern 3: Expert Consultation","text":"<p>Specialized experts provide knowledge:</p> <pre><code>class ExpertSystem\n  def initialize\n    @experts = {\n      database: HTM.new(\n        robot_name: \"Database Expert\",\n        robot_id: \"expert-database\"\n      ),\n      security: HTM.new(\n        robot_name: \"Security Expert\",\n        robot_id: \"expert-security\"\n      ),\n      performance: HTM.new(\n        robot_name: \"Performance Expert\",\n        robot_id: \"expert-performance\"\n      )\n    }\n\n    @general = HTM.new(\n      robot_name: \"General Assistant\",\n      robot_id: \"assistant-general\"\n    )\n  end\n\n  def consult(topic)\n    # Determine which expert to consult\n    expert_type = determine_expert(topic)\n    expert = @experts[expert_type]\n\n    # Get expert knowledge\n    knowledge = expert.recall(\n      timeframe: \"all time\",\n      topic: topic,\n      strategy: :hybrid,\n      limit: 10\n    )\n\n    # General assistant learns from expert\n    knowledge.each do |k|\n      @general.add_node(\n        \"learned_#{SecureRandom.hex(4)}\",\n        \"Learned from #{expert_type} expert: #{k['value']}\",\n        type: :fact,\n        importance: k['importance'],\n        tags: [\"learned\", expert_type.to_s],\n        related_to: [k['key']]\n      )\n    end\n\n    knowledge\n  end\n\n  private\n\n  def determine_expert(topic)\n    # Simple keyword matching\n    case topic.downcase\n    when /database|sql|query/\n      :database\n    when /security|auth|encryption/\n      :security\n    when /performance|speed|optimization/\n      :performance\n    else\n      :database  # default\n    end\n  end\nend\n\nsystem = ExpertSystem.new\nknowledge = system.consult(\"PostgreSQL query optimization\")\n</code></pre>"},{"location":"guides/multi-robot/#pattern-4-collaborative-decision-making","title":"Pattern 4: Collaborative Decision Making","text":"<p>Multiple robots contribute to decisions:</p> <pre><code>class CollaborativeDecision\n  def initialize(topic)\n    @topic = topic\n    @participants = []\n  end\n\n  def add_participant(name, role)\n    bot = HTM.new(\n      robot_name: \"#{name} (#{role})\",\n      robot_id: \"decision-#{role.downcase}-#{SecureRandom.hex(4)}\"\n    )\n    @participants &lt;&lt; { name: name, role: role, bot: bot }\n    bot\n  end\n\n  def gather_input(bot, opinion)\n    bot.add_node(\n      \"opinion_#{SecureRandom.hex(4)}\",\n      opinion,\n      type: :context,\n      importance: 8.0,\n      tags: [\"decision\", @topic, \"opinion\"]\n    )\n  end\n\n  def make_decision(decision_maker)\n    # Recall all opinions\n    opinions = decision_maker.recall(\n      timeframe: \"last hour\",\n      topic: \"decision #{@topic} opinion\",\n      strategy: :hybrid,\n      limit: 50\n    )\n\n    puts \"#{decision_maker.robot_name} considering:\"\n    opinions.each do |opinion|\n      puts \"- #{opinion['value'][0..100]}...\"\n    end\n\n    # Document final decision\n    decision_maker.add_node(\n      \"decision_#{@topic}_final\",\n      \"Final decision on #{@topic} after considering team input\",\n      type: :decision,\n      importance: 10.0,\n      tags: [\"decision\", @topic, \"final\"]\n    )\n  end\nend\n\n# Usage\ndecision = CollaborativeDecision.new(\"database-choice\")\n\n# Gather input\ndeveloper = decision.add_participant(\"Alice\", \"Developer\")\ndecision.gather_input(developer, \"PostgreSQL for reliability\")\n\narchitect = decision.add_participant(\"Bob\", \"Architect\")\ndecision.gather_input(architect, \"PostgreSQL for ACID compliance\")\n\ndba = decision.add_participant(\"Carol\", \"DBA\")\ndecision.gather_input(dba, \"PostgreSQL for operational maturity\")\n\n# Make decision\nlead = decision.add_participant(\"Dave\", \"TechLead\")\ndecision.make_decision(lead)\n</code></pre>"},{"location":"guides/multi-robot/#shared-vs-private-knowledge","title":"Shared vs Private Knowledge","text":""},{"location":"guides/multi-robot/#sharing-strategies","title":"Sharing Strategies","text":"<p>Control what gets shared:</p> <pre><code>class SmartSharing\n  def initialize(robot_id)\n    @htm = HTM.new(robot_name: \"Smart Bot\", robot_id: robot_id)\n    @private_prefix = \"private_#{robot_id}_\"\n  end\n\n  def add_shared(key, value, **opts)\n    # Shared with all robots\n    @htm.add_node(key, value, **opts.merge(\n      tags: (opts[:tags] || []) + [\"shared\"]\n    ))\n  end\n\n  def add_private(key, value, **opts)\n    # Use robot-specific key prefix\n    private_key = \"#{@private_prefix}#{key}\"\n    @htm.add_node(private_key, value, **opts.merge(\n      tags: (opts[:tags] || []) + [\"private\"],\n      importance: (opts[:importance] || 5.0)\n    ))\n  end\n\n  def recall_shared(topic)\n    # Only shared knowledge\n    @htm.recall(\n      timeframe: \"all time\",\n      topic: \"shared #{topic}\",\n      strategy: :hybrid\n    ).select { |m| m['tags']&amp;.include?(\"shared\") }\n  end\n\n  def recall_private(topic)\n    # Only my private knowledge\n    @htm.recall(\n      timeframe: \"all time\",\n      topic: topic,\n      strategy: :hybrid\n    ).select { |m| m['key'].start_with?(@private_prefix) }\n  end\nend\n\n# Usage\nbot1 = SmartSharing.new(\"bot-001\")\nbot1.add_shared(\"shared_fact\", \"Everyone should know this\", type: :fact)\nbot1.add_private(\"my_thought\", \"Private thought\", type: :context)\n\nbot2 = SmartSharing.new(\"bot-002\")\nshared = bot2.recall_shared(\"fact\")  # Can see shared_fact\nprivate = bot2.recall_private(\"thought\")  # Won't see bot1's private thoughts\n</code></pre>"},{"location":"guides/multi-robot/#cross-robot-queries","title":"Cross-Robot Queries","text":""},{"location":"guides/multi-robot/#finding-robot-activity","title":"Finding Robot Activity","text":"<pre><code># Get all robots and their activity\ndef get_robot_activity\n  config = HTM::Database.default_config\n  conn = PG.connect(config)\n\n  result = conn.exec(\n    &lt;&lt;~SQL\n      SELECT\n        r.id,\n        r.name,\n        COUNT(n.id) as memory_count,\n        MAX(n.created_at) as last_memory,\n        r.last_active\n      FROM robots r\n      LEFT JOIN nodes n ON r.id = n.robot_id\n      GROUP BY r.id, r.name, r.last_active\n      ORDER BY r.last_active DESC\n    SQL\n  )\n\n  robots = result.to_a\n  conn.close\n  robots\nend\n\n# Display activity\nrobots = get_robot_activity\nputs \"Robot Activity Report:\"\nrobots.each do |r|\n  puts \"\\n#{r['name']} (#{r['id']})\"\n  puts \"  Memories: #{r['memory_count']}\"\n  puts \"  Last memory: #{r['last_memory']}\"\n  puts \"  Last active: #{r['last_active']}\"\nend\n</code></pre>"},{"location":"guides/multi-robot/#cross-robot-search","title":"Cross-Robot Search","text":"<pre><code>def search_across_robots(topic, limit_per_robot: 5)\n  config = HTM::Database.default_config\n  conn = PG.connect(config)\n\n  # Get all robots\n  robots = conn.exec(\"SELECT id, name FROM robots\")\n\n  results = {}\n\n  robots.each do |robot|\n    # Search memories from this robot\n    stmt = conn.prepare(\n      \"search_#{robot['id']}\",\n      &lt;&lt;~SQL\n        SELECT key, value, type, importance, created_at\n        FROM nodes\n        WHERE robot_id = $1\n        AND to_tsvector('english', value) @@ plainto_tsquery('english', $2)\n        ORDER BY importance DESC\n        LIMIT $3\n      SQL\n    )\n\n    robot_results = conn.exec_prepared(\n      \"search_#{robot['id']}\",\n      [robot['id'], topic, limit_per_robot]\n    )\n\n    results[robot['name']] = robot_results.to_a\n  end\n\n  conn.close\n  results\nend\n\n# Usage\nresults = search_across_robots(\"authentication\")\nresults.each do |robot_name, memories|\n  puts \"\\n=== #{robot_name} ===\"\n  memories.each do |m|\n    puts \"- [#{m['type']}] #{m['value'][0..80]}...\"\n  end\nend\n</code></pre>"},{"location":"guides/multi-robot/#monitoring-multi-robot-systems","title":"Monitoring Multi-Robot Systems","text":""},{"location":"guides/multi-robot/#dashboard","title":"Dashboard","text":"<pre><code>class MultiRobotDashboard\n  def initialize\n    @config = HTM::Database.default_config\n  end\n\n  def summary\n    conn = PG.connect(@config)\n\n    # Total stats\n    total_robots = conn.exec(\"SELECT COUNT(*) FROM robots\").first['count'].to_i\n    total_memories = conn.exec(\"SELECT COUNT(*) FROM nodes\").first['count'].to_i\n\n    # Per-robot breakdown\n    breakdown = conn.exec(\n      &lt;&lt;~SQL\n        SELECT\n          r.name,\n          COUNT(n.id) as memories,\n          AVG(n.importance) as avg_importance,\n          MAX(n.created_at) as last_contribution\n        FROM robots r\n        LEFT JOIN nodes n ON r.id = n.robot_id\n        GROUP BY r.id, r.name\n        ORDER BY memories DESC\n      SQL\n    ).to_a\n\n    conn.close\n\n    {\n      total_robots: total_robots,\n      total_memories: total_memories,\n      breakdown: breakdown\n    }\n  end\n\n  def print_summary\n    data = summary\n\n    puts \"=== Multi-Robot System Dashboard ===\"\n    puts \"Total robots: #{data[:total_robots]}\"\n    puts \"Total memories: #{data[:total_memories]}\"\n    puts \"\\nPer-robot breakdown:\"\n\n    data[:breakdown].each do |robot|\n      puts \"\\n#{robot['name']}\"\n      puts \"  Memories: #{robot['memories']}\"\n      puts \"  Avg importance: #{robot['avg_importance'].to_f.round(2)}\"\n      puts \"  Last contribution: #{robot['last_contribution']}\"\n    end\n  end\nend\n\ndashboard = MultiRobotDashboard.new\ndashboard.print_summary\n</code></pre>"},{"location":"guides/multi-robot/#best-practices","title":"Best Practices","text":""},{"location":"guides/multi-robot/#1-clear-robot-roles","title":"1. Clear Robot Roles","text":"<pre><code># Good: Clear, specific roles\nresearcher = HTM.new(robot_name: \"Research Specialist\", robot_id: \"research-001\")\ncoder = HTM.new(robot_name: \"Code Generator\", robot_id: \"coder-001\")\n\n# Avoid: Vague roles\nbot1 = HTM.new(robot_name: \"Bot 1\", robot_id: \"bot1\")\n</code></pre>"},{"location":"guides/multi-robot/#2-consistent-naming","title":"2. Consistent Naming","text":"<pre><code># Good: Consistent naming scheme\nclass RobotFactory\n  def self.create(service, purpose, instance = \"001\")\n    HTM.new(\n      robot_name: \"#{service.capitalize} #{purpose.capitalize}\",\n      robot_id: \"#{service}-#{purpose}-#{instance}\"\n    )\n  end\nend\n\napi_assistant = RobotFactory.create(\"api\", \"assistant\", \"001\")\napi_validator = RobotFactory.create(\"api\", \"validator\", \"001\")\n</code></pre>"},{"location":"guides/multi-robot/#3-attribution-in-content","title":"3. Attribution in Content","text":"<pre><code># Include attribution in the content itself\nbot.add_node(\n  \"finding_001\",\n  \"Research by #{bot.robot_name}: PostgreSQL outperforms MongoDB\",\n  type: :fact,\n  importance: 8.0\n)\n</code></pre>"},{"location":"guides/multi-robot/#4-regular-reconciliation","title":"4. Regular Reconciliation","text":"<pre><code># Periodically sync understanding across robots\ndef sync_robots(*robots)\n  # Find recent high-importance memories\n  shared_knowledge = robots.first.recall(\n    timeframe: \"last 24 hours\",\n    topic: \"important shared\",\n    strategy: :hybrid,\n    limit: 50\n  ).select { |m| m['importance'].to_f &gt;= 8.0 }\n\n  puts \"Syncing #{shared_knowledge.length} important memories across #{robots.length} robots\"\nend\n</code></pre>"},{"location":"guides/multi-robot/#5-clean-up-inactive-robots","title":"5. Clean Up Inactive Robots","text":"<pre><code>def cleanup_inactive_robots(days: 30)\n  config = HTM::Database.default_config\n  conn = PG.connect(config)\n\n  cutoff = Time.now - (days * 24 * 3600)\n\n  result = conn.exec_params(\n    \"SELECT id, name FROM robots WHERE last_active &lt; $1\",\n    [cutoff]\n  )\n\n  puts \"Inactive robots (last active &gt; #{days} days):\"\n  result.each do |robot|\n    puts \"- #{robot['name']} (#{robot['id']})\"\n  end\n\n  conn.close\nend\n\ncleanup_inactive_robots(days: 90)\n</code></pre>"},{"location":"guides/multi-robot/#complete-example","title":"Complete Example","text":"<pre><code>require 'htm'\n\n# Create a multi-robot development team\nclass DevTeam\n  def initialize\n    @analyst = HTM.new(\n      robot_name: \"Requirements Analyst\",\n      robot_id: \"team-analyst-001\"\n    )\n\n    @developer = HTM.new(\n      robot_name: \"Senior Developer\",\n      robot_id: \"team-developer-001\"\n    )\n\n    @tester = HTM.new(\n      robot_name: \"QA Tester\",\n      robot_id: \"team-tester-001\"\n    )\n  end\n\n  def process_feature(feature_name)\n    puts \"\\n=== Processing Feature: #{feature_name} ===\"\n\n    # 1. Analyst documents requirements\n    puts \"\\n1. Analyst gathering requirements...\"\n    @analyst.add_node(\n      \"req_#{feature_name}\",\n      \"Requirements for #{feature_name}: Must support OAuth2\",\n      type: :fact,\n      importance: 9.0,\n      tags: [\"requirements\", feature_name]\n    )\n\n    # 2. Developer recalls requirements and designs\n    puts \"\\n2. Developer reviewing requirements...\"\n    requirements = @developer.recall(\n      timeframe: \"last hour\",\n      topic: \"requirements #{feature_name}\"\n    )\n\n    puts \"Found #{requirements.length} requirements\"\n\n    @developer.add_node(\n      \"design_#{feature_name}\",\n      \"Design for #{feature_name} based on requirements\",\n      type: :decision,\n      importance: 9.0,\n      tags: [\"design\", feature_name],\n      related_to: [\"req_#{feature_name}\"]\n    )\n\n    # 3. Tester recalls everything and creates test plan\n    puts \"\\n3. Tester creating test plan...\"\n    context = @tester.recall(\n      timeframe: \"last hour\",\n      topic: feature_name,\n      strategy: :hybrid\n    )\n\n    puts \"Tester reviewed #{context.length} items\"\n\n    @tester.add_node(\n      \"test_#{feature_name}\",\n      \"Test plan for #{feature_name}\",\n      type: :context,\n      importance: 8.0,\n      tags: [\"testing\", feature_name],\n      related_to: [\"design_#{feature_name}\", \"req_#{feature_name}\"]\n    )\n\n    # 4. Show collaboration\n    puts \"\\n4. Collaboration summary:\"\n    timeline = @analyst.conversation_timeline(feature_name)\n    timeline.each do |entry|\n      puts \"- #{entry[:robot]}: #{entry[:type]}\"\n    end\n\n    # 5. Show attribution\n    puts \"\\n5. Who contributed:\"\n    breakdown = @analyst.which_robot_said(feature_name)\n    breakdown.each do |robot_id, count|\n      puts \"- #{robot_id}: #{count} memories\"\n    end\n  end\nend\n\n# Run the team\nteam = DevTeam.new\nteam.process_feature(\"oauth-integration\")\n</code></pre>"},{"location":"guides/multi-robot/#next-steps","title":"Next Steps","text":"<ul> <li>Context Assembly - Build context from multi-robot memories</li> <li>Long-term Memory - Understand the shared storage layer</li> <li>Search Strategies - Find relevant memories across robots</li> </ul>"},{"location":"guides/recalling-memories/","title":"Recalling Memories from HTM","text":"<p>This guide covers HTM's powerful RAG-based retrieval system for finding relevant memories from your knowledge base.</p>"},{"location":"guides/recalling-memories/#basic-recall","title":"Basic Recall","text":"<p>The <code>recall</code> method searches long-term memory using timeframe and topic:</p> <pre><code>memories = htm.recall(\n  timeframe: \"last week\",     # Time range to search\n  topic: \"database design\",   # What to search for\n  limit: 20,                  # Max results (default: 20)\n  strategy: :vector          # Search strategy (default: :vector)\n)\n\nmemories.each do |memory|\n  puts memory['value']\n  puts \"Similarity: #{memory['similarity']}\"\n  puts \"Importance: #{memory['importance']}\"\n  puts \"Created: #{memory['created_at']}\"\n  puts\nend\n</code></pre> <p> <p>HTM RAG-Based Recall Process</p> <p> 1. User Query recall(   \"database design\")</p> <p></p> <p> 2. Generate Embedding Ollama/OpenAI [0.23, -0.57, ...]</p> <p></p> <p> 3. Search Database Vector + Temporal + Full-text</p> <p> </p> <p> Vector Search pgvector HNSW Cosine similarity Semantic matching ~80ms</p> <p> Full-Text Search PostgreSQL GIN ts_query matching Keyword matching ~30ms</p> <p> Hybrid Search Both searches RRF scoring Best results ~120ms</p> <p> </p> <p> 4. Ranked Results 1. \"PostgreSQL design\" (0.92) 2. \"Database schema\" (0.89) 3. \"Table relationships\" (0.85)</p> <p></p> <p> 5. Load to Working Memory \u2022 Add to in-memory cache \u2022 Fast LLM access \u2022 Return to user</p> <p> Key Features: \u2713 Temporal filtering \u2713 Semantic search \u2713 Keyword matching \u2713 Importance ranking</p> <p> Performance: Vector: ~80ms Full-text: ~30ms Hybrid: ~120ms \u2713 Optimized indexes</p> <p> </p>"},{"location":"guides/recalling-memories/#understanding-timeframes","title":"Understanding Timeframes","text":"<p>HTM supports both natural language timeframes and explicit ranges.</p>"},{"location":"guides/recalling-memories/#natural-language-timeframes","title":"Natural Language Timeframes","text":"<pre><code># Last 24 hours (default if unparseable)\nhtm.recall(timeframe: \"today\", topic: \"...\")\n\n# Yesterday\nhtm.recall(timeframe: \"yesterday\", topic: \"...\")\n\n# Last week\nhtm.recall(timeframe: \"last week\", topic: \"...\")\n\n# Last N days\nhtm.recall(timeframe: \"last 7 days\", topic: \"...\")\nhtm.recall(timeframe: \"last 30 days\", topic: \"...\")\n\n# This month\nhtm.recall(timeframe: \"this month\", topic: \"...\")\n\n# Last month\nhtm.recall(timeframe: \"last month\", topic: \"...\")\n</code></pre>"},{"location":"guides/recalling-memories/#explicit-time-ranges","title":"Explicit Time Ranges","text":"<p>For precise control, use Ruby time ranges:</p> <pre><code># Specific date range\nstart_date = Time.new(2024, 1, 1)\nend_date = Time.new(2024, 12, 31)\nhtm.recall(\n  timeframe: start_date..end_date,\n  topic: \"annual report\"\n)\n\n# Last 24 hours precisely\nhtm.recall(\n  timeframe: (Time.now - 24*3600)..Time.now,\n  topic: \"errors\"\n)\n\n# All time\nhtm.recall(\n  timeframe: Time.at(0)..Time.now,\n  topic: \"architecture decisions\"\n)\n\n# Relative to current time\nthree_days_ago = Time.now - (3 * 24 * 3600)\nhtm.recall(\n  timeframe: three_days_ago..Time.now,\n  topic: \"bug fixes\"\n)\n</code></pre> <p>Choosing Timeframes</p> <ul> <li>Use narrow timeframes (days/weeks) for recent context</li> <li>Use wide timeframes (months/years) for historical facts</li> <li>Use \"all time\" for searching unchanging facts or decisions</li> </ul>"},{"location":"guides/recalling-memories/#search-strategies","title":"Search Strategies","text":"<p>HTM provides three search strategies, each with different strengths.</p>"},{"location":"guides/recalling-memories/#vector-search-semantic","title":"Vector Search (Semantic)","text":"<p>Vector search uses embeddings to find semantically similar memories.</p> <pre><code>memories = htm.recall(\n  timeframe: \"last month\",\n  topic: \"improving application performance\",\n  strategy: :vector,\n  limit: 10\n)\n</code></pre> <p>How it works:</p> <ol> <li>Converts your topic to a vector embedding via Ollama</li> <li>Finds memories with similar embeddings using cosine similarity</li> <li>Returns results ordered by semantic similarity</li> </ol> <p>Best for:</p> <ul> <li>Conceptual searches (\"how to optimize queries\")</li> <li>Related topics (\"database\" finds \"PostgreSQL\", \"SQL\")</li> <li>Fuzzy matching (\"ML\" finds \"machine learning\")</li> <li>Understanding user intent</li> </ul> <p>Example:</p> <pre><code># Will find memories about databases, even without the word \"PostgreSQL\"\nmemories = htm.recall(\n  timeframe: \"last year\",\n  topic: \"data persistence strategies\",\n  strategy: :vector\n)\n\n# Finds: \"Use PostgreSQL\", \"Database indexing\", \"SQL optimization\"\n</code></pre> <p>Similarity Scores</p> <p>Vector search returns a <code>similarity</code> score (0-1). Scores &gt; 0.8 indicate high relevance, 0.6-0.8 moderate relevance, &lt; 0.6 low relevance.</p>"},{"location":"guides/recalling-memories/#full-text-search-keywords","title":"Full-text Search (Keywords)","text":"<p>Full-text search uses PostgreSQL's text search for exact keyword matching.</p> <pre><code>memories = htm.recall(\n  timeframe: \"last week\",\n  topic: \"PostgreSQL indexing\",\n  strategy: :fulltext,\n  limit: 10\n)\n</code></pre> <p>How it works:</p> <ol> <li>Tokenizes your query into keywords</li> <li>Uses PostgreSQL's <code>ts_vector</code> and <code>ts_query</code> for matching</li> <li>Returns results ranked by text relevance</li> </ol> <p>Best for:</p> <ul> <li>Exact keyword matches (\"PostgreSQL\", \"Redis\")</li> <li>Technical terms (\"JWT\", \"OAuth\")</li> <li>Proper nouns (\"Alice\", \"Project Phoenix\")</li> <li>Acronyms (\"API\", \"SQL\", \"REST\")</li> </ul> <p>Example:</p> <pre><code># Will only find memories containing \"JWT\"\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"JWT authentication\",\n  strategy: :fulltext\n)\n\n# Finds: \"JWT token validation\", \"Implemented JWT auth\"\n# Misses: \"Token-based authentication\" (no keyword match)\n</code></pre> <p>Ranking Scores</p> <p>Full-text search returns a <code>rank</code> score. Higher values indicate better keyword matches.</p>"},{"location":"guides/recalling-memories/#hybrid-search-best-of-both","title":"Hybrid Search (Best of Both)","text":"<p>Hybrid search combines full-text and vector search for optimal results.</p> <pre><code>memories = htm.recall(\n  timeframe: \"last month\",\n  topic: \"database performance issues\",\n  strategy: :hybrid,\n  limit: 10\n)\n</code></pre> <p>How it works:</p> <ol> <li>First, runs full-text search to find keyword matches (prefilter)</li> <li>Then, ranks those results by vector similarity</li> <li>Combines precision of keywords with understanding of semantics</li> </ol> <p>Best for:</p> <ul> <li>General-purpose searches (default recommendation)</li> <li>When you want both keyword matches and related concepts</li> <li>Balancing precision and recall</li> <li>Production applications</li> </ul> <p>Example:</p> <pre><code># Combines keyword matching with semantic understanding\nmemories = htm.recall(\n  timeframe: \"last quarter\",\n  topic: \"scaling our PostgreSQL database\",\n  strategy: :hybrid\n)\n\n# Prefilter: Finds all memories mentioning \"PostgreSQL\" or \"database\"\n# Ranking: Orders by semantic similarity to \"scaling\" concepts\n</code></pre> <p>When to Use Hybrid</p> <p>Hybrid is the recommended default strategy. It provides good results across different query types without needing to choose between vector and full-text.</p>"},{"location":"guides/recalling-memories/#search-strategy-comparison","title":"Search Strategy Comparison","text":"Strategy Speed Accuracy Best Use Case Vector Medium High for concepts Understanding intent, related topics Full-text Fast High for keywords Exact terms, proper nouns Hybrid Medium Highest overall General purpose, best default"},{"location":"guides/recalling-memories/#query-optimization-tips","title":"Query Optimization Tips","text":""},{"location":"guides/recalling-memories/#1-be-specific","title":"1. Be Specific","text":"<pre><code># Vague: Returns too many irrelevant results\nhtm.recall(timeframe: \"last year\", topic: \"data\")\n\n# Specific: Returns targeted results\nhtm.recall(timeframe: \"last year\", topic: \"PostgreSQL query optimization\")\n</code></pre>"},{"location":"guides/recalling-memories/#2-use-appropriate-timeframes","title":"2. Use Appropriate Timeframes","text":"<pre><code># Too wide: Includes outdated information\nhtm.recall(timeframe: \"last 5 years\", topic: \"current project status\")\n\n# Right size: Recent context\nhtm.recall(timeframe: \"last week\", topic: \"current project status\")\n</code></pre>"},{"location":"guides/recalling-memories/#3-adjust-limit-based-on-need","title":"3. Adjust Limit Based on Need","text":"<pre><code># Few results: Quick overview\nhtm.recall(timeframe: \"last month\", topic: \"errors\", limit: 5)\n\n# Many results: Comprehensive search\nhtm.recall(timeframe: \"last year\", topic: \"architecture decisions\", limit: 50)\n</code></pre>"},{"location":"guides/recalling-memories/#4-try-different-strategies","title":"4. Try Different Strategies","text":"<pre><code># Start with hybrid (best all-around)\nresults = htm.recall(topic: \"authentication\", strategy: :hybrid)\n\n# If too many results, try full-text (more precise)\nresults = htm.recall(topic: \"JWT authentication\", strategy: :fulltext)\n\n# If no results, try vector (more flexible)\nresults = htm.recall(topic: \"user validation methods\", strategy: :vector)\n</code></pre>"},{"location":"guides/recalling-memories/#filtering-by-metadata","title":"Filtering by Metadata","text":"<p>HTM supports metadata filtering directly in the <code>recall()</code> method. This is more efficient than post-filtering because the database does the work.</p> <pre><code># Filter by single metadata field\nmemories = htm.recall(\n  topic: \"user settings\",\n  metadata: { category: \"preference\" }\n)\n# =&gt; Returns only nodes with metadata containing { category: \"preference\" }\n\n# Filter by multiple metadata fields\nmemories = htm.recall(\n  topic: \"API configuration\",\n  metadata: { environment: \"production\", version: 2 }\n)\n# =&gt; Returns nodes with BOTH environment: \"production\" AND version: 2\n\n# Combine with other filters\nmemories = htm.recall(\n  topic: \"database changes\",\n  timeframe: \"last month\",\n  strategy: :hybrid,\n  metadata: { breaking_change: true },\n  limit: 10\n)\n</code></pre> <p>Metadata filtering uses PostgreSQL's JSONB containment operator (<code>@&gt;</code>), which means: - The node's metadata must contain ALL the key-value pairs you specify - The node's metadata can have additional fields (they're ignored) - Nested objects work: <code>metadata: { user: { role: \"admin\" } }</code> matches <code>{ user: { role: \"admin\", name: \"...\" } }</code></p>"},{"location":"guides/recalling-memories/#combining-search-with-filters","title":"Combining Search with Filters","text":"<p>While <code>recall</code> handles timeframes, topics, and metadata, you can filter results further:</p> <pre><code># Recall memories\nmemories = htm.recall(\n  timeframe: \"last month\",\n  topic: \"database\",\n  strategy: :hybrid,\n  limit: 50\n)\n\n# Filter by type\ndecisions = memories.select { |m| m['type'] == 'decision' }\n\n# Filter by importance\ncritical = memories.select { |m| m['importance'].to_f &gt;= 8.0 }\n\n# Filter by robot\nmy_memories = memories.select { |m| m['robot_id'] == htm.robot_id }\n\n# Filter by date\nrecent = memories.select do |m|\n  Time.parse(m['created_at']) &gt; Time.now - 7*24*3600\nend\n</code></pre>"},{"location":"guides/recalling-memories/#advanced-query-patterns","title":"Advanced Query Patterns","text":""},{"location":"guides/recalling-memories/#pattern-1-multi-topic-search","title":"Pattern 1: Multi-Topic Search","text":"<p>Search for multiple related topics:</p> <pre><code>def search_multiple_topics(timeframe, topics, strategy: :hybrid, limit: 10)\n  results = []\n\n  topics.each do |topic|\n    results.concat(\n      htm.recall(\n        timeframe: timeframe,\n        topic: topic,\n        strategy: strategy,\n        limit: limit\n      )\n    )\n  end\n\n  # Remove duplicates by key\n  results.uniq { |m| m['key'] }\nend\n\n# Usage\nmemories = search_multiple_topics(\n  \"last month\",\n  [\"database optimization\", \"query performance\", \"indexing strategies\"]\n)\n</code></pre>"},{"location":"guides/recalling-memories/#pattern-2-iterative-refinement","title":"Pattern 2: Iterative Refinement","text":"<p>Start broad, then narrow:</p> <pre><code># First pass: Broad search\nbroad_results = htm.recall(\n  timeframe: \"last year\",\n  topic: \"architecture\",\n  strategy: :vector,\n  limit: 100\n)\n\n# Analyze results, refine query\nrelevant_terms = broad_results\n  .select { |m| m['similarity'].to_f &gt; 0.7 }\n  .flat_map { |m| m['tags'] }\n  .uniq\n\n# Second pass: Refined search\nrefined_results = htm.recall(\n  timeframe: \"last year\",\n  topic: \"architecture #{relevant_terms.join(' ')}\",\n  strategy: :hybrid,\n  limit: 20\n)\n</code></pre>"},{"location":"guides/recalling-memories/#pattern-3-threshold-filtering","title":"Pattern 3: Threshold Filtering","text":"<p>Only keep high-quality matches:</p> <pre><code>def recall_with_threshold(timeframe:, topic:, threshold: 0.7, strategy: :vector)\n  results = htm.recall(\n    timeframe: timeframe,\n    topic: topic,\n    strategy: strategy,\n    limit: 50  # Get more candidates\n  )\n\n  # Filter by similarity threshold\n  case strategy\n  when :vector, :hybrid\n    results.select { |m| m['similarity'].to_f &gt;= threshold }\n  when :fulltext\n    # For fulltext, use rank threshold (adjust as needed)\n    results.select { |m| m['rank'].to_f &gt;= threshold }\n  end\nend\n\n# Usage\nhigh_quality = recall_with_threshold(\n  timeframe: \"last month\",\n  topic: \"performance optimization\",\n  threshold: 0.8\n)\n</code></pre>"},{"location":"guides/recalling-memories/#pattern-4-time-weighted-search","title":"Pattern 4: Time-Weighted Search","text":"<p>Weight results by recency:</p> <pre><code>def recall_time_weighted(timeframe:, topic:, recency_weight: 0.3)\n  memories = htm.recall(\n    timeframe: timeframe,\n    topic: topic,\n    strategy: :hybrid,\n    limit: 50\n  )\n\n  # Calculate time-weighted score\n  now = Time.now\n  memories.each do |m|\n    created = Time.parse(m['created_at'])\n    age_days = (now - created) / (24 * 3600)\n\n    # Decay factor: newer is better\n    recency_score = Math.exp(-age_days / 30.0)  # 30-day half-life\n\n    # Combine similarity and recency\n    similarity = m['similarity'].to_f\n    m['weighted_score'] = (\n      similarity * (1 - recency_weight) +\n      recency_score * recency_weight\n    )\n  end\n\n  # Sort by weighted score\n  memories.sort_by { |m| -m['weighted_score'] }\nend\n</code></pre>"},{"location":"guides/recalling-memories/#pattern-5-context-aware-search","title":"Pattern 5: Context-Aware Search","text":"<p>Include current context in search:</p> <pre><code>class ContextualRecall\n  def initialize(htm)\n    @htm = htm\n    @current_context = []\n  end\n\n  def add_context(key, value)\n    @current_context &lt;&lt; { key: key, value: value }\n  end\n\n  def recall(timeframe:, topic:, strategy: :hybrid)\n    # Enhance topic with current context\n    context_terms = @current_context.map { |c| c[:value] }.join(\" \")\n    enhanced_topic = \"#{topic} #{context_terms}\"\n\n    @htm.recall(\n      timeframe: timeframe,\n      topic: enhanced_topic,\n      strategy: strategy,\n      limit: 20\n    )\n  end\nend\n\n# Usage\nrecall = ContextualRecall.new(htm)\nrecall.add_context(\"project\", \"e-commerce platform\")\nrecall.add_context(\"focus\", \"checkout flow\")\n\n# Search includes context automatically\nresults = recall.recall(\n  timeframe: \"last month\",\n  topic: \"payment processing\"\n)\n</code></pre>"},{"location":"guides/recalling-memories/#retrieving-specific-memories","title":"Retrieving Specific Memories","text":"<p>For known keys, use <code>retrieve</code> instead of <code>recall</code>:</p> <pre><code># Retrieve by exact key\nmemory = htm.retrieve(\"decision_database\")\n\nif memory\n  puts memory['value']\n  puts \"Type: #{memory['type']}\"\n  puts \"Created: #{memory['created_at']}\"\nelse\n  puts \"Memory not found\"\nend\n</code></pre> <p>Note</p> <p><code>retrieve</code> is faster than <code>recall</code> because it doesn't require embedding generation or similarity calculation.</p>"},{"location":"guides/recalling-memories/#working-with-search-results","title":"Working with Search Results","text":""},{"location":"guides/recalling-memories/#result-structure","title":"Result Structure","text":"<p>Each memory returned by <code>recall</code> has these fields:</p> <pre><code>memory = {\n  'id' =&gt; 123,                           # Database ID\n  'key' =&gt; \"decision_001\",               # Unique key\n  'value' =&gt; \"Decision text...\",         # Content\n  'type' =&gt; \"decision\",                  # Memory type\n  'category' =&gt; \"architecture\",          # Category (if set)\n  'importance' =&gt; 9.0,                   # Importance score\n  'created_at' =&gt; \"2024-01-15 10:30:00\", # Timestamp\n  'robot_id' =&gt; \"uuid...\",               # Which robot added it\n  'token_count' =&gt; 150,                  # Token count\n  'metadata' =&gt; { 'priority' =&gt; 'high', 'version' =&gt; 2 },  # JSONB metadata\n  'similarity' =&gt; 0.85                   # Similarity score (vector/hybrid)\n  # or 'rank' for fulltext\n}\n</code></pre>"},{"location":"guides/recalling-memories/#processing-results","title":"Processing Results","text":"<pre><code>memories = htm.recall(timeframe: \"last month\", topic: \"errors\")\n\n# Sort by importance\nby_importance = memories.sort_by { |m| -m['importance'].to_f }\n\n# Group by type\nby_type = memories.group_by { |m| m['type'] }\n\n# Extract just the content\ncontent = memories.map { |m| m['value'] }\n\n# Create summary\nsummary = memories.map do |m|\n  \"[#{m['type']}] #{m['value'][0..100]}... (#{m['importance']})\"\nend.join(\"\\n\\n\")\n</code></pre>"},{"location":"guides/recalling-memories/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/recalling-memories/#use-case-1-error-analysis","title":"Use Case 1: Error Analysis","text":"<p>Find recent errors and their solutions:</p> <pre><code># Find recent errors\nerrors = htm.recall(\n  timeframe: \"last 7 days\",\n  topic: \"error exception failure\",\n  strategy: :fulltext,\n  limit: 20\n)\n\n# Group by error type\nerror_types = errors\n  .map { |e| e['value'][/Error: (.+?)\\\\n/, 1] }\n  .compact\n  .tally\n\nputs \"Error frequency:\"\nerror_types.sort_by { |_, count| -count }.each do |type, count|\n  puts \"  #{type}: #{count} occurrences\"\nend\n</code></pre>"},{"location":"guides/recalling-memories/#use-case-2-decision-history","title":"Use Case 2: Decision History","text":"<p>Track decision evolution:</p> <pre><code># Get all decisions about a topic\ndecisions = htm.recall(\n  timeframe: Time.at(0)..Time.now,  # All time\n  topic: \"authentication\",\n  strategy: :hybrid,\n  limit: 50\n).select { |m| m['type'] == 'decision' }\n\n# Sort chronologically\ntimeline = decisions.sort_by { |d| d['created_at'] }\n\nputs \"Decision timeline:\"\ntimeline.each do |decision|\n  puts \"#{decision['created_at']}: #{decision['value'][0..100]}...\"\nend\n</code></pre>"},{"location":"guides/recalling-memories/#use-case-3-knowledge-aggregation","title":"Use Case 3: Knowledge Aggregation","text":"<p>Gather all knowledge about a topic:</p> <pre><code>def gather_knowledge(topic)\n  # Gather different types of memories\n  facts = htm.recall(\n    timeframe: \"all time\",\n    topic: topic,\n    strategy: :hybrid\n  ).select { |m| m['type'] == 'fact' }\n\n  decisions = htm.recall(\n    timeframe: \"all time\",\n    topic: topic,\n    strategy: :hybrid\n  ).select { |m| m['type'] == 'decision' }\n\n  code = htm.recall(\n    timeframe: \"all time\",\n    topic: topic,\n    strategy: :hybrid\n  ).select { |m| m['type'] == 'code' }\n\n  {\n    facts: facts,\n    decisions: decisions,\n    code_examples: code\n  }\nend\n\nknowledge = gather_knowledge(\"PostgreSQL\")\n</code></pre>"},{"location":"guides/recalling-memories/#use-case-4-conversation-context","title":"Use Case 4: Conversation Context","text":"<p>Recall recent conversation:</p> <pre><code>def get_conversation_context(session_id, turns: 5)\n  # Get recent conversation turns\n  htm.recall(\n    timeframe: \"last 24 hours\",\n    topic: \"session_#{session_id}\",\n    strategy: :fulltext,\n    limit: turns * 2  # user + assistant messages\n  ).select { |m| m['type'] == 'context' }\n    .sort_by { |m| m['created_at'] }\n    .last(turns * 2)\nend\n</code></pre>"},{"location":"guides/recalling-memories/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/recalling-memories/#search-speed","title":"Search Speed","text":"<ul> <li>Full-text: Fastest (~50-100ms)</li> <li>Vector: Medium (~100-300ms)</li> <li>Hybrid: Medium (~150-350ms)</li> </ul> <p>Times vary based on database size and query complexity.</p>"},{"location":"guides/recalling-memories/#optimizing-queries","title":"Optimizing Queries","text":"<pre><code># Slow: Wide timeframe + high limit\nhtm.recall(timeframe: \"last 5 years\", topic: \"...\", limit: 1000)\n\n# Fast: Narrow timeframe + reasonable limit\nhtm.recall(timeframe: \"last week\", topic: \"...\", limit: 20)\n</code></pre>"},{"location":"guides/recalling-memories/#caching-results","title":"Caching Results","text":"<p>For repeated queries:</p> <pre><code>class CachedRecall\n  def initialize(htm, cache_ttl: 300)\n    @htm = htm\n    @cache = {}\n    @cache_ttl = cache_ttl\n  end\n\n  def recall(**args)\n    cache_key = args.hash\n\n    if cached = @cache[cache_key]\n      return cached[:results] if Time.now - cached[:time] &lt; @cache_ttl\n    end\n\n    results = @htm.recall(**args)\n    @cache[cache_key] = { results: results, time: Time.now }\n    results\n  end\nend\n</code></pre>"},{"location":"guides/recalling-memories/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/recalling-memories/#no-results","title":"No Results","text":"<pre><code>results = htm.recall(timeframe: \"last week\", topic: \"xyz\")\n\nif results.empty?\n  # Try wider timeframe\n  results = htm.recall(timeframe: \"last month\", topic: \"xyz\")\n\n  # Try different strategy\n  results = htm.recall(\n    timeframe: \"last month\",\n    topic: \"xyz\",\n    strategy: :vector  # More flexible\n  )\n\n  # Try related terms\n  results = htm.recall(\n    timeframe: \"last month\",\n    topic: \"xyz related similar\",\n    strategy: :vector\n  )\nend\n</code></pre>"},{"location":"guides/recalling-memories/#low-quality-results","title":"Low-Quality Results","text":"<pre><code># Filter by similarity threshold\ngood_results = results.select do |m|\n  m['similarity'].to_f &gt; 0.7  # Only high-quality matches\nend\n\n# Or boost limit and take top results\nhtm.recall(timeframe: \"...\", topic: \"...\", limit: 100)\n  .sort_by { |m| -m['similarity'].to_f }\n  .first(10)\n</code></pre>"},{"location":"guides/recalling-memories/#ollama-connection-issues","title":"Ollama Connection Issues","text":"<p>If vector search fails:</p> <pre><code>begin\n  results = htm.recall(topic: \"...\", strategy: :vector)\nrescue =&gt; e\n  warn \"Vector search failed: #{e.message}\"\n  warn \"Falling back to full-text search\"\n  results = htm.recall(topic: \"...\", strategy: :fulltext)\nend\n</code></pre>"},{"location":"guides/recalling-memories/#next-steps","title":"Next Steps","text":"<ul> <li>Context Assembly - Use recalled memories with your LLM</li> <li>Search Strategies - Deep dive into search algorithms</li> <li>Working Memory - Understand how recall populates working memory</li> </ul>"},{"location":"guides/recalling-memories/#complete-example","title":"Complete Example","text":"<pre><code>require 'htm'\n\nhtm = HTM.new(robot_name: \"Search Demo\")\n\n# Add test memories\nhtm.add_node(\n  \"decision_db\",\n  \"Chose PostgreSQL for its reliability and ACID compliance\",\n  type: :decision,\n  importance: 9.0,\n  tags: [\"database\", \"postgresql\", \"architecture\"]\n)\n\nhtm.add_node(\n  \"code_connection\",\n  \"conn = PG.connect(dbname: 'mydb')\",\n  type: :code,\n  importance: 6.0,\n  tags: [\"postgresql\", \"ruby\", \"connection\"]\n)\n\n# Vector search: Semantic understanding\nputs \"=== Vector Search ===\"\nvector_results = htm.recall(\n  timeframe: \"all time\",\n  topic: \"data persistence strategies\",\n  strategy: :vector,\n  limit: 10\n)\n\nvector_results.each do |m|\n  puts \"#{m['value'][0..80]}...\"\n  puts \"  Similarity: #{m['similarity']}\"\n  puts\nend\n\n# Full-text search: Exact keywords\nputs \"\\n=== Full-text Search ===\"\nfulltext_results = htm.recall(\n  timeframe: \"all time\",\n  topic: \"PostgreSQL\",\n  strategy: :fulltext,\n  limit: 10\n)\n\nfulltext_results.each do |m|\n  puts \"#{m['value'][0..80]}...\"\n  puts \"  Rank: #{m['rank']}\"\n  puts\nend\n\n# Hybrid search: Best of both\nputs \"\\n=== Hybrid Search ===\"\nhybrid_results = htm.recall(\n  timeframe: \"all time\",\n  topic: \"database connection setup\",\n  strategy: :hybrid,\n  limit: 10\n)\n\nhybrid_results.each do |m|\n  puts \"[#{m['type']}] #{m['value'][0..80]}...\"\n  puts \"  Importance: #{m['importance']}, Similarity: #{m['similarity']}\"\n  puts\nend\n</code></pre>"},{"location":"guides/search-strategies/","title":"Search Strategies Deep Dive","text":"<p>HTM provides three search strategies for retrieving memories: vector search, full-text search, and tag-enhanced hybrid search. This guide explores each strategy in depth, when to use them, and how to optimize performance.</p>"},{"location":"guides/search-strategies/#overview","title":"Overview","text":"Strategy Method Strength Best For Vector Semantic similarity via embeddings Understanding meaning Conceptual queries, related topics Full-text PostgreSQL text search Exact keyword matching Specific terms, proper nouns Hybrid Vector + fulltext + tag matching Best overall accuracy General purpose queries <p> <p>HTM Search Strategy Comparison</p> <p> Vector Search Semantic Similarity</p> <p>How it works: \u2022 Generate query embedding \u2022 Find nearest neighbors \u2022 Rank by cosine similarity</p> <p>Best for: \u2713 Conceptual queries \u2713 Related topics \u2713 Understanding intent</p> <p> Full-Text Search Keyword Matching</p> <p>How it works: \u2022 Tokenize query \u2022 Match against ts_vector \u2022 Rank by tf-idf</p> <p>Best for: \u2713 Exact keywords \u2713 Proper nouns \u2713 Acronyms &amp; commands</p> <p> Hybrid Search Best of Both Worlds</p> <p>How it works: \u2022 Run both searches \u2022 Apply RRF scoring \u2022 Merge &amp; rank results</p> <p>Best for: \u2713 General queries \u2713 Production default \u2713 Mixed terminology</p> <p>Example Query: \"improve database performance\"</p> <p> Vector Results 1. \"Query optimization\" (0.92) 2. \"Caching strategies\" (0.87) 3. \"Index tuning\" (0.85) 4. \"Connection pooling\" (0.82) Finds conceptually related memories (May miss exact terms) Speed: ~80ms</p> <p> Full-Text Results 1. \"Database performance\" (0.95) 2. \"Improve query speed\" (0.88) 3. \"Performance testing\" (0.72) (May miss related concepts) Finds exact keyword matches (Needs right words) Speed: ~30ms</p> <p> Hybrid Results 1. \"Database performance\" (0.96) 2. \"Query optimization\" (0.93) 3. \"Improve query speed\" (0.91) 4. \"Caching strategies\" (0.89) Balanced precision &amp; recall (Recommended!) Speed: ~120ms </p>"},{"location":"guides/search-strategies/#vector-search-semantic","title":"Vector Search (Semantic)","text":"<p>Vector search finds memories based on semantic similarity using embeddings.</p>"},{"location":"guides/search-strategies/#how-it-works","title":"How It Works","text":"<pre><code>User Query: \"database optimization techniques\"\n      \u2193\n   Ollama Embedding (gpt-oss)\n      \u2193\n  [0.234, -0.567, 0.123, ...]  \u2190 1536-dimensional vector\n      \u2193\n   PostgreSQL + pgvector\n      \u2193\n  Find nearest neighbors using cosine similarity\n      \u2193\n  Results ranked by similarity score\n</code></pre>"},{"location":"guides/search-strategies/#basic-usage","title":"Basic Usage","text":"<pre><code>memories = htm.recall(\n  \"improving application performance\",\n  timeframe: \"last month\",\n  strategy: :vector,\n  limit: 10,\n  raw: true  # Get full node data with scores\n)\n\nmemories.each do |m|\n  puts \"#{m['content']}\"\n  puts \"Similarity: #{m['similarity']}\"  # 0.0 to 1.0\n  puts\nend\n</code></pre>"},{"location":"guides/search-strategies/#understanding-similarity-scores","title":"Understanding Similarity Scores","text":"<p>Similarity scores indicate how related the memory is to your query:</p> <pre><code># High similarity (0.8-1.0): Very relevant\n# - Query: \"PostgreSQL optimization\"\n# - Result: \"Optimizing PostgreSQL queries with indexes\" (0.92)\n\n# Medium similarity (0.6-0.8): Moderately relevant\n# - Query: \"database performance\"\n# - Result: \"Caching strategies for web applications\" (0.72)\n\n# Low similarity (0.4-0.6): Loosely related\n# - Query: \"user authentication\"\n# - Result: \"Session management best practices\" (0.58)\n\n# Very low similarity (&lt;0.4): Probably not relevant\n# - Query: \"database backup\"\n# - Result: \"Frontend styling with CSS\" (0.23)\n</code></pre>"},{"location":"guides/search-strategies/#when-vector-search-excels","title":"When Vector Search Excels","text":"<p>1. Conceptual Queries</p> <pre><code># Query about concepts, not specific keywords\nmemories = htm.recall(\n  timeframe: \"last year\",\n  topic: \"ways to speed up slow applications\",\n  strategy: :vector\n)\n\n# Finds:\n# - \"Database query optimization\" (0.89)\n# - \"Caching strategies\" (0.87)\n# - \"Code profiling techniques\" (0.85)\n# - \"Load balancing approaches\" (0.82)\n</code></pre> <p>2. Related Topics</p> <pre><code># Find related concepts even without exact keywords\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"machine learning\",\n  strategy: :vector\n)\n\n# Finds:\n# - \"Neural network architecture\" (no \"ML\" keyword!)\n# - \"Training data preparation\"\n# - \"Model evaluation metrics\"\n# - \"Predictive analytics\"\n</code></pre> <p>3. Understanding Intent</p> <pre><code># Different phrasings of same intent\nqueries = [\n  \"how to make code faster\",\n  \"performance optimization techniques\",\n  \"speeding up application execution\",\n  \"reducing runtime overhead\"\n]\n\nqueries.each do |q|\n  results = htm.recall(timeframe: \"all time\", topic: q, strategy: :vector)\n  # All queries return similar results!\nend\n</code></pre> <p>4. Multilingual Support</p> <pre><code># If embeddings support multiple languages\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"base de donn\u00e9es\",  # French: database\n  strategy: :vector\n)\n\n# Can find English memories about databases\n# (depends on embedding model's training)\n</code></pre>"},{"location":"guides/search-strategies/#vector-search-limitations","title":"Vector Search Limitations","text":"<p>1. Specific Terms</p> <pre><code># Bad for exact technical terms\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"JWT\",  # Specific acronym\n  strategy: :vector\n)\n\n# May miss exact \"JWT\" mentions\n# Better to use full-text for acronyms\n</code></pre> <p>2. Proper Nouns</p> <pre><code># Not ideal for names\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"Alice Thompson\",\n  strategy: :vector\n)\n\n# May not prioritize exact name matches\n# Use full-text or hybrid instead\n</code></pre>"},{"location":"guides/search-strategies/#optimizing-vector-search","title":"Optimizing Vector Search","text":"<p>1. Adjust Similarity Threshold</p> <pre><code>def vector_search_with_threshold(topic, threshold: 0.7)\n  results = htm.recall(\n    timeframe: \"all time\",\n    topic: topic,\n    strategy: :vector,\n    limit: 50\n  )\n\n  # Filter by threshold\n  results.select { |m| m['similarity'].to_f &gt;= threshold }\nend\n\nhigh_quality = vector_search_with_threshold(\"database\", threshold: 0.8)\n</code></pre> <p>2. Use Descriptive Queries</p> <pre><code># Vague: Returns less relevant results\nhtm.recall(topic: \"API\", strategy: :vector)\n\n# Descriptive: Returns more relevant results\nhtm.recall(topic: \"RESTful API design patterns and best practices\", strategy: :vector)\n</code></pre> <p>3. Query Expansion</p> <pre><code>def expanded_vector_search(base_query, related_terms)\n  # Combine base query with related terms\n  expanded = \"#{base_query} #{related_terms.join(' ')}\"\n\n  htm.recall(\n    timeframe: \"all time\",\n    topic: expanded,\n    strategy: :vector,\n    limit: 20\n  )\nend\n\nresults = expanded_vector_search(\n  \"database\",\n  [\"PostgreSQL\", \"SQL\", \"relational\", \"ACID\"]\n)\n</code></pre>"},{"location":"guides/search-strategies/#full-text-search-keywords","title":"Full-text Search (Keywords)","text":"<p>Full-text search uses PostgreSQL's powerful text search capabilities for exact keyword matching.</p>"},{"location":"guides/search-strategies/#how-it-works_1","title":"How It Works","text":"<pre><code>User Query: \"PostgreSQL indexing\"\n      \u2193\n   PostgreSQL ts_query\n      \u2193\n  Tokenize: [\"postgresql\", \"index\"]\n      \u2193\n   Match against ts_vector in database\n      \u2193\n  Rank by relevance (tf-idf)\n      \u2193\n  Results ranked by text rank\n</code></pre>"},{"location":"guides/search-strategies/#basic-usage_1","title":"Basic Usage","text":"<pre><code>memories = htm.recall(\n  timeframe: \"last month\",\n  topic: \"PostgreSQL indexing\",\n  strategy: :fulltext,\n  limit: 10\n)\n\nmemories.each do |m|\n  puts \"#{m['value']}\"\n  puts \"Rank: #{m['rank']}\"  # Higher = better match\n  puts\nend\n</code></pre>"},{"location":"guides/search-strategies/#when-full-text-search-excels","title":"When Full-text Search Excels","text":"<p>1. Exact Keywords</p> <pre><code># Finding specific technical terms\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"JWT OAuth2 authentication\",\n  strategy: :fulltext\n)\n\n# Finds memories containing these exact terms\n</code></pre> <p>2. Proper Nouns</p> <pre><code># Finding people, places, products\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"Alice Thompson\",\n  strategy: :fulltext\n)\n\n# Exact name matches prioritized\n</code></pre> <p>3. Acronyms</p> <pre><code># Technical acronyms\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"REST API CRUD SQL\",\n  strategy: :fulltext\n)\n\n# Finds exact acronym matches\n</code></pre> <p>4. Code and Commands</p> <pre><code># Finding specific code or commands\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"pg_dump VACUUM\",\n  strategy: :fulltext\n)\n\n# Exact command matches\n</code></pre>"},{"location":"guides/search-strategies/#full-text-search-features","title":"Full-text Search Features","text":"<p>1. Boolean Operators</p> <pre><code># PostgreSQL supports AND, OR, NOT\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"PostgreSQL AND (indexing OR optimization)\",\n  strategy: :fulltext\n)\n</code></pre> <p>2. Phrase Matching</p> <pre><code># Find exact phrases\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: '\"database connection pool\"',  # Exact phrase\n  strategy: :fulltext\n)\n</code></pre> <p>3. Stemming</p> <pre><code># PostgreSQL automatically stems words\n# \"running\" matches \"run\", \"runs\", \"runner\"\n\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"optimize\",  # Matches \"optimizing\", \"optimized\", etc.\n  strategy: :fulltext\n)\n</code></pre>"},{"location":"guides/search-strategies/#full-text-search-limitations","title":"Full-text Search Limitations","text":"<p>1. No Semantic Understanding</p> <pre><code># Doesn't understand meaning\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"database\",\n  strategy: :fulltext\n)\n\n# Won't find \"PostgreSQL\" unless query includes it\n# (PostgreSQL doesn't match \"database\" keyword)\n</code></pre> <p>2. Keyword Dependency</p> <pre><code># Must use exact keywords\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"speed up application\",\n  strategy: :fulltext\n)\n\n# Won't find \"performance optimization\"\n# (different keywords, same concept)\n</code></pre>"},{"location":"guides/search-strategies/#optimizing-full-text-search","title":"Optimizing Full-text Search","text":"<p>1. Use Multiple Keywords</p> <pre><code># Include variations and synonyms\nmemories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"database PostgreSQL SQL relational\",\n  strategy: :fulltext\n)\n</code></pre> <p>2. Wildcard Searches</p> <pre><code># Use prefix matching (requires direct SQL)\nconfig = HTM::Database.default_config\nconn = PG.connect(config)\n\nresult = conn.exec_params(\n  &lt;&lt;~SQL,\n    SELECT key, value\n    FROM nodes\n    WHERE to_tsvector('english', value) @@ to_tsquery('english', $1)\n  SQL\n  ['postgres:*']  # Matches postgresql, postgres, etc.\n)\n\nconn.close\n</code></pre>"},{"location":"guides/search-strategies/#hybrid-search-tag-enhanced","title":"Hybrid Search (Tag-Enhanced)","text":"<p>Hybrid search combines full-text, vector, and tag matching for optimal results. This is the recommended strategy for most use cases.</p>"},{"location":"guides/search-strategies/#how-it-works_2","title":"How It Works","text":"<pre><code>User Query: \"PostgreSQL performance tuning\"\n      \u2193\n  Step 1: Find Matching Tags\n  - Search tags for query terms (3+ chars)\n  - E.g., finds \"database:postgresql\", \"performance:optimization\"\n      \u2193\n  Step 2: Build Candidate Pool\n  - Full-text matches (keyword)\n  - Nodes with matching tags (categorical)\n      \u2193\n  Step 3: Score and Rank\n  - Vector similarity (semantic)\n  - Tag boost (categorical match)\n  - Combined score: (similarity \u00d7 0.7) + (tag_boost \u00d7 0.3)\n      \u2193\n  Final Results\n  - Keyword precision + Semantic understanding + Tag relevance\n</code></pre>"},{"location":"guides/search-strategies/#basic-usage_2","title":"Basic Usage","text":"<pre><code>memories = htm.recall(\n  \"PostgreSQL performance optimization\",\n  timeframe: \"last month\",\n  strategy: :hybrid,\n  limit: 10,\n  raw: true  # Get full node data with scores\n)\n\n# Results have keyword matches, semantic relevance, AND tag boosting\nmemories.each do |m|\n  puts \"#{m['content']}\"\n  puts \"Similarity: #{m['similarity']}\"     # Vector similarity (0-1)\n  puts \"Tag Boost: #{m['tag_boost']}\"        # Tag match score (0-1)\n  puts \"Combined: #{m['combined_score']}\"   # Weighted combination\n  puts\nend\n</code></pre>"},{"location":"guides/search-strategies/#tag-enhanced-scoring","title":"Tag-Enhanced Scoring","text":"<p>The hybrid search automatically:</p> <ol> <li>Finds matching tags: Searches tags for query term matches</li> <li>Includes tagged nodes: Adds nodes with matching tags to candidate pool</li> <li>Calculates combined score: <code>(similarity \u00d7 0.7) + (tag_boost \u00d7 0.3)</code></li> </ol> <pre><code># Check which tags match a query\nmatching_tags = htm.long_term_memory.find_query_matching_tags(\"PostgreSQL database\")\n# =&gt; [\"database:postgresql\", \"database:postgresql:extensions\", \"database:sql\"]\n\n# These tags boost relevance of associated nodes in hybrid search\n</code></pre>"},{"location":"guides/search-strategies/#when-hybrid-search-excels","title":"When Hybrid Search Excels","text":"<p>1. General Purpose Queries</p> <pre><code># Best for most use cases\nmemories = htm.recall(\n  \"how to improve database query speed\",\n  timeframe: \"last year\",\n  strategy: :hybrid,\n  raw: true\n)\n\n# Combines:\n# - Keyword matches (database, query, speed)\n# - Semantic understanding (optimization, performance)\n# - Tag boost (nodes tagged with \"database:*\")\n</code></pre> <p>2. Mixed Terminology</p> <pre><code># Query with both specific and general terms\nmemories = htm.recall(\n  \"JWT token authentication security best practices\",\n  timeframe: \"last year\",\n  strategy: :hybrid,\n  raw: true\n)\n\n# Finds:\n# - Exact \"JWT\" mentions (full-text)\n# - Related security concepts (vector)\n# - Nodes tagged \"auth:jwt\", \"security:*\" (tag boost)\n</code></pre> <p>3. Production Applications</p> <pre><code># Recommended default for production\nclass ProductionSearch\n  def initialize(htm)\n    @htm = htm\n  end\n\n  def search(query, timeframe: \"last 90 days\")\n    @htm.recall(\n      query,\n      timeframe: timeframe,\n      strategy: :hybrid,  # Best all-around\n      limit: 20,\n      raw: true\n    )\n  end\nend\n</code></pre>"},{"location":"guides/search-strategies/#hybrid-search-parameters","title":"Hybrid Search Parameters","text":"<p>Prefilter Limit</p> <p>The number of candidates considered from each source (fulltext and tags):</p> <pre><code># In LongTermMemory#search_hybrid:\n# prefilter_limit: 100 (default)\n\n# Direct access with custom prefilter\nresults = htm.long_term_memory.search_hybrid(\n  timeframe: (Time.now - 365*24*3600)..Time.now,\n  query: \"database optimization\",\n  limit: 10,\n  embedding_service: HTM::EmbeddingService.new,\n  prefilter_limit: 200  # More candidates\n)\n</code></pre>"},{"location":"guides/search-strategies/#optimizing-hybrid-search","title":"Optimizing Hybrid Search","text":"<p>1. Balance Keywords and Concepts</p> <pre><code># Good: Mix of specific keywords and concepts\nhtm.recall(\n  \"PostgreSQL query optimization indexing performance\",\n  strategy: :hybrid\n)\n\n# Suboptimal: Only keywords\nhtm.recall(\"PostgreSQL SQL\", strategy: :hybrid)\n\n# Suboptimal: Only concepts\nhtm.recall(\"making things faster\", strategy: :hybrid)\n</code></pre> <p>2. Use Appropriate Timeframes</p> <pre><code># Narrow timeframe: Faster, more recent results\nhtm.recall(\n  \"recent errors\",\n  timeframe: \"last week\",\n  strategy: :hybrid\n)\n\n# Wide timeframe: Comprehensive, slower\nhtm.recall(\n  \"architecture decisions\",\n  timeframe: \"last year\",\n  strategy: :hybrid\n)\n</code></pre> <p>3. Check Tag Coverage</p> <pre><code># See which tags exist for better query formulation\npopular = htm.long_term_memory.popular_tags(limit: 20)\npopular.each do |tag|\n  puts \"#{tag[:name]}: #{tag[:usage_count]} nodes\"\nend\n</code></pre>"},{"location":"guides/search-strategies/#strategy-comparison","title":"Strategy Comparison","text":""},{"location":"guides/search-strategies/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Approximate performance on 10,000 nodes:</p> <pre><code>require 'benchmark'\n\nBenchmark.bm(15) do |x|\n  x.report(\"Vector:\") do\n    htm.recall(timeframe: \"last month\", topic: \"database\", strategy: :vector)\n  end\n\n  x.report(\"Full-text:\") do\n    htm.recall(timeframe: \"last month\", topic: \"database\", strategy: :fulltext)\n  end\n\n  x.report(\"Hybrid:\") do\n    htm.recall(timeframe: \"last month\", topic: \"database\", strategy: :hybrid)\n  end\nend\n\n# Typical results (vary by query and data):\n#                       user     system      total        real\n# Vector:           0.150000   0.020000   0.170000 (  0.210000)\n# Full-text:        0.080000   0.010000   0.090000 (  0.110000)\n# Hybrid:           0.180000   0.025000   0.205000 (  0.250000)\n</code></pre>"},{"location":"guides/search-strategies/#accuracy-comparison","title":"Accuracy Comparison","text":"<pre><code># Test query: \"improving application speed\"\n\n# Vector results (semantic understanding):\n# 1. \"Performance optimization techniques\" (0.91)\n# 2. \"Code profiling for bottlenecks\" (0.88)\n# 3. \"Caching strategies\" (0.85)\n# 4. \"Database query optimization\" (0.82)\n\n# Full-text results (keyword matching):\n# 1. \"Application deployment speed\" (0.95) - Has \"application\" &amp; \"speed\"\n# 2. \"Improving code quality\" (0.72) - Has \"improving\"\n# (May miss relevant results without exact keywords)\n\n# Hybrid results (best of both):\n# 1. \"Performance optimization techniques\" (0.93)\n# 2. \"Application caching strategies\" (0.91)\n# 3. \"Code profiling for bottlenecks\" (0.89)\n# 4. \"Database query optimization\" (0.86)\n</code></pre>"},{"location":"guides/search-strategies/#strategy-selection-guide","title":"Strategy Selection Guide","text":""},{"location":"guides/search-strategies/#decision-tree","title":"Decision Tree","text":"<pre><code>Start\n  \u2193\nDo you need exact keyword matches?\n  YES \u2192 Do you also need semantic understanding?\n          YES \u2192 Use HYBRID\n          NO  \u2192 Use FULL-TEXT\n  NO  \u2192 Do you need conceptual/semantic search?\n          YES \u2192 Use VECTOR\n          NO  \u2192 Use HYBRID (default)\n</code></pre>"},{"location":"guides/search-strategies/#use-case-matrix","title":"Use Case Matrix","text":"Use Case Recommended Strategy Why General search Hybrid Best overall Finding specific terms Full-text Exact matches Conceptual queries Vector Understanding Proper nouns/names Full-text or Hybrid Exact matching Technical acronyms Full-text Keyword precision Related topics Vector Semantic similarity Production default Hybrid Balanced performance Code/command search Full-text Exact syntax Research queries Vector Conceptual understanding"},{"location":"guides/search-strategies/#code-examples","title":"Code Examples","text":"<pre><code>class SmartSearch\n  def initialize(htm)\n    @htm = htm\n  end\n\n  def search(query, timeframe: \"last month\")\n    # Automatically choose strategy based on query\n    strategy = detect_strategy(query)\n\n    @htm.recall(\n      timeframe: timeframe,\n      topic: query,\n      strategy: strategy,\n      limit: 20\n    )\n  end\n\n  private\n\n  def detect_strategy(query)\n    # Check for proper nouns (capital words)\n    has_proper_nouns = query.match?(/\\b[A-Z][a-z]+\\b/)\n\n    # Check for acronyms (all caps words)\n    has_acronyms = query.match?(/\\b[A-Z]{2,}\\b/)\n\n    # Check for specific technical terms\n    has_technical_terms = query.match?(/\\b(JWT|OAuth|SQL|API|REST)\\b/)\n\n    if has_acronyms || has_technical_terms\n      :fulltext  # Use full-text for exact matches\n    elsif has_proper_nouns\n      :hybrid    # Mix of exact and semantic\n    else\n      :vector    # Conceptual search\n    end\n  end\nend\n\n# Usage\nsearch = SmartSearch.new(htm)\nsearch.search(\"JWT authentication\")    # \u2192 Uses :fulltext\nsearch.search(\"Alice Thompson said\")   # \u2192 Uses :hybrid\nsearch.search(\"performance issues\")    # \u2192 Uses :vector\n</code></pre>"},{"location":"guides/search-strategies/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"guides/search-strategies/#1-multi-strategy-search","title":"1. Multi-Strategy Search","text":"<pre><code>def comprehensive_search(query, timeframe: \"last month\")\n  # Run all three strategies\n  vector_results = htm.recall(\n    timeframe: timeframe,\n    topic: query,\n    strategy: :vector,\n    limit: 10\n  )\n\n  fulltext_results = htm.recall(\n    timeframe: timeframe,\n    topic: query,\n    strategy: :fulltext,\n    limit: 10\n  )\n\n  hybrid_results = htm.recall(\n    timeframe: timeframe,\n    topic: query,\n    strategy: :hybrid,\n    limit: 10\n  )\n\n  # Combine and deduplicate\n  all_results = (vector_results + fulltext_results + hybrid_results)\n    .uniq { |m| m['key'] }\n\n  # Sort by best score\n  all_results.sort_by do |m|\n    -(m['similarity']&amp;.to_f || m['rank']&amp;.to_f || 0)\n  end.first(15)\nend\n</code></pre>"},{"location":"guides/search-strategies/#2-fallback-strategy","title":"2. Fallback Strategy","text":"<pre><code>def search_with_fallback(query, timeframe: \"last month\")\n  # Try hybrid first\n  results = htm.recall(\n    timeframe: timeframe,\n    topic: query,\n    strategy: :hybrid,\n    limit: 10\n  )\n\n  # If no results, try vector (more flexible)\n  if results.empty?\n    warn \"No hybrid results, trying vector search...\"\n    results = htm.recall(\n      timeframe: timeframe,\n      topic: query,\n      strategy: :vector,\n      limit: 10\n    )\n  end\n\n  # If still no results, try full-text\n  if results.empty?\n    warn \"No vector results, trying full-text search...\"\n    results = htm.recall(\n      timeframe: timeframe,\n      topic: query,\n      strategy: :fulltext,\n      limit: 10\n    )\n  end\n\n  results\nend\n</code></pre>"},{"location":"guides/search-strategies/#3-confidence-scoring","title":"3. Confidence Scoring","text":"<pre><code>def search_with_confidence(query)\n  results = htm.recall(\n    timeframe: \"all time\",\n    topic: query,\n    strategy: :hybrid,\n    limit: 20\n  )\n\n  # Add confidence scores\n  results.map do |m|\n    similarity = m['similarity'].to_f\n    importance = m['importance'].to_f\n\n    # Calculate confidence (0-100)\n    confidence = (\n      similarity * 60 +      # 60% weight on similarity\n      (importance / 10.0) * 40  # 40% weight on importance\n    ).round(2)\n\n    m.merge('confidence' =&gt; confidence)\n  end.sort_by { |m| -m['confidence'] }\nend\n</code></pre>"},{"location":"guides/search-strategies/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/search-strategies/#no-results-with-vector-search","title":"No Results with Vector Search","text":"<pre><code># If vector search returns nothing:\n# 1. Check Ollama is running\n# 2. Try broader query\n# 3. Widen timeframe\n# 4. Fall back to full-text\n\nif vector_results.empty?\n  # Try full-text as fallback\n  htm.recall(topic: query, strategy: :fulltext)\nend\n</code></pre>"},{"location":"guides/search-strategies/#poor-quality-results","title":"Poor Quality Results","text":"<pre><code># Filter by quality threshold\ndef quality_search(query, min_similarity: 0.7)\n  results = htm.recall(\n    timeframe: \"all time\",\n    topic: query,\n    strategy: :hybrid,\n    limit: 50\n  )\n\n  results.select { |m| m['similarity'].to_f &gt;= min_similarity }\nend\n</code></pre>"},{"location":"guides/search-strategies/#complete-example","title":"Complete Example","text":"<pre><code>require 'htm'\n\nhtm = HTM.new(robot_name: \"Search Demo\")\n\n# Add test data\nhtm.add_node(\"pg_001\", \"PostgreSQL indexing tutorial\", type: :code, importance: 7.0)\nhtm.add_node(\"perf_001\", \"Performance optimization guide\", type: :fact, importance: 8.0)\nhtm.add_node(\"cache_001\", \"Caching strategies for speed\", type: :decision, importance: 9.0)\n\n# Compare strategies\nquery = \"how to make database faster\"\n\nputs \"=== Vector Search (Semantic) ===\"\nvector = htm.recall(timeframe: \"all time\", topic: query, strategy: :vector)\nvector.each { |m| puts \"- #{m['value']} (#{m['similarity']})\" }\n\nputs \"\\n=== Full-text Search (Keywords) ===\"\nfulltext = htm.recall(timeframe: \"all time\", topic: query, strategy: :fulltext)\nfulltext.each { |m| puts \"- #{m['value']} (#{m['rank']})\" }\n\nputs \"\\n=== Hybrid Search (Combined) ===\"\nhybrid = htm.recall(timeframe: \"all time\", topic: query, strategy: :hybrid)\nhybrid.each { |m| puts \"- #{m['value']} (#{m['similarity']})\" }\n</code></pre>"},{"location":"guides/search-strategies/#next-steps","title":"Next Steps","text":"<ul> <li>Recalling Memories - Learn more about recall API</li> <li>Context Assembly - Use search results with LLMs</li> <li>Long-term Memory - Understand the storage layer</li> </ul>"},{"location":"guides/working-memory/","title":"Working Memory Management","text":"<p>Working memory is HTM's token-limited active context system designed for immediate LLM use. This guide explains how it works, how to manage it effectively, and best practices for optimal performance.</p>"},{"location":"guides/working-memory/#what-is-working-memory","title":"What is Working Memory?","text":"<p>Working memory is an in-memory cache that:</p> <ul> <li>Stores active memories for fast access</li> <li>Respects token limits (default: 128,000 tokens)</li> <li>Evicts old/unimportant memories when full</li> <li>Syncs with long-term memory for durability</li> </ul> <p>Think of it as RAM for your robot's consciousness - fast, limited, and volatile.</p>"},{"location":"guides/working-memory/#architecture","title":"Architecture","text":""},{"location":"guides/working-memory/#initialization","title":"Initialization","text":"<p>Configure working memory size when creating HTM:</p> <pre><code># Default: 128K tokens (roughly 512KB of text)\nhtm = HTM.new(\n  robot_name: \"Assistant\",\n  working_memory_size: 128_000\n)\n\n# Large working memory for extensive context\nhtm = HTM.new(\n  robot_name: \"Long Context Bot\",\n  working_memory_size: 1_000_000  # 1M tokens\n)\n\n# Small working memory for focused tasks\nhtm = HTM.new(\n  robot_name: \"Focused Bot\",\n  working_memory_size: 32_000  # 32K tokens\n)\n</code></pre> <p>Choosing Memory Size</p> <ul> <li>32K-64K: Focused tasks, single conversations</li> <li>128K-256K: General purpose, multiple topics (recommended)</li> <li>512K-1M: Extensive context, long sessions</li> <li>&gt;1M: Specialized use cases only (memory overhead)</li> </ul>"},{"location":"guides/working-memory/#how-working-memory-works","title":"How Working Memory Works","text":""},{"location":"guides/working-memory/#adding-memories","title":"Adding Memories","text":"<p>When you add a node, it goes to both working and long-term memory:</p> <pre><code>htm.add_node(\n  \"fact_001\",\n  \"User prefers Ruby for scripting\",\n  type: :fact,\n  importance: 7.0\n)\n\n# Internally:\n# 1. Calculate token count\n# 2. Store in long-term memory (PostgreSQL)\n# 3. Add to working memory (in-memory)\n# 4. Check capacity, evict if needed\n</code></pre>"},{"location":"guides/working-memory/#recalling-memories","title":"Recalling Memories","text":"<p>When you recall, memories are added to working memory:</p> <pre><code>memories = htm.recall(\n  timeframe: \"last week\",\n  topic: \"database design\"\n)\n\n# Internally:\n# 1. Search long-term memory (RAG)\n# 2. For each result:\n#    a. Check if space available\n#    b. Evict if needed\n#    c. Add to working memory\n</code></pre>"},{"location":"guides/working-memory/#automatic-eviction","title":"Automatic Eviction","text":"<p>When working memory is full, HTM evicts memories using a smart algorithm:</p> <pre><code># Algorithm:\n# 1. Calculate eviction score = importance \u00d7 recency\n# 2. Sort by score (lowest first)\n# 3. Evict until enough space\n# 4. Mark as evicted in long-term memory\n</code></pre> <p>Note</p> <p>Evicted memories are not deleted - they remain in long-term memory and can be recalled later.</p>"},{"location":"guides/working-memory/#monitoring-utilization","title":"Monitoring Utilization","text":""},{"location":"guides/working-memory/#basic-stats","title":"Basic Stats","text":"<pre><code>wm = htm.working_memory\n\nputs \"Nodes: #{wm.node_count}\"\nputs \"Tokens: #{wm.token_count} / #{wm.max_tokens}\"\nputs \"Utilization: #{wm.utilization_percentage}%\"\n</code></pre>"},{"location":"guides/working-memory/#detailed-monitoring","title":"Detailed Monitoring","text":"<pre><code>class MemoryMonitor\n  def initialize(htm)\n    @htm = htm\n  end\n\n  def report\n    wm = @htm.working_memory\n    stats = @htm.memory_stats\n\n    puts \"=== Working Memory Report ===\"\n    puts \"Capacity: #{wm.max_tokens} tokens\"\n    puts \"Used: #{wm.token_count} tokens (#{wm.utilization_percentage}%)\"\n    puts \"Free: #{wm.max_tokens - wm.token_count} tokens\"\n    puts \"Nodes: #{wm.node_count}\"\n    puts\n    puts \"Average tokens per node: #{wm.token_count / wm.node_count}\" if wm.node_count &gt; 0\n    puts\n    puts \"=== Long-term Memory ===\"\n    puts \"Total nodes: #{stats[:total_nodes]}\"\n    puts \"Database size: #{(stats[:database_size] / 1024.0 / 1024.0).round(2)} MB\"\n  end\n\n  def health_check\n    util = @htm.working_memory.utilization_percentage\n\n    case util\n    when 0..50\n      { status: :healthy, message: \"Plenty of space\" }\n    when 51..80\n      { status: :warning, message: \"Approaching capacity\" }\n    when 81..95\n      { status: :critical, message: \"Nearly full, evictions likely\" }\n    else\n      { status: :full, message: \"At capacity, frequent evictions\" }\n    end\n  end\nend\n\nmonitor = MemoryMonitor.new(htm)\nmonitor.report\nhealth = monitor.health_check\nputs \"Health: #{health[:status]} - #{health[:message]}\"\n</code></pre>"},{"location":"guides/working-memory/#eviction-behavior","title":"Eviction Behavior","text":""},{"location":"guides/working-memory/#understanding-eviction","title":"Understanding Eviction","text":"<p>HTM evicts memories based on two factors:</p> <ol> <li>Importance: Higher importance = less likely to evict</li> <li>Recency: Newer memories = less likely to evict</li> </ol> <pre><code># Eviction score calculation\nscore = importance \u00d7 (1 / age_in_hours)\n\n# Example scores:\n# High importance (9.0), recent (1 hour): 9.0 \u00d7 1.0 = 9.0 (keep)\n# High importance (9.0), old (24 hours): 9.0 \u00d7 0.042 = 0.38 (maybe evict)\n# Low importance (2.0), recent (1 hour): 2.0 \u00d7 1.0 = 2.0 (evict soon)\n# Low importance (2.0), old (24 hours): 2.0 \u00d7 0.042 = 0.08 (evict first)\n</code></pre>"},{"location":"guides/working-memory/#eviction-example","title":"Eviction Example","text":"<pre><code># Fill working memory\nhtm = HTM.new(\n  robot_name: \"Test\",\n  working_memory_size: 10_000  # Small for demo\n)\n\n# Add important fact (will stay)\nhtm.add_node(\n  \"critical\",\n  \"Critical system password\",\n  importance: 10.0\n)\n\n# Add many low-importance items\n100.times do |i|\n  htm.add_node(\n    \"temp_#{i}\",\n    \"Temporary note #{i}\",\n    importance: 1.0\n  )\nend\n\n# Check what survived\nwm = htm.working_memory\nputs \"Surviving nodes: #{wm.node_count}\"\n\n# Critical fact should still be there\ncritical = htm.retrieve(\"critical\")\nputs \"Critical fact present: #{!critical.nil?}\"\n</code></pre>"},{"location":"guides/working-memory/#manual-eviction","title":"Manual Eviction","text":"<p>You can trigger eviction manually:</p> <pre><code># Access the eviction mechanism (internal API)\nneeded_tokens = 50_000\n\nevicted = htm.working_memory.evict_to_make_space(needed_tokens)\n\nputs \"Evicted #{evicted.length} memories:\"\nevicted.each do |mem|\n  puts \"- #{mem[:key]}: #{mem[:value][0..50]}...\"\nend\n</code></pre> <p>Warning</p> <p>Manual eviction is rarely needed. HTM handles this automatically during normal operations.</p>"},{"location":"guides/working-memory/#best-practices","title":"Best Practices","text":""},{"location":"guides/working-memory/#1-set-appropriate-importance","title":"1. Set Appropriate Importance","text":"<pre><code># Critical data: Never evict\nhtm.add_node(\n  \"api_key\",\n  \"Production API key\",\n  importance: 10.0\n)\n\n# Important context: Retain longer\nhtm.add_node(\n  \"user_goal\",\n  \"User wants to optimize database\",\n  importance: 8.0\n)\n\n# Temporary context: Evict when needed\nhtm.add_node(\n  \"current_topic\",\n  \"Discussing query optimization\",\n  importance: 5.0\n)\n\n# Disposable notes: Evict first\nhtm.add_node(\n  \"scratch\",\n  \"Temporary calculation result\",\n  importance: 1.0\n)\n</code></pre>"},{"location":"guides/working-memory/#2-monitor-utilization-regularly","title":"2. Monitor Utilization Regularly","text":"<pre><code>class WorkingMemoryManager\n  def initialize(htm, threshold: 80.0)\n    @htm = htm\n    @threshold = threshold\n  end\n\n  def check_and_warn\n    util = @htm.working_memory.utilization_percentage\n\n    if util &gt; @threshold\n      warn \"Working memory at #{util}%!\"\n      warn \"Consider increasing working_memory_size or reducing context\"\n    end\n  end\n\n  def auto_adjust_importance\n    util = @htm.working_memory.utilization_percentage\n\n    # If critically full, boost importance of current context\n    if util &gt; 90\n      # Implementation would require tracking current context keys\n      # and updating their importance in the database\n      warn \"Critical capacity reached\"\n    end\n  end\nend\n</code></pre>"},{"location":"guides/working-memory/#3-use-context-strategically","title":"3. Use Context Strategically","text":"<p>Don't load unnecessary data into working memory:</p> <pre><code># Bad: Load everything\nall_memories = htm.recall(\n  timeframe: \"all time\",\n  topic: \"anything\",\n  limit: 1000\n)\n# This fills working memory with potentially irrelevant data\n\n# Good: Load what you need\nrelevant = htm.recall(\n  timeframe: \"last week\",\n  topic: \"current project\",\n  limit: 20\n)\n# This keeps working memory focused\n</code></pre>"},{"location":"guides/working-memory/#4-clean-up-when-done","title":"4. Clean Up When Done","text":"<p>Remove temporary memories:</p> <pre><code>def with_temporary_context(htm, key, value)\n  # Add temporary context\n  htm.add_node(key, value, type: :context, importance: 2.0)\n\n  yield\n\n  # Clean up\n  htm.forget(key, confirm: :confirmed)\nend\n\nwith_temporary_context(htm, \"scratch_001\", \"Temp data\") do\n  # Use the temporary context\n  context = htm.create_context(strategy: :recent)\n  # ... do work\nend\n# Temp data is now removed\n</code></pre>"},{"location":"guides/working-memory/#5-batch-operations-carefully","title":"5. Batch Operations Carefully","text":"<p>Be mindful when adding many memories at once:</p> <pre><code># Risky: Might fill working memory quickly\n1000.times do |i|\n  htm.add_node(\"item_#{i}\", \"Data #{i}\", importance: 5.0)\nend\n\n# Better: Add with appropriate importance\n1000.times do |i|\n  htm.add_node(\n    \"item_#{i}\",\n    \"Data #{i}\",\n    importance: 3.0  # Lower importance for bulk data\n  )\nend\n\n# Or: Monitor during batch operations\nbatch_data.each_with_index do |data, i|\n  htm.add_node(\"item_#{i}\", data, importance: 5.0)\n\n  # Check capacity every 100 items\n  if i % 100 == 0\n    util = htm.working_memory.utilization_percentage\n    puts \"Utilization: #{util}%\"\n  end\nend\n</code></pre>"},{"location":"guides/working-memory/#working-memory-strategies","title":"Working Memory Strategies","text":""},{"location":"guides/working-memory/#strategy-1-sliding-window","title":"Strategy 1: Sliding Window","text":"<p>Keep only recent memories:</p> <pre><code>class SlidingWindow\n  def initialize(htm, window_size: 50)\n    @htm = htm\n    @window_size = window_size\n    @keys = []\n  end\n\n  def add(key, value, **opts)\n    @htm.add_node(key, value, **opts)\n    @keys &lt;&lt; key\n\n    # Evict oldest if window exceeded\n    if @keys.length &gt; @window_size\n      oldest = @keys.shift\n      @htm.forget(oldest, confirm: :confirmed) rescue nil\n    end\n  end\nend\n</code></pre>"},{"location":"guides/working-memory/#strategy-2-importance-thresholding","title":"Strategy 2: Importance Thresholding","text":"<p>Only keep high-importance memories:</p> <pre><code>class ImportanceFilter\n  def initialize(htm, min_importance: 7.0)\n    @htm = htm\n    @min_importance = min_importance\n  end\n\n  def add(key, value, importance:, **opts)\n    @htm.add_node(key, value, importance: importance, **opts)\n\n    # If low importance and memory is tight, evict immediately\n    if importance &lt; @min_importance &amp;&amp;\n       @htm.working_memory.utilization_percentage &gt; 80\n\n      # Let it evict naturally or remove from working memory\n      # (Note: HTM doesn't expose direct working memory removal,\n      #  so we rely on natural eviction)\n    end\n  end\nend\n</code></pre>"},{"location":"guides/working-memory/#strategy-3-topic-based-management","title":"Strategy 3: Topic-Based Management","text":"<p>Group memories by topic and manage separately:</p> <pre><code>class TopicManager\n  def initialize(htm)\n    @htm = htm\n    @topics = Hash.new { |h, k| h[k] = [] }\n  end\n\n  def add(key, value, topic:, **opts)\n    @htm.add_node(key, value, **opts)\n    @topics[topic] &lt;&lt; key\n  end\n\n  def clear_topic(topic)\n    keys = @topics[topic] || []\n    keys.each do |key|\n      @htm.forget(key, confirm: :confirmed) rescue nil\n    end\n    @topics.delete(topic)\n  end\n\n  def focus_on_topic(topic)\n    # Clear all other topics to make space\n    @topics.keys.each do |t|\n      clear_topic(t) unless t == topic\n    end\n  end\nend\n</code></pre>"},{"location":"guides/working-memory/#token-counting","title":"Token Counting","text":"<p>HTM uses Tiktoken to count tokens:</p> <pre><code># Token counts vary by content\nshort = \"Hello world\"  # ~2 tokens\nmedium = \"A\" * 100     # ~25 tokens\nlong = \"word \" * 1000  # ~1000 tokens\n\n# Check token count of a string\nembedding_service = HTM::EmbeddingService.new\ntokens = embedding_service.count_tokens(long)\nputs \"Token count: #{tokens}\"\n</code></pre> <p>Token vs Characters</p> <ul> <li>1 token \u2248 4 characters (English)</li> <li>128K tokens \u2248 512KB text</li> <li>Code uses fewer tokens per character</li> <li>Special characters use more tokens</li> </ul>"},{"location":"guides/working-memory/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/working-memory/#memory-overhead","title":"Memory Overhead","text":"<p>Working memory has minimal overhead:</p> <pre><code># Memory usage per node (approximate):\n# - Key: ~50 bytes\n# - Value: N bytes (your content)\n# - Metadata: ~100 bytes\n# - Total: ~150 bytes + content\n\n# For 1000 nodes with 500-char content:\n# 1000 \u00d7 (150 + 500) = ~650KB\n\n# Token count is stored but content dominates\n</code></pre>"},{"location":"guides/working-memory/#access-speed","title":"Access Speed","text":"<p>Working memory is very fast:</p> <pre><code>require 'benchmark'\n\nhtm = HTM.new(robot_name: \"Perf Test\")\n\n# Add 1000 memories\n1000.times do |i|\n  htm.add_node(\"key_#{i}\", \"Value #{i}\", importance: 5.0)\nend\n\n# Benchmark working memory access\nBenchmark.bm do |x|\n  x.report(\"create_context:\") do\n    1000.times { htm.create_context(strategy: :balanced) }\n  end\nend\n\n# Typical results:\n# create_context: ~1ms per call\n</code></pre>"},{"location":"guides/working-memory/#optimization-tips","title":"Optimization Tips","text":"<pre><code># 1. Avoid frequent context assembly\n# Bad: Assemble context every message\ndef process_message(message)\n  context = htm.create_context  # Slow if called frequently\n  llm.chat(context + message)\nend\n\n# Good: Cache context, update periodically\n@context_cache = nil\n@context_age = 0\n\ndef process_message(message)\n  if @context_cache.nil? || @context_age &gt; 10\n    @context_cache = htm.create_context\n    @context_age = 0\n  end\n  @context_age += 1\n\n  llm.chat(@context_cache + message)\nend\n\n# 2. Use appropriate token limits\n# Don't request more than your LLM can handle\ncontext = htm.create_context(\n  strategy: :balanced,\n  max_tokens: 100_000  # Match LLM's context window\n)\n\n# 3. Monitor and adjust\nutil = htm.working_memory.utilization_percentage\nif util &gt; 90\n  # Reduce working memory size or increase eviction\nend\n</code></pre>"},{"location":"guides/working-memory/#debugging-working-memory","title":"Debugging Working Memory","text":""},{"location":"guides/working-memory/#inspecting-contents","title":"Inspecting Contents","text":"<pre><code>class WorkingMemoryInspector\n  def initialize(htm)\n    @htm = htm\n  end\n\n  def show_contents\n    wm = @htm.working_memory\n\n    puts \"=== Working Memory Contents ===\"\n    puts \"Total nodes: #{wm.node_count}\"\n    puts \"Total tokens: #{wm.token_count}\"\n    puts\n\n    # Access internal structure (advanced)\n    # Note: This requires access to WorkingMemory internals\n    # For production, use public APIs only\n  end\n\n  def find_large_nodes(threshold: 1000)\n    # Find nodes using many tokens\n    # This would require iterating working memory\n    # (not directly exposed in current API)\n  end\n\n  def show_eviction_candidates\n    # Show which nodes would be evicted next\n    # Based on importance and recency\n  end\nend\n</code></pre>"},{"location":"guides/working-memory/#common-issues","title":"Common Issues","text":"<p>Issue: Working memory always full</p> <pre><code># Check if you're adding too much\nstats = htm.memory_stats\nwm_util = stats[:working_memory][:utilization]\n\nif wm_util &gt; 95\n  puts \"Working memory consistently full\"\n  puts \"Solutions:\"\n  puts \"1. Increase working_memory_size\"\n  puts \"2. Lower importance of bulk data\"\n  puts \"3. Reduce recall limit\"\n  puts \"4. Clean up temporary data more frequently\"\nend\n</code></pre> <p>Issue: Important data getting evicted</p> <pre><code># Increase importance of critical data\nhtm.add_node(\n  \"critical_data\",\n  \"Important information\",\n  importance: 9.5  # High enough to avoid eviction\n)\n</code></pre> <p>Issue: Memory utilization too low</p> <pre><code># Working memory underutilized\nwm_util = htm.working_memory.utilization_percentage\n\nif wm_util &lt; 20\n  puts \"Working memory underutilized\"\n  puts \"Consider:\"\n  puts \"1. Reducing working_memory_size to save RAM\"\n  puts \"2. Recalling more context\"\n  puts \"3. Using larger token limits in create_context\"\nend\n</code></pre>"},{"location":"guides/working-memory/#next-steps","title":"Next Steps","text":"<ul> <li>Context Assembly - Use working memory effectively with LLMs</li> <li>Long-term Memory - Understand persistent storage</li> <li>Adding Memories - Learn about importance scoring</li> </ul>"},{"location":"guides/working-memory/#complete-example","title":"Complete Example","text":"<pre><code>require 'htm'\n\n# Initialize with moderate working memory\nhtm = HTM.new(\n  robot_name: \"Memory Manager\",\n  working_memory_size: 128_000\n)\n\n# Monitor class\nclass Monitor\n  def initialize(htm)\n    @htm = htm\n  end\n\n  def report\n    wm = @htm.working_memory\n    puts \"Utilization: #{wm.utilization_percentage}%\"\n    puts \"Nodes: #{wm.node_count}\"\n    puts \"Tokens: #{wm.token_count} / #{wm.max_tokens}\"\n  end\nend\n\nmonitor = Monitor.new(htm)\n\n# Add memories with different importance\nputs \"Adding critical data...\"\nhtm.add_node(\"critical\", \"Critical system data\", importance: 10.0)\nmonitor.report\n\nputs \"\\nAdding important data...\"\n10.times do |i|\n  htm.add_node(\"important_#{i}\", \"Important item #{i}\", importance: 8.0)\nend\nmonitor.report\n\nputs \"\\nAdding regular data...\"\n50.times do |i|\n  htm.add_node(\"regular_#{i}\", \"Regular item #{i}\", importance: 5.0)\nend\nmonitor.report\n\nputs \"\\nAdding temporary data...\"\n100.times do |i|\n  htm.add_node(\"temp_#{i}\", \"Temporary item #{i}\", importance: 2.0)\nend\nmonitor.report\n\n# Check what survived\nputs \"\\n=== Survival Check ===\"\ncritical = htm.retrieve(\"critical\")\nputs \"Critical survived: #{!critical.nil?}\"\n\n# Create context\nputs \"\\nCreating context...\"\ncontext = htm.create_context(strategy: :important, max_tokens: 50_000)\nputs \"Context length: #{context.length} characters\"\n\n# Final stats\nputs \"\\n=== Final Stats ===\"\nmonitor.report\n</code></pre>"}]}